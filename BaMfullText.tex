\thispagestyle{empty}
\chapter{The Bayesian Prior}\label{Prior.Chapter}
\setcounter{examplecounter}{1}

\section{A Prior Discussion of Priors}
Specifying Bayesian models necessarily means providing prior distributions for unknown parameters.  The prior plays a critical 
role in Bayesian inference through the updating statement: $\pi(\theta) \propto p(\theta)L(\theta|\X)$.  Central to the 
Bayesian philosophy is that all unknown quantities are described probabilistically, even before the data has been observed.  
Generally these unknown quantities are the model parameters, $\T$ in the general notational sense, but missing data are 
handled in the same fashion.  This is very much at odds with the frequentist notion that unknown parameters are fixed, 
unyielding quantities that can be estimated with procedures that are either repeated many times or imagined to be repeated 
many times.  The immobile parameter perspective, although widespread, is contradictory to the way that most social and 
behavioral scientists conduct research.  It simply is not possible to rerun elections, repeat surveys under exactly the 
same conditions, replay the stock market with exactly matching economic forces, fight the same war, or re-expose clinical 
subjects to identical stimuli.\index{subjectindex}{survey data!replicability}

The core of the disagreement between frequentists and Bayesians is the differing fundamental interpretations of probability.  
Frequentists believe that probabilities are long-run tendencies of events that eventually converge on some true population 
proportion that can be interpreted as a probability of such events even in the short term.
\index{subjectindex}{frequentism!probability interpretation}\index{subjectindex}{degree of belief} Bayesians generally 
interpret probability as ``degree of belief,'' meaning that prior distributions are descriptions of relative likelihoods of 
events based on the researcher's past experience, personal intuition, or expert opinion, and posterior distributions are 
these prior distributions updated by conditioning on new observed data.\index{subjectindex}{subjective probability} Some 
authors also focus on the distinction between priors that are explicitly based on previous empirical work versus priors 
that represent general knowledge possessed by the researcher (Zellner\index{authorindex}{Zellner, A.} 1971, p.18).  
Mathematically this does not change the mechanics of prior inclusion, however.

It is important to understand that priors are not merely annoyances that must be dealt with before moving on to more
interesting parts of the specification process.  They are actually an opportunity to systematically include qualitative, 
narrative, and intuitive knowledge into statistical models.  Because there is a lot of historical controversy concerning
the impact of prior distributions, many applied Bayesians in the social and behavioral sciences seek to use only highly 
diffuse forms such as the uniform distribution.  Some of these researchers merely want the probabilistic inferential 
process and the powerful machinery of MCMC without being ``fully Bayesian.''  These ``Bayesians of convenience''  miss
the point that informed prior distributions are incredibly useful for integrating non-quantitative information into the 
statistical model.  Specifically, see  Gill and Walker (2005) in political science,
\index{authorindex}{Gill, J.} \index{authorindex}{Walker, L.}
Vanpaemel (2010) in psychology, \index{authorindex}{Vanpaemel, W.} 
Raftery (1999) in sociology, \index{authorindex}{Raftery, A. E.}
Gill and Meier (2000) in public administration, \index{authorindex}{Gill, J.}\index{authorindex}{Meier, K. J.}
Wolfson, Kadane and Small (1996) in policy studies,
\index{authorindex}{Kadane, J. B.} \index{authorindex}{Wolfson, L. J.}\index{authorindex}{Small, M. J.}
and Martin and Quinn (2007) in legal studies.\index{authorindex}{Quinn, K. M.}\index{authorindex}{Martin, A.}

\section{A Plethora of Priors}
We have already seen several types of prior distributions in previous chapters.  This chapter lays out the three general categories
of prior distributions used in Bayesian models: conjugate, uninformed, and informative.  The categorization is artificial (but
convenient) since the boundaries are blurred: conjugate forms are sometimes highly informed, the distinction between uninformed and
informed is across a relative spectrum, and there does not exist a prior distribution with absolutely no information (hence the 
use of ``uninformative'' rather than ``noninformative'' here).  This 
distinction was made clear in a historical context by Diaconis\index{authorindex}{Diaconis, P.} and Ylvisaker
\index{authorindex}{Ylvisaker, D.} (1985).   In their words, the classical Bayesian
\index{subjectindex}{prior distribution!classical Bayesian} views the prior distribution as a necessary inconvenience and typically 
attempts to specify a ``flat'' prior\index{subjectindex}{prior distribution!flat} so as to interject the least amount of prior 
knowledge as possible.  These priors are correctly called \emph{uninformative}\index{subjectindex}{prior distribution!uninformative} 
and are sometimes difficult to find.  Note that these are frequently called \emph{non}informative, but this is actually an 
inaccurate term for reasons that we will see shortly.  Modern parametric\index{subjectindex}{prior distribution!modern parametric} 
Bayesians specify priors possessing deliberate characteristics, such as conjugacy, and assign prior parameter values according to 
specific criteria unrelated to substantive knowledge.  Subjective Bayesians elicit prior distributions according to preexisting 
scientific knowledge in a substantive field.\index{subjectindex}{prior distribution!subjective}  This can come from previous 
empirical work in the field or from expert opinions by non-statisticians.  In practice these categories are far from mutually 
exclusive, and it is more common to see a mixed approach that combines aspects of previous knowledge, mathematical convenience, 
and a desire not to overly affect the final conclusions with a strongly influential prior.

Prior distributions remain the most controversial aspect of Bayesian inference.  Critics have focused on the supposedly 
personal-subjective nature of priors, generally neglecting to notice that \emph{all} statistical models involve subjective 
choices.  One ecumenical position taken by this book is that it is rare to approach a problem with absolutely no prior 
information about parameters of interest, and that even in these situations there will exist a class of priors designed to supply 
relatively little prior information.  In fact, most readers would be reluctant to accept model specifications by authors who knew 
\emph{nothing} before analyzing the data.  While it is coy to say ``everyone is a Bayesian, some of us know it,'' most 
researchers tell us about their prior knowledge even if it is not put directly in the form of a prior distribution.  Consider the 
following quotation from Canes-Wrone, Brady, and Cogan (2002) in political science.
\index{authorindex}{Canes-Wrone, B.} \index{authorindex}{Brady, D. W.} \index{authorindex}{Cogan, J. F.}
\begin{quote}
        The effects of campaign spending and district ideology are \emph{consistently in the expected direction} and statistically 
	significant.  Those on challenger quality also have the \emph{correct sign} in each regime and sample and are significant 
	with the exception of the marginal regime of the 1980-1996 test.  In addition, the coefficients for the remaining 
	variables that are not included as a main effect typically have the \emph{predicted sign} and they are significant \emph{only 
	with the expected sign}.
\end{quote}
(Italics added.)
Obviously there is a lot of prior information revealed in that passage.  Not only are they indicating the direction of effects, 
they tell us which ones were expected to be statistically reliable.  The Bayesian prior provides a way for researchers to be more 
overt about their knowledge, attitudes, and opinions on studied social phenomena.

Leamer (1983)\index{authorindex}{Leamer, E. E.} specifies a hierarchy of priors based on the level of confidence in some effect of 
interest:  truths (axioms) $>$ facts (proven from axioms/observations) $>$ opinions $>$ conventions.  So modeling decisions should be 
based on the most supportable level of this hierarchy.  For instance, if we design an experiment in which one of two possible events 
occurs and the number of trials is established beforehand such that each trial is produced identically from the same distribution, 
then the \emph{truth} is that a binomial distribution should be used to model the likelihood for the event of primary interest.  The 
weakest form of prior evidence comes from conventions.  These include not only defensible model choices, but also things like linearity
	assumptions, normal specifications, and pre-set $\alpha$ levels.\footnote{The common $\alpha$ levels of $0.1$, $0.05$, and
	$0.01$ come from Fisher's tables and the reluctance of mid-20th century scientists to challenge or recalculate normal 
	tail values.  Fisher's justification actually rests on no scientific principle other than the assertion that these levels 
	represented some standard convention in human thought: ``It is usual and convenient for experimenters to take 5 per cent 
	as a standard level of significance\ldots'' (1934, p.15).\index{authorindex}{Fisher, R. A.}\index{authorindex}{Leamer, E. E.}}
Leamer's point is that ``\ldots the choice of a particular sampling distribution, or a particular prior distribution, is
inherently whimsical,'' so we should not pretend to find ``objective'' priors, and should not bury assumptions, but instead should
seek to overtly obtain and describe the highest current level in the hierarchy.  Of course we don't always have truths or facts
and therefore need to rely on opinions: ``As I see it, the fundamental problem facing econometrics is how adequately to control
the whimsical character of inference, how sensibly to base inferences on opinions when facts are unavailable'' (Leamer 1983,
p.38).

What may have seemed like a weakness to 20th century critics of Bayesian inference is actually a core strength.  Priors are a
means of systematically incorporating existing human knowledge, quantitative or qualitative, into the statistical specification.
For excellent discussions of the nuances see Berger\index{authorindex}{Berger, J. O.} (1985, p.74-82), Savage
\index{authorindex}{Savage, L. J.} (1972, Chapters 3 and 4), Barnett\index{authorindex}{Barnett, V.} (1973, p.80-88), and the
thoughtful essays contained in Wright\index{authorindex}{Wright, G.} and Ayton\index{authorindex}{Ayton, P.} (1994).  

This chapter covers the previously noted trilogy of prior distributions commonly applied in Bayesian work: conjugate, uninformed, 
and informative.  All priors are subjective in the sense that the decision to use any prior is completely that of the researcher, 
subject to mathematical constraints that may apply.  It is important to remember that the choice of priors is no more subjective 
than the choice of likelihood, the selection or collection of a given sample, the algorithm for estimation, or the statistic used 
for data reduction.  The set of priors described herein are by no means the complete set of available forms.  For a review of 
additional prior specifications see Kass\index{authorindex}{Kass, R. E.} and Wasserman\index{authorindex}{Wasserman, L.} (1996).  
\index{subjectindex}{prior distribution!typology}

\section{Conjugate Prior Forms}
\index{subjectindex}{Bayesian inference}\index{subjectindex}{prior distribution!conjugate}\label{Conjugate.Section}
One difficult aspect of Bayesian inference is that the posterior distribution of the $\T$ vector might not have an analytically 
tractable form (\emph{the} primary motivation for the simulation techniques discussed in later chapters), particularly in higher 
dimensions.  Specifically, producing marginals from high-dimension $\pi(\T)$ by repeated analytical integration may be difficult 
or even impossible mathematically.  One way to guarantee that the posterior has an easily calculable form is to specify a 
\emph{conjugate prior}.  As briefly described in Chapter~\ref{Model.Chapter} in Section~\ref{Bayesian.Framework.Intro} and 
developed in some detail for the normal model in Chapter~\ref{Normal.Model.Chapter} in Section~\ref{normal.model.inverse.gamma.prior}, 
\index{subjectindex}{prior distribution!conjugate} conjugacy is a joint property of the prior and the likelihood function that 
provides a posterior from the same distributional family as the prior.\index{subjectindex}{conjugacy!definition} In other words, the 
mathematical form of the prior distribution ``passes through'' the data-conditioning phase and endures in the posterior: closure 
under sampling.  Thus the general form for the distribution of the effect of interest is invariant to Bayesian inference.  We have 
already seen two important cases in Chapter~\ref{Normal.Model.Chapter} where a normal prior along with a normal joint likelihood 
function produced a normal posterior for the mean parameter, and an inverse gamma prior with the same normal likelihood produced 
an inverse gamma posterior for the variance parameter.  It is important to keep in mind that inverse gamma distributions with small 
parameter values are not necessarily low information forms since they place most of the density near one, implying very small
precision (see Figure~\ref{inverse.gamma.fig}).  In variance terms these forms then specify a great amount of the density at 
large values that may be unrealistic in modeling terms.  (Gelman 2006, Hodges and Sargent 2001, Natarajan and Kass 2000).
\index{authorindex}{Gelman, A.} \index{authorindex}{Hodges, J. S.} \index{authorindex}{Sargent, D. J.}

\subsection{Example: Conjugacy in Exponential Specifications}\label{exponential.conjugacy.example}
\index{subjectindex}{conjugacy!exponential/gamma}
A very simple way to model the time that something endures (wars, lifetimes, regimes, bull markets, marriages, etc.) is to use the 
exponential PDF, which is a special case of the gamma distribution where the shape parameter is fixed at one
(Appendix~\ref{distribution.appendix}).  This has the form:
\begin{equation}
    \mathcal{E}(X|\theta) = \theta \exp[-\theta X], \quad 0 \le X, 0 < \theta
\end{equation}
(note the use of the ``rate'' form of the gamma distribution).
\index{subjectindex}{distribution!exponential}
An attractive candidate prior for $\theta$ in the exponential PDF is the gamma distribution because not only does a random variable 
distributed gamma have the same support as that of an
    exponential,\footnote{Actually the exponential PDF is a special case of the gamma PDF where the first (shape) parameter is fixed at one.  
    See Exercise~\ref{Prior.Chapter}.\ref{expo.gamma.exercise}.}
but also because the gamma is an extremely flexible parametric form.  The gamma PDF from Appendix~\ref{distribution.appendix} is:
\begin{equation}
    f(\theta|\alpha,\beta) = \frac{1}{\Gamma(\alpha)}\beta^\alpha \theta^{\alpha-1}\exp[-\beta \theta],
                \qquad \theta,\alpha,\beta > 0.   \nonumber
\end{equation}
Suppose we now observe $x_1,x_2,\ldots,x_n \sim \;\text{iid}\, \mathcal{E}(X|\theta)$ and produce the likelihood function:
\index{subjectindex}{iid}
\begin{equation}
    L(\theta|\mathbf{x}) = \prod_{i=1}^{n} \theta e^{-\theta x_i}
                = \theta^n \exp\left[-\theta \sum_{i=1}^n x_i\right].  \nonumber
\end{equation}
Note that $\sum_{i=1}^n x_i$ is a sufficient statistic for $\theta$.  The posterior distribution is produced as follows:
\index{subjectindex}{sufficient statistic}
\begin{align}
    \pi(\theta|\mathbf{x})  &\propto  L(\theta|\mathbf{x})p(\theta) \nonumber \9
                            &= \theta^n \exp\left[-\theta \sum_{i=1}^n x_i\right]
                               \frac{1}{\Gamma(\alpha)}\beta^\alpha \theta^{\alpha-1}\exp[-\beta\theta]
                   	       \nonumber \9
                	    &\propto \theta^{(\alpha+n)-1} \exp\left[-\theta\left(\sum_{i=1}^n x_i + \beta\right)\right].
\end{align}
It is easy to see that this is the kernel of a $\mathcal{G}(\alpha+n,\sum x_i + \beta)$ PDF, and therefore the gamma distribution is 
shown to be conjugate to the exponential likelihood function.

\subsection{The Exponential Family Form}
Recall that a family of PDFs or PMFs qualify as an exponential family form if they can be rearranged in a specific manner that 
recharacterizes familiar functions into a formula that is useful theoretically and demonstrates similarity between seemingly disparate 
mathematical forms.\index{subjectindex}{exponential family form}  Suppose we temporarily consider only a one-parameter PDF or PMF, 
$f(x|\theta)$, with one datum.  This function is classified as an exponential family only if it can be rearranged to be of the form:
$f(x|\theta) = \exp\bigl[t(x)u(\theta)\bigr]r(x)s(\theta)$, where $r$ and $t$ are real-valued functions of $x$ that do not depend on 
$\theta$, and $s$ and $u$ are real-valued functions of $\theta$ that do not depend on $x$, and $r(x) > 0, s(\theta) > 0 \; \forall x,\theta$.
Also, many forms, such as the normal, include a scale parameter making the form of the exponential family: 
$f(x|\theta) = \exp\bigl[t(x)u(\theta)/\phi\bigr]r(x)s(\theta)$, where $\phi$ (sometimes denoted $a(\phi)$) is a scale parameter 
($\sigma^2$ for the normal), and sometimes explicitly weighted as in $\phi/\omega_i$.  Because of the properties of logs and exponentiation, 
this form can always be rewritten as:
\begin{equation}\label{expo.form}
        f(x|\theta) = \exp\bigl[\;
        \underbrace{t(x)u(\theta)}_{\overset{\text{\footnotesize{interaction}}} {\text{component}}} +
        \underbrace{\log (r(x)) + \log (s(\theta))}_{\text{additive component}}
        \;\bigr],
\end{equation}
(Gill 2000).\index{authorindex}{Gill, J.} \index{subjectindex}{exponential family form}  The labeled interaction component in 
\eqref{expo.form}, $t(x)u(\theta)$, reflects the product-indistinguishable relationship between $x$ and $\theta$.  It should be noted that 
this component must specify $t(x)u(\theta)$ in a strictly multiplicative 
	manner.\footnote{For instance, consider the Weibull PDF\index{subjectindex}{distribution!Weibull} (useful for modeling general 
	failure times): $f(x|\gamma,\beta)=\frac{\gamma}{\beta}x^{\gamma-1}\exp(-x^{\gamma}/\beta)$ for $x\ge 0,\gamma,\beta>0$.  The 
	term $-\frac{1}{\beta}x^{\gamma}$ in the exponent disqualifies this PDF from the exponential family classification since it 
	cannot be expressed in the additive or multiplicative form of (\ref{expo.form}).  However, if $\gamma$ is known (or we are 
	willing to assign an estimate), then the Weibull PDF reduces to an exponential family form.}
The structure of (\ref{expo.form}) is preserved under sampling so the joint density function of iid random 
variables $\mathbf{X} = \{X_1,X_2,\ldots,X_n\}$ is simply:\index{subjectindex}{exponential family form!likelihood}
\index{subjectindex}{iid}
\begin{equation}\label{joint.expo.form}
        f(\mathbf{x}|\theta) = \exp\left[ u(\theta)\sum_{i=1}^{n}t(x_i) + \sum_{i=1}^{n}\log(r(x_i)) +
                n\log(s(\theta)) \right].
\end{equation}

Fisher\index{authorindex}{Fisher, R. A.} (1934) originated this idea to show that many common PDFs and PMFs are actually all
special cases of the more general classification labeled  \emph{exponential family} because  subfunctions are contained within the
exponent component.  This contrasts sharply with the econometric approach of seeing these distributions as leading to separate modeling
constructs.  Fisher also showed that these isolated subfunctions quite naturally produce a small number of sufficient
statistics\index{subjectindex}{sufficient statistic} that compactly summarize even large data sets without any loss of
information.  Barndorff-Nielsen\index{authorindex}{Barndorff-Nielsen, O. E.} (1978, p.114) demonstrated that exponential family
probability functions possess all of their moments (defined in Appendix~\ref{GLM.Chapter}), and are therefore easier to
characterize in a Bayesian framework.  Morris \index{authorindex}{Morris, C. N.} (1982, 1983a), building upon the results of
Diaconis\index{authorindex}{Diaconis, P.} and Ylvisaker\index{authorindex}{Ylvisaker, D.} (1979) showed that the most commonly
used exponential families of probability functions when specified as likelihood functions possess conjugate priors.
Consonni\index{authorindex}{Consonni, G.} and Veronese \index{authorindex}{Veronese, P.} (1992) proved that this is true for any
mean parameter in an exponential family form with quadratic variance (the mean parameter is contained in an algebraic term with an
exponent equal to 2), and Guti\'{e}rrez-Pe\~{n}a \index{authorindex}{Guti\'{e}rrez-Pe\~{n}a, E.} and
Smith\index{authorindex}{Smith, A. F. M.} (1995) extend this result with transformations of the mean prior.  

We can develop a general form of the posterior that corresponds to the exponential family expression of the joint likelihood function in 
\eqref{joint.expo.form}.  Start with a corresponding conjugate prior form in the same generalized notation:
\index{subjectindex}{exponential family form!prior}
\begin{equation}\label{generalized.prior}
    p(\theta|k,\gamma) = c(k,\gamma)\exp[ku(\theta)\gamma + k\log(s(\theta))].
\end{equation}
Now calculate the posterior from (\ref{generalized.prior}) and (\ref{joint.expo.form}):
\begin{align}
    \pi(\theta|\mathbf{x},k,\gamma) &\propto f(\mathbf{x}|\theta) p(\theta|k,\gamma) \nonumber \\
                    &\propto \exp\left[ u(\theta)\left(\sum_{i=1}^{n}t(x_i) + k\gamma\right)
                         + (n+k)\log(s(\theta)) \right] \nonumber \\
                    &=       \exp\left[ u(\theta)(n+k)\left(\frac{\sum_{i=1}^{n}t(x_i) + k\gamma}{n+k}\right)
                         + (n+k)\log(s(\theta)) \right].
\end{align}\index{subjectindex}{exponential family form!posterior}
So the posterior distribution is an exponential family form like the prior with parameters: 
$k'=(n+k), \gamma'=\frac{\sum_{i=1}^{n}t(x_i) + k\gamma}{n+k}$.  The connection between the exponential family form and conjugacy is a very 
useful result because if we know that the form of our likelihood function is an exponential family form, then it is almost certain to have 
a conjugate prior.  Table~\ref{conjugate.table} provides a list of common PDFs and PMFs with their associated conjugate prior.

\blstable\index{subjectindex}{distribution!common exponential family forms}
\begin{table}[h]
\begin{center}
\hspace{-55pt}
\tabletitle{\textsc{Some Exponential Family Forms and Their Conjugate Priors}}\label{conjugate.table}
\vspace{11pt}
\renewcommand{\arraystretch}{1.2}
\begin{small}
\begin{tabular}{lllll}
Likelihood Form     		& \multicolumn{2}{c}{Conjugate Prior Distribution} 
				& \multicolumn{2}{c}{Hyperparameters}        	\\
\hline
Bernoulli           		&$\qquad$& Beta          &$\,$& $\alpha>0$, $\beta>0$             		\\
Binomial            		&$\qquad$& Beta          &$\,$& $\alpha>0$, $\beta>0$             		\\
Multinomial         		&$\qquad$& Dirichlet     &$\,$& $\theta_j >0$, $\Sigma \theta_j=\theta_0$ 	\\
Negative Binomial$\qquad$       &$\qquad$& Beta          &$\,$& $\alpha>0$, $\beta>0$                 		\\
Poisson             		&$\qquad$& Gamma         &$\,$& $\alpha>0$, $\beta>0$				\\
Exponential         		&$\qquad$& Gamma         &$\,$& $\alpha>0$, $\beta>0$                         	\\
Gamma (incl. $\chi^2$)      	&$\qquad$& Gamma         &$\,$& $\alpha>0$, $\beta>0$                 		\\
Normal for $\mu$        	&$\qquad$& Normal        &$\,$& $\mu \in \mathbb{R}$, $\sigma^2>0$        	\\
Normal for $\sigma^2$       	&$\qquad$& Inverse Gamma &$\,$& $\alpha>0$, $\beta>0$             		\\
Pareto for $\alpha$     	&$\qquad$& Gamma         &$\,$& $\alpha>0$, $\beta>0$                     	\\
Pareto for $\beta$      	&$\qquad$& Pareto        &$\,$& $\alpha>0$, $\beta>0$                     	\\
Uniform             		&$\qquad$& Pareto        &$\,$& $\alpha>0$, $\beta>0$                         	\\
\end{tabular}
\end{small}
\end{center}
\end{table} \bls

Two important classes of probability density functions are not members of the exponential family.  The Student's-$t$
\index{subjectindex}{distribution!Student's-$t$} and the uniform\index{subjectindex}{distribution!normal} distribution cannot be put into the 
form of (\ref{expo.form}) above.  In general, a probability function in which the parameterization is dependent on the bounds, such as
the uniform distribution, are not members of the exponential family.  Even if a probability function is not an exponential family member, 
it can sometimes qualify under particular circumstances.

In fact, the exponential family form provides more practical Bayesian help than it appears at first.  Since the exponential family 
form guarantees a finite-dimension sufficient statistic, \index{subjectindex}{sufficient statistic} there are no difficulties in 
finding maximum likelihood estimates for unknown parameters.  This is formalized in the Darmois-Pitman-Koopman Theorem 
\index{subjectindex}{Darmois-Pitman-Koopman Theorem} (independently and almost concurrently proven by Darmois 
\index{authorindex}{Darmois, G.} [1935], Pitman\index{authorindex}{Pitman, E. J. G.} [1936], and Koopman\index{authorindex}{Koopman, L. H.} 
[1936]), which states that a distribution function possesses an associated fixed dimensional sufficient statistic for its parameter (Pitman) 
or parameter vector (Koopman) if and only if the distribution can be expressed in exponential family form (Anderson
\index{authorindex}{Anderson, E. B.} 1970, p.1248; Barankin\index{authorindex}{Barankin, E. W.} and Maitra\index{authorindex}{Maitra, A. P.} 
1963, p.217; Hipp\index{authorindex}{Hipp, C.} 1974, p.1283; Jeffreys\index{authorindex}{Jeffreys, H.} 1961, p.168).  This well-known 
property just assures us that with the exponential family form the right thing happens and that with other forms we may or may not encounter 
additional difficulties; see Bickel\index{authorindex}{Bickel, P. J.} and Doksum\index{authorindex}{Doksum, K.} (1977) or DeGroot (1986) 
\index{authorindex}{DeGroot, M. H.} for basic discussions.  Furthermore, Crain and Morgan (1975) show that Bayesian models based on 
exponential family form produce asymptotically normal posteriors.\index{authorindex}{Crain, B. R.} \index{authorindex}{Morgan, R. L.}

The theoretical linkage between sufficient statistics and the likelihood function is important for our purposes.  The minimum dimension 
sufficient statistic\index{subjectindex}{sufficient statistic!minimum dimension} is actually the smallest collection of statistics that 
completely summarizes the likelihood function (Brown\index{authorindex}{Brown, L. D.} 1964, p.1458; Fraser
\index{authorindex}{Fraser, D. A. S.} 1963, p.117).  Therefore under nonrestrictive regularity conditions, the likelihood function from 
an exponential family distribution can be characterized by a finite, usually small dimension, sufficient statistic \emph{regardless of
the sample size underlying it}.  Therefore, we gain an immense amount of inferential leverage by restricting ourselves to exponential 
family forms and summarizing the sample information through likelihood functions.  In subsequent sections, we will take advantage of 
small-dimensional sufficient statistics to simplify the analyses.

\subsection{Limitations of Conjugacy}\index{subjectindex}{conjugacy!limitations}
The primary advantage in specifying conjugate priors is their mathematical convenience in producing posterior inferences.  This is 
not, however, identical to uncovering some absolute truth in the data-generation process.  A conjugate prior, no matter how 
mathematically convenient or easily interpretable, should not be construed as \emph{the right answer} in a data-analytic exercise 
and in fact may seriously misrepresent the actual truth (Barnett\index{authorindex}{Barnett, V.} 1973, p.188).  
\index{subjectindex}{conjugacy!limitations}  It is also not the case that conjugate priors are no-information default alternatives.  
One should be particularly cautious about using conjugate priors as ignorance priors, such as normal distributions centered at zero
with small precision, in that they still indicate specific parametric prior knowledge.  If the form of the conjugate prior through 
its alternative parametric values (some are more flexible than others) fits prior knowledge about the distribution of the parameter, 
then this is a fortunate and desirable outcome rather than an inevitability.

There was a time when conjugate specifications were very important in Bayesian statistics.  Before the advent of MCMC techniques 
(Chapter~\ref{MCMC.Chapter}), many proposed models were simply too hard to estimate without some trick like conjugacy.  This is no 
longer the case, but conjugate priors can still be useful in practice and they are an excellent expository tool.

\section{Uninformative Prior Distributions}\label{uninformative.section}
An uninformative prior is one in which little new explanatory power about the unknown parameter is 
provided by intention.\index{subjectindex}{prior distribution!uninformative!definition}  Despite the unfortunate name, 
uninformative priors are very useful from the perspective of traditional Bayesianism that sought 
to mitigate frequentist criticisms of intentional subjectivity.  Consider a situation in which 
absolutely no previous subjective information is known about the phenomenon of interest.  Does 
this preclude a Bayesian analysis?  If we could make a probabilistic statement that did not favor 
any outcome over another, then it would be a simple matter to use this as a prior distribution 
allowing us to continue the analysis.  Unfortunately this is often more difficult than one would expect.

\subsection{Uniform Priors}\index{subjectindex}{prior distribution!uniform!as uninformed}\label{section.uniform.prior}
An obvious candidate for the uninformative prior is the uniform distribution.  Uniform priors are particularly easy to specify in the case of 
a parameter with bounded support.  For instance, a uniform prior for the probability parameter in a Bernoulli, binomial, or negative 
binomial model can be specified by: $p(\theta) =1, \; 0 \le \theta \le 1$, or if there is some reason to specify a non-normalized uniform: 
$p(\theta) =1, \; 0 \le \theta \le k$.  The second is non-normalized because it does not integrate to one, but as we demonstrated in 
Chapter~\ref{Model.Chapter}, this provides no problem whatsoever in Bayesian analysis. Both of these forms are referred to as \emph{proper} 
since they integrate to a finite quantity.\index{subjectindex}{prior distribution!proper}  Proper uniform priors can be specified for 
parameters defined over unbounded space if we are willing to impose prior restrictions.  Thus if it is reasonable to restrict the range of 
values for a variance parameter in a normal model, instead of specifying it over $[0\range\infty]$, we restrict it to $[0\range\nu]$ and 
can now articulate it as $p(\sigma) = 1/\nu, \; 0 \le \theta \le \nu$.

It is also possible to specify \emph{improper} uniform priors that do not possess bounded integrals and surprisingly, these result in 
fully proper posteriors under most circumstances (although this is far from guaranteed, see Section~\ref{Improper.Priors.Section}).  
\index{subjectindex}{prior distribution!uniform!improper} Consider the common case of an uninformative uniform prior for the mean of a 
normal distribution.  It would necessarily have uniform mass over the interval: $p(\theta) = c, \; [-\infty \le \theta \le \infty]$. 
Therefore to give \emph{any} nonzero probability to values on this support, $p(\theta) = \epsilon > 0$, would lead to a prior with 
infinite density: $\int_{-\infty}^{\infty}p(\theta)d\theta=\infty$.

The uniform prior is not invariant under transformation: simple transformations of the uniform prior produce a re-expression that is not 
uniform and loses whatever sense of uninformedness that the equiprobability characteristic of the uniform gives.  For example, suppose 
again that we are interested in developing an uninformative prior for a normal model variance term and specify the improper uniform prior: 
$p(\sigma) = c, \; 0 \le \sigma < \infty$.  A simple transformation that provides a parameter space over the entire real line is given by: 
$\tau=\log(\sigma)$, and the new PDF is given by applying the transformation with Jacobian ($J=|\frac{d}{d\tau}g^{-1}(\tau)|$),
to account for the rate of change difference, to the original PDF:\index{subjectindex}{prior distribution!uniform!invariance}
\begin{align}
       \tau &= g(\sigma) = \log(\sigma) \longrightarrow g^{-1}(\tau) = \sigma = e^\tau  \nonumber\\
    p(\tau) &= p(g^{-1}(\tau)) \left|\frac{d}{d\tau}g^{-1}(\tau)\right|
            = (c) \left|\frac{d}{d\tau}e^\tau\right| \propto e^\tau.                \nonumber
\end{align}
This resulting prior clearly violates even the vaguest sense of uninformedness or ``flatness,'' and makes a strong statement about values 
that are \emph{a priori} more likely than others.  Lest one think that this problem is restricted to the class of improper uniform priors, 
consider a proper uniform prior on a Bernoulli probability parameter: $f(p) = 1, \; 0 \le p \le 1$.  If we change from the probability 
metric to the odds ratio metric (fairly common), then we impose the transformation: $q=\frac{p}{1-p}$ and the new distribution
is given by:\index{subjectindex}{odds ratio metric}
\begin{align}
       q &= g(p) = \frac{p}{1-p} \longrightarrow g^{-1}(q) = \frac{q}{1+q}  		\nonumber\9
    f(q) &= f(g^{-1}(q)) \left|\frac{d}{d q}g^{-1}(q) \right|
            = (1) \left|\frac{d}{d q}\frac{q}{1+q}\right| = (1+q)^{-2}.   \nonumber
\end{align}
Once again, a straightforward change imposes a serious departure from the uniform characteristic: no prior information about $p$ does not 
imply no prior information about a simple transformation of $p$.

\begin{wrapfigure}[15]{r}{3.2in}
  \vspace{3pt}
  \parbox[c]{3in}{\epsfig{file=Images/priors.figure04.ps,width=2.1in,angle=270}}    %\\[-200pt]
  %\vspace{11pt}
  \caption{\textsc{Prior from Transformation}}
  \vspace{-254pt}
\end{wrapfigure}
This is shown in the adjacent figure (with the right-hand-tail truncated for convenience).  Notice that it is asymmetrical
and places more prior density on the region $[0\range 1]$ than on the region $[1\range 2]$, which may violate some people's
notion of symmetry around the odds ratio of one (equally probable success and failure).  Nevertheless, it is interesting to 
note that it has a symmetric property around one by area:
\begin{equation}
	\int_0^1 (1+q)^{-2} dq = \int_1^\infty (1+q)^{-2} dq = \half.
\end{equation}

\index{subjectindex}{prior distribution!uniform!history}
In the early 20th century, there was a great amount of controversy focused around the use of uniform priors and their role in
calculating ``inverse probability''\index{subjectindex}{inverse probability} (i.e., the early application of Bayes' Law).
\index{subjectindex}{prior distribution!uniform!controversy}  Fisher\index{authorindex}{Fisher, R. A.} (1930, p.531) was
characteristically negative on the subject: ``\ldots how are we to avoid the staggering falsity of saying that however extensive
our knowledge of the values of $x$ may be, yet we know nothing and can know nothing about the values of $\theta$?''  Other
predecessors and contemporaries had notable criticisms of the default uniform prior specified in Bayes\index{authorindex}{Bayes,
T.} (1763) and Laplace \index{authorindex}{Laplace, P. S.} (1774).  Boole\index{authorindex}{Boole, G.} (1854, p.370) objected to
its ``arbitrary nature,'' and Venn\index{authorindex}{Venn, J.} (1866, p.182) called it ``completely arbitrary.'' de
Morgan\index{authorindex}{de Morgan, A.} (1847, p.188) was uncomfortable with the implication that events that were observed to
occur have the same probability as events that were observed not to occur.  Edgeworth\index{authorindex}{Edgeworth, F. Y.} (1921,
pp.82-83 footnote) in comments directed at Pearson pointed out that there are many continuous prior specifications that can cancel
out in the calculation of inverse probabilities so fixating on the uniform is unnecessary, and in 1884 he may have been the first
scholar to point out the invariance property of the uniform distribution discussed above.

What is really clear from reading these early authors is that they rarely decoupled the use of uniform priors from Bayesian inference in 
general.\index{subjectindex}{Bayesian inference}  Therefore, for a long period of time, producing posterior probabilities with Bayes' Law 
was synonymous with flat prior specifications and the associated philosophical and mathematical problems that ensued.  Because of this 
inflexibility the uniform prior has long been a primary means of critiquing the Bayes-Laplace construct, and too little thought was given 
to broadening the scope of priors.  Certainly no thought was given to finding other ``no information'' priors.

Others have pointed out that the uniform priors have an inherent bias against the endpoints of the specified interval (Novick
\index{authorindex}{Novick, M. R.} and Hall\index{authorindex}{Hall, W. J.} 1965, Villegas\index{authorindex}{Villegas, C.} 1977), and 
therefore do not necessarily provide the coverage that the researcher desires (particularly if these endpoints are of theoretical 
importance as might be the case for the $\mathcal{U}(0,1)$ specification).  Nonetheless, uniform priors remain very popular in applied 
work and there are certainly situations where the uniformness is a desired property for subjective reasons rather than as an 
uninformative choice.

Bauwens\index{authorindex}{Bauwens, L.}, Lubrano\index{authorindex}{Lubrano, M.}, and Richard\index{authorindex}{Richard, J-F.} (1999) 
give some justifications for using ignorance priors like the uniform: \emph{(1)} as the sample size increases any effect from the uniform 
diminishes, \emph{(2)} they are a suitable and convenient choice for so-called nuisance parameters that are going to be integrated out of 
the posterior anyway, and (3) the uniform distribution is a limit of some conjugate prior distributions (for example, a bounded normal 
with increasing scale parameter).

\subsection{Jeffreys Prior} \index{subjectindex}{prior distribution!Jeffreys|(}
Jeffreys \index{authorindex}{Jeffreys, H.} (1961, p.181) addresses the problems associated with uniform priors by suggesting a prior 
that is invariant under transformation:
\blsref\begin{quote}%\begin{footnotesize}
    ...if we took the prior probability density for the parameters to be proportional to
    $||g_{ik}||$, it could be stated for any law that is differentiable with respect to
    all parameters in it, and would have the property that the total probability in any
    region of the $\alpha_i$ would be equal to the total probability in the corresponding
    region of $\alpha_i'$; in other words, it satisfies the rule that equivalent
    propositions have the same probability.
%\end{footnotesize}
\end{quote}\bls
He means that the transformation from $\alpha$ to $\alpha'$ is invariant with respect to 
	probabilities:\footnote{See Dawid \index{authorindex}{Dawid, A. P.} (1983) and Hartigan\index{authorindex}{Hartigan, J. A.} (1964) 
	for broader definitions of invariance.}
\index{subjectindex}{prior distribution!Jeffreys!invariance}  
\begin{equation}
    p_{J}(\alpha) = p_{J}(\alpha')\left| \frac{d\alpha'}{d\alpha} \right|.
\end{equation}
The Jeffreys prior for a single parameter, $\theta$, is produced by the
square root of negative expected value of the second derivative of $f(\x|\theta)$ (and more generally the likelihood function):
\begin{equation}\label{Jeffreys.prior}
    p(\theta) \propto \left[ -E_{\X|\theta} \left( \frac{d^2}{d\theta^2}\log f(\x|\theta) \right) \right]^\frac{1}{2}.
\end{equation}
Note that the expectation here is taken over $f(\X|\theta)$.  This is also the square root of the determinant of the familiar negative 
expected Fisher Information matrix.\index{subjectindex}{Fisher Information}  Expression \ref{Jeffreys.prior} is given in the 
single-dimension case, for a parameter vector, $\T$, the Jeffreys prior is:
\begin{equation}
    p(\T) \propto \left[ -E_{\X|\T} \left(
                  \left[ \frac{\partial}{\partial\T}\log f(\x|\T) \right]'
                  \left[ \frac{\partial}{\partial\T}\log f(\x|\T) \right]
          \right) \right]^{1/2}.
\end{equation}

The Jeffreys prior is straightforward to calculate and use for many parametric forms.  Ibrahim\index{authorindex}{Ibrahim, J. G.} and 
Laud\index{authorindex}{Laud, P. W.} (1991) give a general theoretical justification for using the Jeffreys prior with exponential family 
forms and therefore generalized linear models by showing that proper posteriors are produced.  Hartigan\index{authorindex}{Hartigan, J. A.}
(1983, p.74) tabulates a number of exponential family distributions with their associated Jeffreys prior.  Guti\'{e}rrez-Pe\~{n}a
\index{authorindex}{Guti\'{e}rrez-Pe\~{n}a, E.} and Smith\index{authorindex}{Smith, A. F. M.} (1995) relate the Jeffreys prior to 
standard conjugate priors, Poirer\index{authorindex}{Poirer, D. J.} (1994) gives the Jeffreys prior for the logit model, and Kass
\index{authorindex}{Kass, R. E.} (1989) carefully describes the full properties of the Jeffreys prior, including a geometric interpretation.

\subsubsection{Bernoulli Trials and Jeffreys Prior}\index{subjectindex}{prior distribution!Jeffreys!Bernoulli}
Consider a repeated Bernoulli trial with $x$ successes out of $n$ attempts in which we are interested in obtaining a posterior 
distribution for the unknown probability of success $p$.  The binomial PMF, for the sum of the trials, is given by:
\begin{equation}
    \mathcal{BN}(x|n,p) = \binom{n}{x}p^x(1-p)^{n-x}, \quad x=0,1,\ldots,n,\quad 0 \le p \le 1,
        \nonumber
\end{equation}
and the log likelihood is given by:
\begin{equation}
    \ell(p|n,x) = \log\binom{n}{x} + x\log(p) + (n-x)\log(1-p).
        \nonumber
\end{equation}
The first and second derivatives are given by:\\
    \begin{align}
        \frac{d}{d p} \ell(p|n,x) &= \frac{x}{p} + \frac{n-x}{1-p}(-1) \nonumber \\
                                                &= xp^{-1} - (n-x)(1-p)^{-1},         \nonumber \\
         \frac{d^2}{d p^2} \ell(p|n,x) &= -xp^{-2} - (n-x)(1-p)^{-2}(-1)(-1) \nonumber \\
                                                     &= -\frac{x}{p^2} - \frac{n-x}{(1-p)^2}. \nonumber
    \end{align}
Since $E[x]=np$, the last stage is trivial:
\index{subjectindex}{prior distribution!Jeffreys!Bernoulli model}
\begin{align}
    J = \left( E[- \frac{d^2}{d p^2} \ell(p|n,x)] \right)^{\frac{1}{2}}
      = \left( \frac{np}{p^2} + \frac{n-np}{(1-p)^2} \right)^{\frac{1}{2}}
      = \left( \frac{n}{p(1-p)} \right)^{\frac{1}{2}},   \nonumber\\[-18pt] \nonumber
\end{align} 
\noindent which suggests the prior $p(p)=p^{-\frac{1}{2}}(1-p)^{-\frac{1}{2}}$, a $\mathcal{BE}\left(\frac{1}{2},\frac{1}{2}\right)$
distribution.  This is shown in Figure~\ref{binomial.jeffreys.prior}.  Notice that this form is far from uniform in shape.  The
Jeffreys prior is frequently used, meaning that researchers either highly value the invariance property or they want a form, in
this case, that is bimodal with most of the density at the extremes.  Clearly this is not an ``uninformed'' choice and should not
be treated as a default prior distribution 
\begin{wrapfigure}[15]{r}{3.2in}
  \parbox[c]{3in}{\hspace{0.2in}\epsfig{file=Images/priors.figure03.ps,width=2.7in,height=2.9in,angle=270}}\\[-11pt]
  \vspace{11pt}
  \caption{\textsc{Binomial: Jeffreys Prior}}\label{binomial.jeffreys.prior}
  \vspace{-54pt}
\end{wrapfigure} 
for general purposes.  This highlights an important point.  So-called ``default'' priors are common forms, perhaps even conjugate, 
that some researchers gravitate to out of convenience.  This is a dangerous practice and all prior specifications should be
carefully considered in the context of the data and the model.  Since the advent of modern Bayesian computing (MCMC), analytical
limitations on prior specifications are rare, meaning that a wealth of alternatives exists for any given problem.

\subsubsection{Other Forms of Jeffreys Priors}
In the binomial case the Jeffreys prior is quite easy to calculate, but this is not always true in other cases.  Another simple form
is the Jeffreys prior for a Poisson log-likelihood function.  Starting with: \index{subjectindex}{prior distribution!Jeffreys!Poisson}
\begin{equation*}
	\ell(\lambda|\x) = -n\lambda + (\sum\x_i)\log\lambda - \log\sum(\x_i!),
\end{equation*}
taking derivatives produces:
\begin{align*}
        \frac{d}{d\lambda}\ell(\lambda|\x) &= -n + \lambda^{-1}\sum\x_i           \9
        \frac{d^2}{d\lambda^2}\ell(\lambda|\x) &= -\lambda^{-2}\sum\x_i,           
\end{align*}
followed by the negative expectation over $\x$:
\begin{equation*}
	E_{\x|\lambda}\left( - \frac{d^2}{d\lambda^2}\ell(\lambda|\x) \right) =
	E_{\x|\lambda}\left( \lambda^{-2}\sum\x_i ) \right) = (n\lambda)\lambda^{-2} \propto \lambda^{-1}.
\end{equation*}
Finally, the square root produces $p_{J}(\lambda) = \lambda^{-\half}$.

Invariance to transformation is not the only useful characteristic of the Jeffreys prior.  \emph{Jeffreys' Rule}
\index{subjectindex}{Jeffreys Rule} (Box\index{authorindex}{Box, G. E. P.} and Tiao\index{authorindex}{Tiao, G. C.} 1973, p.42)
states that any prior that is proportional to the Jeffreys prior is uninformative in the sense that it interjects as little
subjective information into the posterior as possible in terms of distributional invariance.  Furthermore, the Jeffreys prior in
the single parameter case, such as the binomial above, can be thought of as specifying a uniform distribution to the
parameterization of the random variable where the Fisher Information \index{subjectindex}{Fisher Information} is constant (Perks
1947).\index{authorindex}{Perks, W.}  Hartigan (1964) \index{authorindex}{Hartigan, J. A.} extends Jeffreys' idea to relative
parameter invariance priors and asymptotically locally invariant priors based on inverses or ratios of log-forms, but these can
get quite complicated without substantial added benefit.  These forms and a more general definition of invariance are covered 
by Dawid (1983).\index{authorindex}{Dawid, A. P.}  In addition, there is a strong debate on priors for time-series models
with unit roots, with Phillips (1991) advocating a Jeffreys prior and others (Sims 1988, Sims and Uhlig 1991) believing that
uniform distributions provide better comparisons to classical unit root distribution theory.  \index{authorindex}{Sims, C. A.}
\index{authorindex}{Uhlig, H.} \index{authorindex}{Phillips, P. C. B.}

The most useful aspect of the Jeffreys prior is that it comes from a very mechanical process, yet almost always produces an 
uninformative form, and with the invariance property.  Thus it can serve as a starting point for a more thoughtful determination 
of an uninformed prior.  It is therefore handy to have basic forms tabulated for reference, see Table~\ref{jeffreys.table}.

\blstable\index{subjectindex}{prior distribution!Jeffreys!summary}
\begin{table}[h]
\hspace{-18pt}
\parbox[c]{\linewidth}{
    \begin{center}
    \tabletitle{\textsc{Some Jeffreys Priors for Common Forms}}\label{jeffreys.table}
    \vspace{11pt}
    \renewcommand{\arraystretch}{1.2}
    \begin{small}
    \begin{tabular}{lllll}
    Likelihood Form     			& {Parameter}        				& Jeffreys Proportional \\
    \hline
    $\quad$ &&\\[-6pt]
    Bernoulli           			& $p \in [0\range 1]$				& $(p(1-p))^{-\frac{1}{2}}$	\\
    Binomial            			& $p \in [0\range 1]$				& $(p(1-p))^{-\frac{1}{2}}$     \\
    Multinomial($k$)       			& $p_i \in [0\range 1],\sum p_i=1$		& $(p_1p_2\cdots p_k)^{-\frac{1}{2}}$     \\
    Negative Binomial               	& $p \in [0\range 1]$   			& $(1-p)^{-1/2}p^{-1}$		\\
    Poisson             			& $\lambda \ge 0$				& $\lambda^{-1/2}$		\\
    Normal for $\sigma^2=1$       		& $-\infty < \mu < \infty$			& $1$				\\
    Normal for $\mu=0$        		& $\sigma^2 > 0$				& $\sigma^{-1}$			\\
    Normal $\mu,\sigma^2$ independent	& $\sigma^2 > 0$				& $\sigma^{-1}$			\\
    Normal $\mu,\sigma^2$ nonindependent    & $-\infty < \mu < \infty, \sigma^2 > 0$	& $\sigma^{-2}$			\\
    \end{tabular}
    \end{small}
    \end{center}
}
\end{table} \bls

\subsubsection{Jeffreys Prior in the Multiparameter Case}\index{subjectindex}{prior distribution!Jeffreys!multivariate}
While the Jeffreys prior is straightforward in one dimension, unfortunately it can be quite difficult in multiparameter models,
except the normal, and the researcher may be required to seek approximate solutions.  In fact it is possible to construct
multidimensional priors that violate the likelihood principle\index{subjectindex}{likelihood principle}
(Birnbaum\index{authorindex}{Birnbaum, A.} 1962, Evans, \index{authorindex}{Evans, M. J.} Fraser,
\index{authorindex}{Fraser, D. A. S.} and Monette\index{authorindex}{Monette, G.} 1986), 
and can lead to poor performing estimators even with the normal model
(Dawid,\index{authorindex}{Dawid, A. P.} Stone,\index{authorindex}{Stone, M.} and Zidek\index{authorindex}{Zidek, J. V.} 1973).
\index{subjectindex}{prior distribution!Jeffreys!problems} In some cases, multivariate Jeffreys priors can be constructed from
independent univariate forms if we are willing to live up to a strict prior independence assumption.

There are situations where Jeffreys priors can be constructed quite easily even in high dimension.  A common application is the
multivariate generalized linear model (\ref{GLM.Chapter}).  In this case common regression models can be easily specified with
Jeffreys priors, producing a closed-form posterior expression that can often be integrated to obtain marginals.  Following the
setup of Ibrahim and Laud \index{authorindex}{Ibrahim, J. G.} \index{authorindex}{Laud, P. W.} (1991), establish $\y$ as $n \times
1$ outcome vector, $\X$ as an $n \times k$ matrix of explanatory variables including a constant, and $\B$ as the $k$ coefficient
whose posterior we wish to describe.  The likelihood of interest is $L(\B|\X,\y,\phi)=\prod_{i=1}^n f(y_i|\theta_i,\phi)$.  Using
standard exponential family language, $b(\theta)$ from the canonical form, scale parameter $\phi$, and $\theta_i =
g^{-1}(\X_i'\B)$, the Fisher Information matrix for this GLM is given by the determinant operation: \index{subjectindex}{Fisher
Information}

\begin{equation}\label{glm.fisher.information}
	\I_\B \propto |\X'\Omega v(\B)\delta(\B)\X|/\phi,
\end{equation}
with:
\begin{align*}
	&\text{\underline{term}}&& \text{\underline{structure}}					&& \text{\underline{elements $(i=1:n)$}}&\\
	&\Omega 		&& n \times n\; \text{(optional) diagonal weight matrix}\qquad 	&& \omega_i				&\\
	&v(\B) 			&& n \times n\; \text{diagonal matrix}				&& d^2b(\theta_i)/d\theta_i^2	&\\
	&\delta(\B)		&& n \times n\; \text{diagonal matrix}				&& d\theta_i/d\X_i'\B		&
\end{align*}
Notice that $\delta(\B)$ expresses the link function here and would be just an identity matrix for the standard linear
model.  The general form of the Jeffrey's prior for this GLM is:
\begin{equation}
	p_{J}(\B) \propto E[\I_\B]^{\half} = |\X'\Omega v(\B)\delta(\B)\X|^{\half},
\end{equation}
producing the joint posterior distribution of the $\B$:
\begin{equation}
	\pi(\B|\X,\y,\phi) = \exp\left[ \sum_{i=1}^n \omega_i(y_i\theta_i-b(\theta_i)/\phi \right] |\X'\Omega v(\B)\delta(\B)\X|^{\half},
\end{equation}
which varies in complexity depending on the particular link function.  In the standard linear model with no weighting, the Jeffreys prior 
above simplifies down to $p_{J}(\B) \propto |\X'\X|^{\half} = c$, for some constant $c$, over the support $[-\infty\range\infty]$. 
\index{subjectindex}{prior distribution!Jeffreys|)}

\subsection{Reference Priors}\vspace{-3pt}
A reference prior is a, not necessarily flat, prior distribution such that for the given problem (only), the likelihood is data translated 
(imposed on the posterior).  \index{subjectindex}{prior distribution!reference!definition}  The distinction between a reference prior and an
uninformative prior is murky and author-dependent. Also, a number of authors refer to reference priors as ``automatic priors'' (Jeffreys
\index{authorindex}{Jeffreys, H.} 1961, Zellner\index{authorindex}{Zellner, A.} and Siow\index{authorindex}{Siow, A.} 1980).  
Box\index{authorindex}{Box, G. E. P.} and Tiao\index{authorindex}{Tiao, G. C.} (1973, p.23) define a reference prior as ``a prior which it is 
convenient to use as a standard'' and is ``dominated by the likelihood.'' Thus it need not be uninformative although some authors
see it that way (e.g., Bernardo\index{authorindex}{Bernardo, J. M.} 1979). Informally Bernardo states that ``Reference analysis
may be described as a method to derive model-based, nonsubjective posteriors, based on information-theoretical ideas, and intended
to describe the inferential content of the data for scientific communication'' (quoted in Irony and Singpurwalla [1996]).  
\index{authorindex}{Irony, T. Z.} \index{authorindex}{Singpurwalla, N. D.} \index{authorindex}{Bernardo, J. M.}
Reference priors can be enormously helpful in dealing with nuisance 
parameters (Robert\index{authorindex}{Robert, C. P.}  2001, p.136), and have been shown to possess desirable asymptotic qualities under 
certain circumstances (Ghosh\index{authorindex}{Ghosh, M.} and Mukerjee\index{authorindex}{Mukerjee, R.} 1992).  Kass
\index{authorindex}{Kass, R. E.} and Wasserman\index{authorindex}{Wasserman, L.} (1995) identify two distinct interpretations of reference 
priors: as an expression of ignorance or as a socially agreed upon standard (model specific) alternative to subjective priors, and they 
proceed to identify a litany of associated problems with the use of reference priors.

The original idea for the modern definition of a reference prior is from Bernardo\index{authorindex}{Bernardo, J. M.} (1979), who introduces 
the useful notion that a difficult parameter vector (perhaps difficult in the sense that the Jeffreys prior does not work well), can be 
segmented into two components, parameters of high interest and nuisance parameters.  \index{subjectindex}{reference prior}  This often leads 
to more complex derivations of the prior even before the data are considered (Berger\index{authorindex}{Berger, J. O.} and Bernardo
\index{authorindex}{Bernardo, J. M.} 1989), but does have the advantage that higher order problems can be dealt with systematically (Berger
\index{authorindex}{Berger, J. O.} and Bernardo\index{authorindex}{Bernardo, J. M.} 1992).  

So a reference prior for the parameters of high interest is found by minimizing the distance between the chosen likelihood and the resulting 
posterior according to some criteria like the Kullback-Leibler distance (see Section~\ref{section.with.KL} for details).  This is also 
called a \emph{dominant likelihood prior}, \index{subjectindex}{prior distribution!dominant likelihood} since it is a prior that is 
dominated by the likelihood function over the region of interest.  One example is a very diffuse normal distribution centered somewhere 
near the expected mode of the posterior.  The Zellner-Siow\index{authorindex}{Zellner, A.} \index{authorindex}{Siow, A.} (1980) prior is 
similar but more dispersed: a Cauchy distribution restricted to the range of interest.  These priors can be nearly uniform through the 
region of interest, but have the advantage of being mathematically easy to deal with.  

\index{subjectindex}{prior distribution!locally uniform}  
Another reference prior of note is the previously mentioned locally uniform prior (see Section~\ref{section.uniform.prior}).  
This is just a normalized or non-normalized uniform prior over a bounded region of the support of
the unknown parameter.  While these can be tremendously helpful as a reference (hence the name), they suffer from the invariance
problem discussed previously, the frequent difficulty in determining the bounds required to make the prior proper, the unrealistic
notion that \emph{nothing} is known in advance, and the classical arguments against the imposition of equiprobability through
uniform priors.  Where such a prior can be useful as a reference benchmark is in comparison with some subjective prior as a means
of demonstrating the posterior effect of the prior deviance from uniformness.  However, not a lot of theoretical importance should
be attached to this difference.

There is a philosophical problem with such reference priors in the social sciences.  To what degree is the model actually Bayesian
if the goal is to reduce the influence of the prior to zero or near zero?  Such a model still retains the Bayesian interpretation
of a posterior, but it removes the ability to include a wealth of information that is usually available in the social
sciences. In fact, suppose that one simply ran a maximum likelihood estimation on some non-Bayesian specification and declared
it equivalent to a Bayesian model with the ``best possible'' reference prior.  This would allow our hypothetical researcher to
discuss results probabilistically (HPD regions, quantiles, etc.) without the trouble of being specific about prior
specification.  For most researchers in this area, the idea runs counter to the core tenets of Bayesian inferences and would
create an artificial class of ``lazy Bayesians,'' which is surely not in the interest of scientific progress in any field.

\subsection{Improper Priors}\label{Improper.Priors.Section}
As discussed previously, it is possible to specify so-called improper priors.  These are prior distributions that do not add (PMF
case) or integrate (PDF case) to a finite value.  \index{subjectindex}{prior distribution!improper!definition}  These improper
forms typically arise in the search for an uninformative prior specification.  An interesting and common case is the normal
distribution with the improper prior: $p(\mu) = c, \; -\infty < \mu < \infty$.  Obviously the equality in this specification can
be replaced with proportionality as the integral is infinity in any case.  We can now produce the posterior distribution for $\mu$
under the assumption that $\sigma^2$ is known (assumed simply for the clarity of exposition).  We will also not use the standard
Bayesian convenience of reducing notational complexity by using proportionality until the last step.  This is done in order to
make the point about the form of the posterior more emphatic.  The posterior for $\mu$ is produced by:
\begin{align}
    p(\mu|\x,\sigma_0) &= \frac{ p(\mu)L(\mu|\x,\sigma_0) }{\int p(\mu)L(\mu|\x,\sigma_0) d\mu } \nonumber\\[7pt]
        &= \frac{ c(2\pi\sigma_0^2)^{-n/2}\exp\left[-\frac{1}{2\sigma_0^2}\sum_{i=1}^{n}(x_i-\mu)^2\right] }
                { \int c(2\pi\sigma_0^2)^{-n/2}\exp\left[-\frac{1}{2\sigma_0^2}\sum_{i=1}^{n}(x_i-\mu)^2\right] d\mu } 
		\nonumber\\[7pt]
        &= \frac{ c(2\pi\sigma_0^2)^{-n/2}\exp\Biggl[ -\frac{1}{2\sigma_0^2}(\sum_{i=1}^{n}x_i^2 -n\bar{x}^2)   
                  -\frac{1}{2\sigma_0^2}(n\mu^2 - 2n\bar{x}\mu + n\bar{x}^2) \Biggr] }
                { \int c(2\pi\sigma_0^2)^{-n/2}\exp\Biggl[ -\frac{1}{2\sigma_0^2}(\sum_{i=1}^{n}x_i^2 -n\bar{x}^2)   
                  -\frac{1}{2\sigma_0^2}(n\mu^2 - 2n\bar{x}\mu + n\bar{x}^2) \Biggr] d\mu }
		\nonumber\\[7pt]
%\end{align}
%continuing\ldots
%\begin{align}
        &= \frac{ c(2\pi\sigma_0^2)^{-n/2}\exp\left[ -\frac{1}{2\sigma_0^2}(\sum_{i=1}^{n}x_i^2 -n\bar{x}^2) \right] 
                  \exp\left[ -\frac{n}{2\sigma_0^2}(\mu-\bar{x})^2 \right] }
                { c(2\pi\sigma_0^2)^{-n/2}\exp\left[ -\frac{1}{2\sigma_0^2}(\sum_{i=1}^{n}x_i^2 -n\bar{x}^2) \right] 
                  \int \exp\left[ -\frac{n}{2\sigma_0^2}(\mu-\bar{x})^2 \right] d\mu }
		\nonumber\\[7pt]
        &= \frac{ \exp\left[ -\frac{n}{2\sigma_0^2}(\mu-\bar{x})^2 \right] }
                { \int \exp\left[ -\frac{n}{2\sigma_0^2}(\mu-\bar{x})^2 \right] d\mu }
		\nonumber\\[7pt]
        &\propto \exp\left[ -\frac{n}{2\sigma_0^2}(\mu-\bar{x})^2 \right].
\end{align}
So it is clear that the posterior for $\mu$ is a non-normalized, but proper $\mathcal{N}(\bar{x},\sigma_0^2/n)$ distribution.  It
may be surprising that an improper prior distribution can still lead to a proper posterior distribution, and conversely that
data-dependent proper priors can become improper priors in the limit (Akaike\index{authorindex}{Akaike, H.} 1980).  Also, improper
priors do not necessarily imply a lack of attention to prior specifications and some authors prefer these forms as diffuse
alternatives without heavy parametric decisions (Taralsden and Lindqvist 2010).
\index{authorindex}{Taralsden, G.}\index{authorindex}{Lindqvist, B. H.} 
\index{subjectindex}{posterior distribution!proper from improper prior}

This example suggests a compromise between the artificiality of conjugate priors and the frequent
difficulties of uninformed priors.   Recall that the posterior distribution of the mean parameter in the
conjugate normal model with the variance known is:
\begin{equation}\label{normal.posterior.example}
    \mu \sim \mathcal{N}\left[ \left(\frac{m}{s^2} + \frac{n\bar{x}}{\sigma_0^2}\right)\bigg/\left(\frac{1}{s^2}
     + \frac{n}{\sigma_0^2}\right),
     \left(\frac{1}{s^2} + \frac{n}{\sigma_0^2}\right)^{-1} \right].
\end{equation}
If an improper prior for $\mu$ is  specified as $\mathcal{N}(m=0,s^2=\infty)$, then the resulting posterior
is clearly proper $\mathcal{N}(\bar{x},\sigma_0^2/n)$.  The compromise suggested is to make the prior
variance ($s^2$) very large so that the prior is normal and proper but very spread out.  This is often
considered a ``conservative'' choice of prior since the relative probability structure is quite flat. These
\emph{diffuse priors} or \emph{vague priors} are quite popular in hierarchical models where there are many
regression-style parameters of only moderate interest.
\index{subjectindex}{prior distribution!diffuse/vague}
\index{subjectindex}{prior distribution!improper}

The best rationale for improper priors, provided that there is appreciable substantive motivation for
assigning them, is that if the model is set up so that the likelihood dominates the prior to such an extent
that the posterior is still proper, then their use is not harmful.  However, the only way to get an improper
posterior is to specify an improper prior, and this decision rests entirely with the researcher.
\index{subjectindex}{prior distribution!improper}
\vspace{-11pt}

\section{Informative Prior Distributions}\index{subjectindex}{prior distribution!informative|(}
This section introduces several forms of informed (or informative) priors.  In this chapter we looked closely at conjugate prior 
specifications, which are generally informed (although specifying $\infty$ for various parameters can dilute such qualities).
Informative priors are those that deliberately insert information that researchers have at hand.  On one level this
seems like a reasonable and reasoned approach since previous scientific knowledge should play a role in statistical
inference.  The key concern
that some readers, reviewers, and editors harbor is that the author is deliberately manipulating prior information 
to obtain a desired posterior result.  Therefore there are two important requirements to any written research using
informative priors: overt declaration of prior specifications, and detailed sensitivity analysis to show the effect
of these priors relative to uninformed types.  The latter requirement is the subject of Chapter~\ref{Model.Quality.Chapter},
and the former requirement is discussed periodically in this section.

So where do informative priors come from?  Generally there is an abundance of previous work in the social and behavioral
sciences that can guide the researcher, including her own.  So generally, these priors are derived from:
\begin{bayeslist}
        \item   previous studies, published work,
        \item   researcher intuition,
        \item   interviewing substantive experts,
        \item   convenience through conjugacy, 
        \item   nonparametrics and other data derived sources,
\end{bayeslist}
which can obviously be overlapping definitions.  Prior information from previous studies need not be in agreement.  
One fruitful strategy is to construct prior specifications from competing intellectual strains in order to contrast the 
resulting posteriors and say something informed about the relative strength of each.  The last item on this list can
be productive if the data used are distinct from that at hand to be used to construct the likelihood functions.  There
is considerable controversy, otherwise, about ``double-use'' of the data.

\subsection{Power Priors}\index{subjectindex}{prior distribution!power}
Ibrahim and Chen (2000a) \index{authorindex}{Ibrahim, J. G.} \index{authorindex}{Chen, M-H.} introduce an informed prior that explicitly 
uses data from previous studies (also discussed by Ibrahim, Chen, and Sinha [2003] as well as Chen and Ibrahim [2006]).  Their idea is 
to weight data from earlier work as 
input for the prior used in the current model.  Define $\x_0$ as these older data and $\x$ as the current data.  Their primary 
application is to clinical trials for AIDS drugs where a considerable amount of previous data exist.  In a social science context, 
there are many settings where previous research informs extant model specifications.  Our interest centers on the unknown parameter 
$\theta$, which is studied in both periods.  Specify a regular prior for $\theta$, $p(\theta)$ that would have been used un-modified if 
the previous data were not included.  This can be a diffuse prior if desired, although it will become informed through this process.  

An elementary power prior is created by updating the regular prior with a likelihood function from the previous data in a very simple
manner, which is scaled by a value $a_0 \in [0\range 1]$:
\begin{equation}\label{power.prior.basic}
	p(\theta|\x_0,a_0) \propto p(\theta) [L(\theta|\x_0)]^{a_0}.
\end{equation}
It is important to remember that this is still a \emph{prior} form and the regular process follows wherein the posterior is obtained by 
conditioning this distribution on the data through the likelihood function based on the current data:
\begin{equation}
	\pi(\theta|\x,\x_0,a_0) \propto p(\theta|\x_0,a_0) L(\theta|\x).
\end{equation}
The parameter $a_0$ scales our confidence in the similarity or applicability of the previous data for current inferences.  If it is close 
to zero then we do not particularly value the older observations or studies, and if it is close to one then we believe strongly in the 
ties to the current data.   So lower values favor the regular prior specification, $p(\theta)$, and the choice of this parameter can be 
very influential.  Note that we would not want this value to exceed one, since that would be equivalent to valuing older over newer data.

To reduce the influence of a single choice for $a_0$, we specify a mixture of these priors using a specified distribution for this 
parameter, $p(a_0|.)$.  Thus \eqref{power.prior.basic} becomes
\begin{align}\label{power.prior.mixture}
	p(\theta|\x_0) &= \int_0^1 p(\theta|\x_0,a_0) p(a_0|\cdot)da_0				\nonumber\9
	               &= \int_0^1 p(\theta) [L(\theta|\x_0)]^{a_0} p(a_0|\cdot)da_0.
\end{align}
Here the parameterization for $p(a_0|\cdot)$ is left vague since its parametric form remains undefined.  Chen and Ibrahim
\index{authorindex}{Ibrahim, J. G.} \index{authorindex}{Chen, M-H.} recommend a beta distribution as the ``natural'' choice, but point 
out that truncated forms of the normal or gamma work as well.  The mixture specification has the effect of inducing heavier tails in 
the marginal distribution of $\theta$ and thus represents a more conservative choice of prior.

\subsection{Elicited Priors}\index{subjectindex}{prior distribution!elicited}
A completely different class of priors is derived not from real or desired mathematical properties, but from previous human knowledge on 
the subject of investigation.  These elicited priors are discussed in detail in Gill and Walker \index{authorindex}{Gill, J.} 
\index{authorindex}{Walker, L.} (2005), with a detailed application to attitudes towards the judicial system in Nicaragua.  Typically the 
source for elicited priors is from subject area experts with little or no concern for the statistical aspects of the project.  These 
include physicians, policy-makers, theoretical economists, and qualitative researchers in various fields.  However, there is no reason 
that politicians, study participants, outside experts, or opinion leaders in general could not be used as a source for informative priors 
as well.   \index{subjectindex}{prior distribution!elicited!definition} \index{subjectindex}{prior distribution!informative}

The bulk of the published work on elicited priors is on the Bayesian analysis of clinical trials.
\index{subjectindex}{prior distribution!elicited!clinical trials} \index{subjectindex}{clinical trials}  In these settings, it is typical 
to elicit qualitative priors from the clinicians as a means of incorporating local expertise into the calculations of posteriors and trial 
stopping points (Freedman\index{authorindex}{Freedman, L. S.} and Spiegelhalter\index{authorindex}{Spiegelhalter, D. J.} 1983, Kadane
\index{authorindex}{Kadane, J. B.} 1986, Spiegelhalter, Abrams, and Myles 2004, Chapter 5).\index{authorindex}{Abrams, K. R.}
\index{authorindex}{Spiegelhalter, D. J.} There is also a small literature on elicitation of priors for variable selection 
(Garthwaite\index{authorindex}{Garthwaite, P. H.} and Dickey\index{authorindex}{Dickey, J. M.} 1988, 1992; 
Ibrahim\index{authorindex}{Ibrahim, J. G.} and Chen\index{authorindex}{Chen, M-H.} 2000b).  Here we will concentrate on the more basic task 
of using elicitation to specify a particular parametric form for the prior.  It is relatively common to use conjugate priors or mixtures 
of conjugate priors for this task so as to remove additional complications.  However, this is certainly not a mathematical or theoretical 
restriction.

Although an overwhelming proportion of the studies employing elicited priors are in the 
medical and biological sciences, the methodology is ideal for a wide range of social 
science applications.  In virtually every field and subfield of the various disciplines 
there are practicing ``experts'' whose opinions can be directly or indirectly elicited.  
\index{subjectindex}{prior distribution!elicited!expert opinions}  Furthermore, the fact that the social and 
behavioral sciences are focused on varying aspects of human behavior means that describing 
current knowledge and thinking about some specific behavioral phenomenon probabilistically 
is a more realistic way to incorporate disparate judgments.  Restated, uncertain and 
divided opinion is better summarized in probabilistic language than with deterministic 
alternatives.  

The central challenge here is how to translate expert knowledge into a specific probability statement.  This process ranges from informal 
assignments to detailed elicitation plans and even regression analysis across multiple experts (Johnson\index{authorindex}{Johnson, V. E.} 
and Albert\index{authorindex}{Albert, J. H.} 1999, Chapter 5).  Spetzler\index{authorindex}{Spetzler, C. S.} and Sta\"{e}l von Holstein
\index{authorindex}{Sta\"{e}l von Holstein, C. S.} (1975) outline three general steps in the process:
\index{subjectindex}{prior distribution!elicited!three phases}
\begin{enumerate}
    \item   {\bf Deterministic Phase.}  The problem is codified and operationalized into specific variables and definitions.
    \item   {\bf Probabilistic Phase.}  Experts are interviewed and tested in order to assign probabilistic values to specific outcomes.
    \item   {\bf Informational Phase.}  The assigned probabilities are tested for inconsistencies and completeness is verified.
\end{enumerate}

The deterministic phase includes specifying the explanatory variables and possibly their assumed parametric role in the model (Steffey
\index{authorindex}{Steffey, D.} 1992), determining data sources and data collection processes fitted to this methodology (Garthwaite
\index{authorindex}{Garthwaite, P. H.} and Dickey\index{authorindex}{Dickey, J. M.} 1992), determining how many experts to query and 
where to find these experts (Carlin,\index{authorindex}{Carlin, B. P.} \etal 1993), and finally judging their contributions (Hogarth
\index{authorindex}{Hogarth, R. M.} 1975).  Some of this work is far from trivial: experts might need to be trained prior to elicitation 
(Winkler\index{authorindex}{Winkler, R. L.} 1967), variable selection can be influenced by the difficulty of elicitation (Garthwaite
\index{authorindex}{Garthwaite, P. H.} and Dickey\index{authorindex}{Dickey, J. M.} 1992), and cost projections can be difficult.

The informational phase is somewhat mechanical and it includes testing elicitation responses 
for consistency, calibrating responses with known data, and perhaps weighting expert opinions.  
Determining consistency is an important requirement and experts differ in their familiarity 
with the details of the project at hand.  Less experienced respondents tend to show more 
inconsistencies (especially with continuous rather than discrete choices), and more 
experienced respondents as well as normative experts show high levels of consistency 
(Hogarth\index{authorindex}{Hogarth, R. M.} 1975, 
Winkler\index{authorindex}{Winkler, R. L.} 1967).  By consistency 
it is meant that answers do not contradict each other, for instance, the subset of an event 
having a higher probability than the event itself.  Calibration generally involves comparison 
of results after the rest of the analysis and can be a safety check for future work as well 
as a confirmation of the reliability of the experts (Seidenfeld\index{authorindex}{Seidenfeld, T.} 1985).  
Sometimes these checks are further complicated when the subject is a rapidly changing area 
and the experts' earlier statements can quickly become outdated (Carlin, \index{authorindex}{Carlin, B. P.}
\etal 1995).  Leamer (1992) also gives a diagnostic approach that helps categorize elicitations into blunt
responses removing the necessity of further inquiry.  \index{authorindex}{Leamer, E. E.}

By far the most challenging is the probabilistic phase and this has consumed the bulk of the literature.  For instance, the
experts can be asked fixed value (``P-methods'') and/or fixed probability (``V-methods'') questions where specific estimates of
the probability or relative likelihood of events are queried (Spetzler and Sta\"{e}l von Holstein 1975, p.347).
\index{authorindex}{Spetzler, C. S.} \index{authorindex}{Sta\"{e}l von Holstein, C. S.} In addition, the experts can be asked
these questions directly with regard to a cumulative density function (CDF), or indirectly by way of physical devices or
hypothetical constructions.  A more challenging, but perhaps informative, approach is to ask open-ended questions and code the
response.  In all of these cases, it is important to clarify to experts that they are giving probability estimates rather than
utility assessments (Kadane\index{authorindex}{Kadane, J. B.} and Winkler\index{authorindex}{Winkler, R. L.} 1988).  The concern
is that these experts will otherwise express their normative ideals about outcomes, and preferred outcomes will be given
unrealistic probabilities.  \index{subjectindex}{prior distribution!informative}

In general it is not feasible to ask subject-matter experts to make determinations about coefficient estimates or about moments for 
specified PDFs and PMFs convenient to the statistician.  So the most common strategy, dating back to the seminal paper of Kadane 
\index{authorindex}{Kadane, J. B.} \etal (1980), is to query these experts about outcome variable quantiles for given (hypothetical) 
levels of explanatory variables.\index{subjectindex}{expert elicitation}  For example, in one study an emergency room physician is asked 
about survival probabilities of patients with specified injury type, injury severity score, trauma score, age, and type of injury 
(Bedrick, Christensen, and Johnson 1997).  The idea is then to take these quantiles and solve for the parameters of an assumed 
distributional form for the (often conjugate) prior.  \index{subjectindex}{eliciting survival probability}
\index{authorindex}{Bedrick, E. J.} \index{authorindex}{Christensen, R.} \index{authorindex}{Johnson, W.}

One particularly simple application is in the case of a binomial outcome.  For psychological reasons, it appears to be easier to elicit 
hypothetical binary outcomes.  Using the beta conjugate prior several authors have suggested algorithms for elicitation (Chaloner and 
Duncan 1983, 1987; Gavasakar 1988).\index{authorindex}{Chaloner, K.} \index{authorindex}{Duncan, G. T.} \index{authorindex}{Gavasakar, U.}
The basic process is to hypothesize a fixed set of Bernoulli trials, ask the expert to give a most 
likely number of successes given the particular scenario and reasonable bounds on the uncertainty,\index{subjectindex}{uncertainty!prior} 
work these values backward into the beta-binomial PMF to get the beta parameters, and finally show the expert the posterior implications 
of these values.  If they are found to be unreasonable, then adjustments are made and the process repeats itself.  

\subsubsection{The Community of Elicited Priors}\index{subjectindex}{elicited prior distribution!types}
The priors that are elicited from experts can have a variety of characterizations.  Kass and Greenhouse (1989)
                \index{authorindex}{Kass, R. E.}
                \index{authorindex}{Greenhouse, J. B.}
coined the phrase ``community of priors'' to describe the range of attitudes that equally qualified experts 
may have about the same phenomenon.  These can be categorized as well:
\begin{bayeslist}
	\item	{\bf Clinical Priors.}	\index{subjectindex}{elicited prior distribution!clinical}
		These are priors elicited from substantive experts who are taking part in the 
		research project.  This is often done because these individuals are easily 
		captured for interviews and are motivated by a direct stake in the outcome.

	\item	{\bf Skeptical Priors.}	\index{subjectindex}{elicited prior distribution!skeptical}
		These are priors built with the assumption that the hypothesized effect does not
		actually exist and are usually operationalized with a zero mean.  Skeptical priors
		can be created because of actual skepticism or because overcoming such a prior 
		provides stronger evidence: ``\ldots set up as representing an adversary who will need
		to be disillusioned by the data \ldots'' (Spiegelhalter \etal 1994, Spiegelhalter, Abrams, and Myles 2004).
                \index{authorindex}{Spiegelhalter, D. J.}
                \index{authorindex}{Abrams, K. R.}
                \index{authorindex}{Myles, J. P.}

	\item	{\bf Enthusiastic Priors.}	\index{subjectindex}{elicited prior distribution!enthusiastic}
		These are obviously the opposite of the skeptical prior.  The priors are built
		around the positions of partisan experts or advocates and generally assume the
		existence of the hypothesized effect.  For comparative purposes, enthusiastic
		priors can be specified with the same variance, but different mean, as 
		corresponding skeptical priors.	

	\item	{\bf Reference Priors.}\index{subjectindex}{elicited prior distribution!reference}
		Such priors are occasionally produced from expert sources, but they are somewhat 
		misguided because the purpose of elicitation is to glean information that can
		be described formally.
\end{bayeslist}

The priors are restated from Spiegelhalter \index{authorindex}{Spiegelhalter, D. J.} \etal (1994) in order to be less focused on the 
application to clinical trials.\index{subjectindex}{clinical trials} The key point is to understand the differing perspectives of experts.  
One approach is to contrast the posterior results obtained from divergent prior perspectives, including a formalized process of overcoming 
adversarial prior specifications in favor of priors more sympathetic to research questions through additional sampling (Lindley and 
Singpurwalla 1991), \index{authorindex}{Lindley, D. V.} \index{authorindex}{Singpurwalla, N. D.} randomization strategies (Kass and 
Greenhouse 1989), \index{authorindex}{Kass, R. E.} \index{authorindex}{Greenhouse, J. B.} scoring rules (Savage 1971), or other means.
\index{authorindex}{Savage, L. J.}

\subsubsection{Simple Elicitation Using Linear Regression}
An analyst asks an expert for predictions on an expected outcome for some interval-measured event of interest.  The V-method question 
asked is: what would be an expected low value as a $0.25$ quantile (labeled $x_{0.25}$) and an expected high value as a $0.75$ quantile 
(labeled $x_{0.75}$)?  These two supplied quantile values, $x_{0.25}$ and $x_{0.75}$, correspond to normal z-scores 
$z_{0.25}=-0.6745$ and $z_{0.75}=0.6745$, which specify the shape of a normal PDF since there are two equations and two unknowns:
\begin{equation}\label{basic.normal.quantiles}
	z_{0.25} = \frac{x_{0.25} - \alpha}{\beta} \qquad\qquad z_{0.75} = \frac{x_{0.75} - \alpha}{\beta}.
\end{equation}
Here $\alpha$ and $\beta$ are the mean and standard deviation parameters of the normal PDF: 
\begin{equation}\label{normal.alpha.beta}
    f(x|\alpha,\beta) = (2\pi\beta^2)^{-\frac{1}{2}} \exp\left[-\frac{1}{2\beta^2} (x-\alpha)^2\right].  
\end{equation}
This notation for the parameters of
a normal is different, but the reason shall soon become apparent.  When we solve for $\alpha$ and $\beta$ in
\eqref{basic.normal.quantiles}, we have a fully defined prior distribution in \eqref{normal.alpha.beta} and the elicitation is complete.

Of course, one expert is typically not enough to produce robust prior forms, so now query experts $1,2,\ldots,J$.  This produces an 
over-specified series of equations since there are $J \times 2$ equations and only two unknowns (Spiegelhalter \etal [1994],
for instance,  use $J=10$).  It is necessary to assume that these experts are \emph{exchangeable} meaning that they all provide equal 
quality elicitations.   

Secondly, given the cost of interviewing, we are likely to ask each expert for more than just these two 
quantiles.  Each assessor is asked to give five quantile values at $m = [0.01,0.25,0.5,0.75,0.99]$ corresponding to standard normal 
points $z_{m}$. At this point, \eqref{basic.normal.quantiles} can be re-expressed for the quantile level $m$ given by assessor $j$: 
$x_{jm} = \alpha + \beta z_{jm}$, and the total amount of expert-elicited information constitutes the following over-specification 
($J\times 5$ equations and 2 unknowns) of a normal distribution:
%\newpage
\begin{align*}
	& x_{11} = \alpha + \beta z_{11} & x_{21} = \alpha + \beta z_{21} & \ldots 
	& x_{(J-1)1} = \alpha + \beta z_{(J-1)1} && x_{J1} = \alpha + \beta z_{J1} \9
	& x_{12} = \alpha + \beta z_{12} & x_{22} = \alpha + \beta z_{22} & \ldots 
	& x_{(J-1)2} = \alpha + \beta z_{(J-1)2} && x_{J2} = \alpha + \beta z_{J2} \9
	& x_{13} = \alpha + \beta z_{13} & x_{23} = \alpha + \beta z_{23} & \ldots 
	& x_{(J-1)3} = \alpha + \beta z_{(J-1)3} && x_{J3} = \alpha + \beta z_{J3} \9
	& x_{14} = \alpha + \beta z_{14} & x_{24} = \alpha + \beta z_{24} & \ldots 
	& x_{(J-1)4} = \alpha + \beta z_{(J-1)4} && x_{J4} = \alpha + \beta z_{J4} \9
	& x_{15} = \alpha + \beta z_{15} & x_{25} = \alpha + \beta z_{25} & \ldots 
	& x_{(J-1)5} = \alpha + \beta z_{(J-1)5} && x_{J5} = \alpha + \beta z_{J5} 
	\nonumber 
\end{align*}
The approach suggested by this setup is to run a simple linear regression to estimate $\alpha$ as the intercept and $\beta$ as the slope.  
There are two issues to worry about.  One must check for logical inconsistencies in consistent quantile values for each assessor 
(see Lindley, Tversky, and Brown \index{authorindex}{Lindley, D. V.} \index{authorindex}{Tversky, A.} \index{authorindex}{Brown, R. V.}
[1979] for a discussion of problems).  Also, it is critical to apply necessary mathematical constraints such as ensuring that the 
estimated coefficient for $\beta$ remains positive (if substantively required) since the basic linear model imposes no such restriction 
(Raiffa and Schlaifer 1961). \index{authorindex}{Raiffa, H.} \index{authorindex}{Schlaifer, R.}

\begin{examplelist}
	\item	{\bf Eliciting Expected Campaign Spending.}  	\index{subjectindex}{example!campaign spending}
	We are interested in eliciting a prior distribution for expected campaign contributions received by major-party candidates in an 
	impending U.S. Senate election in order to specify an encompassing Bayesian model.  Elicitation replaces data here since the
	election has not yet taken place.  Eight campaign experts are queried for quantiles at levels $m = [0.1, 0.5, 0.9]$, and they 
	provide the following values reflecting the national range of expected total intake by Senate candidates (in thousands):
	\begin{align*}
		& x_{11} = 400 		&& x_{12} = 2500 	&& x_{13} = 4000 	& \\
		& x_{21} = 150  	&& x_{22} = 1000  	&& x_{23} = 2500  	& \\
		& x_{31} = 300 		&& x_{32} = ~900  	&& x_{33} = 1800 	& \\
		& x_{41} = 250  	&& x_{42} = 1200 	&& x_{43} = 2000 	& \\
		& x_{51} = 450 		&& x_{52} = 1800 	&& x_{53} = 3000 	& \\
		& x_{61} = 100 		&& x_{62} = 1000 	&& x_{63} = 2500 	& \\
		& x_{71} = 500  	&& x_{72} = 2100  	&& x_{73} = 4200  	& \\
		& x_{81} = 300 		&& x_{82} = 1200 	&& x_{83} = 2000	& \\
	\end{align*}
	where $x_{83}$ is expert $8$'s third quantile.  None of the experts have supplied quantile values out of logical order, so 
	these results are consistent.  Using these ``data'' we regress $x$ on $z$ to obtain the intercept and slope values: 
    \begin{R.Code}
x <- c( 400, 2500, 4000,  150, 1000, 2500,  300,  900, 
       1800,  250, 1200, 2000,  450, 1800, 3000,  100, 
       1000, 2500,  500, 2100, 4200,  300, 1200 ,2000)
z <- qnorm(rep(c(0.1,0.5,0.9),8))
summary(lm(x~z))
    \end{R.Code} 
    \vspace{-15pt}
    which returns:
    \vspace{-5pt}
    \begin{R.Code}
            Estimate Std. Error t value Pr(>|t|)
(Intercept)   1506.3      127.0  11.858 4.99e-11
qnorm(y)       953.4      121.4   7.854 8.00e-08
    \end{R.Code}
    \vspace{-5pt}
    Thus the normal prior mean is $\hat{\alpha} = 1506.3$ and the normal prior standard deviation is $\hat{\beta} = 953.4$.
\end{examplelist}

\subsubsection{Variance Components Elicitation}\index{subjectindex}{variance components elicitation|(}
A problem with direct quantile elicitation is that assessors often misjudge the probability of unusual values because it is more 
difficult to visualize and estimate tail behavior than to estimate means or medians.  Hora \etal \index{authorindex}{Hora, S. C.} 
(1992) found that when non-technical assessors are asked to estimate spread by providing high probability coverage intervals such as at 
99\%, then they tend to perceive this as near-certainty coverage and overstate the bounds. But this finding is not universal: in other 
settings people tend to think of rare events in the tails of distributions as more likely than they really are (an effect exploited by 
casinos and lotteries).  Accordingly, O'Hagan \index{authorindex}{O'Hagan, A.} (1998) improves elicited estimates of spread by 
separately requiring assessors to consider two types of uncertainty: 
\begin{bayeslist}
	\item	uncertainty about an estimate relative to an assumed known summary statistic 
	\item	secondary uncertainty of this summary only.  
\end{bayeslist}
The elicitee first gives a (modal) point estimate for the explanatory variable coefficient, $\tau$, and is then asked ``given your 
recent estimate of $\tau$, what is the middle 50\% probability interval around $\tau$?''  The elicitees must understand that this is 
the interval that contains the middle half of the expected values.  So this (V-method) specifies a density estimate centered at the 
assessors modal point, and if the form of the distribution is assumed or known, then the exact value for the variance can be calculated
under a distributional assumption (normal, Student's-$t$, or a log-normal if a right-skewed interval is required).
\index{subjectindex}{posterior distribution!Student's-$t$}

O'Hagan\index{authorindex}{O'Hagan, A.}  (1998) prefers asking for the middle 66\% of the density, which he calls the ``two-to-one 
interval'' since the middle coverage is twice that of the combined tails.  Now if a normal prior is used then this interval quickly 
yields a value for the standard deviation since it covers approximately two of them.  It should actually be multiplied by $\frac{68}{66}$ 
to be exactly correct but analysts often do not worry about the difference since measurement error is almost certainly greater than the 
difference.  Now the researcher calculates the implied variance from this and shows the assessor credible intervals at familiar 
$(1-\alpha)$-levels.  If these are deemed by the elicitee to be too large or too small, then the process is repeated. 

We want to elicit prior distributions for $\tau_i$ across $n$ cases, with unknown total $T=\sum_{i=1}^{n}\tau_i$. The assessor first 
provides point estimates for each case: $x_1,x_1,\ldots,x_n$, so that the estimated total is given by $x_T=\sum_{i=1}^{n}x_i$. These 
are useful values, but it is still necessary to get a measure of uncertainty in order to produce a variance for the full elicited prior 
distribution.

The individual deviance of the $i$th estimate from its true value can be rewritten algebraically:\vspace{-3pt}
\begin{equation}\label{vc1.equation}
	\tau_i - x_i = \left( \tau_i-\frac{x_i}{x_T}T \right) + \frac{x_i}{x_T}\left(T - x_T \right).
\end{equation} 
The first quantity on the right-hand side of \eqref{vc1.equation} is the deviance of $\tau_i$ from an estimate that would be provided 
if we knew $T$ for certain:
\begin{equation}\label{expected.tau}
	E[\tau_i|T] = \frac{x_i}{x_T} T
\end{equation}
(which can be considered as between-case deviance).  Now the second quantity on the right-hand side of \eqref{vc1.equation} is the 
weighted deviation of $T$, i.e., uncertainty about the true total.  The expected value form \eqref{expected.tau} helps us obtain the 
variance of $\tau_i$:
\begin{align}\label{vc2.equation}
	\Var(\tau_i) &= E\left[ \Var(\tau_i|T) \right] + \Var\left( E[\tau_i|T] \right) \nonumber\9
	&= E\left[ \tau_i - \frac{x_i}{x_T} T \right]^2 
	+ \left( E\left[E[\tau_i|T]^2\right] - (E\left[E[\tau_i|T]\right])^2 \right) \nonumber\9
	&= E\left[ \tau_i - \frac{x_i}{x_T} T \right]^2 
	+ \left(\frac{x_i}{x_T}\right)^2 \Var(T),
\end{align}
which shows the general form of the two variance components.  A better form for elicitation is achieved by dividing both sides of this 
equation by $x_i^2$:
\begin{equation}\label{vc3.equation}
	\Var\left( \frac{\tau_i}{x_i} \right) = \Var\left( \frac{\tau_i}{x_i} - \frac{T}{x_T} \right) + \Var\left(\frac{T}{x_T}\right).
\end{equation}
At this point elicitees are queried about the middle spread around these two quantities individually.  \emph{First}, they give an 
estimate of middle spread around $\frac{T}{x_T}$, assuming accuracy of the sum $x_T$ as an estimate of $T$.  \emph{Then}, they give the middle 
spread around each $\frac{\tau_i}{x_i}$ assuming that $\frac{T}{x_T}=1$.  This means that there is no second component in the variance to 
consider at this moment.  Once the individual means and variances are elicited, these $\frac{x_i}{x_T}$ values are put into the assumed 
distribution defined over $[0\range 1]$ (they are proportions) to form the complete prior distribution specification.  Two common
distributional forms are the normal CDF and the beta distribution.  In the case where $\frac{\tau_i}{T}$ is from a beta distribution, 
we can solve for the parameters with the beta distribution mean and variance: 
$E\left[\frac{\tau_i}{T}\right] = \frac{\alpha}{\alpha+\beta}$, \
$\Var\left(\frac{\tau_i}{T}\right) = \frac{\alpha\beta}{(\alpha+\beta)^2(\alpha+\beta+1)}$.

\begin{examplelist}
	\item	{\bf Minority Political Participation.}	\index{subjectindex}{example!minority political participation}
	This example is from Gill and Walker (2005). \index{authorindex}{Gill, J.} \index{authorindex}{Walker, L.}
	An expert on minority electoral participation is asked to estimate upcoming Hispanic turnout for $n$ precincts in a given 
	district: $\tau_1,\tau_2,\ldots,\tau_n$, with total Hispanic turnout in the district equal to $T$.  She first gives estimates 
	$x_1,x_2,\ldots,x_n$ for each precinct, which give a district turnout estimate of $T$ by summing, $x_T$.  This result does not 
	yet give the variance information necessary to build a prior using an assumed normal distribution.

	The expert is now asked to provide the two-to-one interval for $\frac{T}{x_T}$, giving $[0.7\range 1.3]$: they believe that the 
	summed estimate of Hispanic turnout is correct to plus or minus 30\% with probability 0.66 (from the two-to-one interval). 
	To confirm the expert's certainty about this, the value $\sigma_T=0.3$ is plugged into the normal CDF at levels to give 
	credible interval summaries:
	\begin{align*}
		50\%\, CI &= \left[ \Phi_{\mu=1,\sigma=0.3}(0.25) \range \Phi_{\mu=1,\sigma=0.3}(0.75) \right]
			   = [0.798 \range 1.202]	\9
		99\%\, CI &= \left[ \Phi_{\mu=1,\sigma=0.3}(0.005) \range \Phi_{\mu=1,\sigma=0.3}(0.995) \right]
			   = [0.227 \range 1.773].
	\end{align*}
	These are then displayed to the elicitee, and if she agrees that these are reasonable summaries then the variance is 
	$\sigma_T^2 = (0.3)^2 = 0.09$ and there is no need to iterate here.  The expert is now asked to repeat this process for 
	each of the $x_i$ estimates under the assumption that $x_T = T$ (the estimate of the total above is correct).  This 
	temporary fixing of $x_T$ means that the right-hand-side of \eqref{vc3.equation} reduces to the variance of 
	$\frac{\tau_i}{x_i}$ and the expert can do the same interval process as was done with $\frac{T}{x_T}$ for each of the $n$ 
	precincts. 

	Suppose that two-to-one interval for the estimate of Hispanic turnout at the first precinct ($x_1=0.2$) is given as 
	$[0.5\range 1.5]$: she believes the estimate to be correct to plus or minus 50\% with probability 0.66. This gives a 
	variance of $\sigma_1 = (0.5)^2 = 0.25$, and we will note that the subsequent 50\% and 99\% credible interval summaries are 
	approved by the expert.  So the total elicited variance for the first precinct is given by \eqref{vc3.equation} where 
	$x_1^2$ is moved back to the right-hand-side: $\Var(\tau_1) = x_1^2(0.25 + 0.09) = 0.0136$.
\end{examplelist}
\index{subjectindex}{variance components elicitation|)}

\subsubsection{Predictive Modal Elicitation}\index{subjectindex}{predictive modal elicitation|(}
If the outcome variable of interest is distributed Bernoulli or binomial, then it is usually straightforward to query experts 
\emph{directly} for prior probabilities.  For psychological reasons probabilities of binary or summed binary outcomes are more 
intuitively easy to visualize (Cosmides and Tooby 1996).\index{authorindex}{Cosmides, L.} \index{authorindex}{Tooby, J.} Using the 
natural (conjugate) choice of a beta conjugate prior Chaloner and Duncan (1983, 1987) \index{authorindex}{Chaloner, K.}
\index{authorindex}{Duncan, G. T.} produce the predictive modal (PM) elicitation algorithm (see also Gavasakar [1988] 
\index{authorindex}{Gavasakar, U.} for a second application).  First fix a hypothetical total number of Bernoulli trials, and then 
ask the elicitee to specify the most likely number of successes as well as reasonable bounds on the uncertainty.  These values are 
then worked backward into the beta-binomial parametric setup (Chapter~\ref{Intro.Chapter}) to get the beta prior distribution 
parameters.  The elicitee is now shown the implications of their stipulated values on the shape of the beta prior on a computer 
terminal or with pre-prepared flip-charts.  If the elicitee finds the distribution to be unlike their prior expectations, then 
adjustments are made in the deterministic phase and the process is repeated.

\index{subjectindex}{iid}
The data, $X_1,X_2,\ldots,X_n$, are distributed iid Bernoulli, so use $Y=\sum_{i=1}^{n}X_i$ distributed $\text{binomial}(n,p)$. 
Interest lies in the posterior distribution of the probability of occurrence of some event of interest (bill passes/fails, 
treaty/no-treaty, etc.), which is $p$ in this binomial PMF.  The PM method first assumes a beta distribution prior for $p$ with 
unknown parameters: $p\sim \text{beta}(A,B)$ with $A,B>1$. 
The individual steps are:
\begin{bayeslist}
	\item	select a fixed number of trials for a hypothetical experiment ($n=20$ is recommended) 
	\item	ask the elicitees to give the prior predictive modal value for this $n$: the most likely number of successes 
		out of $n$ trials, $m \in (1\range n)$. 
\end{bayeslist}
The numerator of Bayes' Law is: 
\begin{equation}
    f(y|p)f(p) = \frac{\Gamma(n+1)\Gamma(A+B)}{\Gamma(y+1)\Gamma(n-y+1)\Gamma(A)\Gamma(B)}p^{y+A-1} (1-p)^{n-y+B-1}.  
\end{equation}
Now the marginal distribution for $y$ is obtained by integrating over $p$: 
\begin{align}
    f(y|A,B) &= \int_0^1 \frac{\Gamma(n+1)\Gamma(A+B)}{\Gamma(y+1)\Gamma(n-y+1)\Gamma(A)\Gamma(B)} p^{y+A-1}(1-p)^{n-y+B-1}dp \nonumber\\
             &= \frac{\Gamma(n+1)\Gamma(A+B)}{\Gamma(y+1)\Gamma(n-y+1)\Gamma(A)\Gamma(B)} \frac{\Gamma(y+A)\Gamma(n-y+B)}{\Gamma(n+A+B)}
\end{align}
(as shown before on page~\pageref{betabinomial.example}).
Since $m$ is the mode of this distribution, then $f(y=m|A,B)$ is the maximum value obtainable for the function $f(y|A,B)$.  The random
variable $y$ can only take on discrete values so $f(y=m-1|A,B) < f(y=m|A,B)$ and $f(y=m+1|A,B) < f(y=m|A,B)$.  Chaloner and Duncan 
calculate the following two ratios using properties of the binomial distribution:
\begin{align}
	d_\ell &= \frac{f(y=m-1|A,B)}{f(y=m|A,B)} = \frac{ (n-m)(m+A) }{ (m+1)(n-m+B-1) } \nonumber\9
	d_r    &= \frac{f(y=m+1|A,B)}{f(y=m|A,B)} = \frac{ m(n-m+B) }{ (n-m+1)(m+A-1) }. 
	\label{linear.restrictions}
\end{align}
Both terms are bounded by $(0\range 1)$.  Once the elicitee has identified $m$ for the researcher, then the prior parameters $(A,B)$ 
must be constrained to lie in a cone originating at $[1,1]$ in the $A, B$ plane as shown by the solid lines in Figure~\ref{cone.figure}. 
This cone is determined because the equations in \eqref{linear.restrictions} define linear limits in two-space from the same starting point.
Any particular point within the cone represents a two-dimensional Cartesian distance from the uniform prior since the origin of the cone 
specifies a $\text{beta}(1,1)$. 

\begin{figure}[b!]
\vspace{-20pt}
\parbox[c]{\linewidth}{ \hspace{0.95in} 
\begin{center}
	\epsfig{file=Images/priors.figure02.ps,height=4.62in,width=3.75in,clip=,angle=270}  
\caption{\textsc{Finding the Beta Parameters}} \label{cone.figure}
\end{center}
}
\end{figure}

We do not yet have a complete answer since there are an infinite number of $(A,B)$ pairs that could be selected and still remain inside 
the cone.  At this point the elicitee is told to think about \emph{spread} around the mode and is shown a histogram for 
$\text{binomial}(n,m/n)$, and is asked:  ``If we were to go one unit up (and down), how much do you think the probability
of occurrence would decrease?''  With these two values (up and down) we can now calculate $d_\ell$ and $d_r$ directly.  Thus the 
equations in \eqref{linear.restrictions} define line segments for values of $A$ and $B$ that are necessarily bounded by the cone. The 
point of intersection is the $(A,B)$ pair that satisfies both the one unit up restriction and the one unit down restriction, assuming 
that the following restriction is met:
\begin{equation}
	d_\ell d_r > \frac{ m(n-m) }{ (m+1)(n-m+1) }.
\end{equation}
If it this is not true, then the assessor is asked to provide new values of $f(y=m-1|A,B)$ and $f(y=m+1|A,B)$.  The line segments and 
their intersection are also shown in Figure~\ref{cone.figure}.  Label this point of intersection $(A_1,B_1)$, and calculate a new beta 
mode with these values:
\begin{equation}
	m_1 = \frac{A_1 - 1}{A_1 + B_1 - 2}.
\end{equation}
We then display it to the assessor with the middle 50\% of the density. The assessor is then asked if this interval is too small, too large, 
or just right. The interval is adjusted according to the following:
\begin{equation*}
	\text{``too small''} \Longrightarrow h = -1, \quad
	\text{``too large''} \Longrightarrow h = +1, \quad
	\text{``just right''} \Longrightarrow h = 0, 
\end{equation*}
where $h$ is inserted into: 
\begin{equation}
	A_2 = 1 + 2^h (A_1 - 1), \qquad\qquad B_2 = 1 + 2^h (B_1 - 1), 
\end{equation}
for adjusted parameter values.   This process is repeated until the assessor is satisfied with the interval ($h=0$).  

\begin{examplelist}
	\item	{\bf Labor Disputes in OECD Countries.}	\index{subjectindex}{example!labor disputes in OECD countries}
		Suppose we are interested in eliciting a prior distribution for the probability of a strike given a major labor dispute 
		in 20 OECD countries over a certain year. Given $n=20$ an elicitee indicates $m=5$, and is then shown a histogram of the 
		$\text{binomial}(20,5/20)$ distribution. The assessor now asserts that the one unit up and down probability change is 
		$1/50$, so $d_\ell=0.98=\frac{ (15)(5+A) }{ (6)(14+B) }$ and $d_r=0.98=\frac{ 5(15+B) }{ (16)(4+A) }$ (these values are 
		acceptable to the condition above since $d_\ell d_r=0.9604>\frac{m(n-m)}{(m+1)(n-m+1)}=0.7813$).  Solving the equations 
		produces $A_1 = 1.4$, and $B_1 = 2.327$, which gives a modal value for the probability of a strike given a labor dispute 
		of $\hat{p}=0.232$ with the middle of 50\% of the density $[0.193\range 0.537]$.  The elicitee states that this interval 
		is too large, so we set $h=+1$ and produce $A_2 = 1 + 2^h (1.4 - 1) = 1.8$, and $B_2 = 1 + 2^h (2.327 - 1) = 3.654$, 
		which gives the middle of 50\% of the density as $[0.184\range 0.456]$.  The elicitee still believes that this is too large so 
		the process is repeated with $h=+1$ now providing $[0.182\range 0.386]$.  This interval is smaller than the first and is 
		acceptable to the elicitee.  Therefore the elicited prior distribution for the probability of a strike given a major 
		labor dispute has the distribution $\text{beta}(2.6,6.308)$.
\end{examplelist}
\index{subjectindex}{predictive modal elicitation|)}

\subsubsection{Prior Elicitation for the Normal-Linear Model}\index{subjectindex}{elicitation!normal-linear model}
Using the standard regression setup described on page~\pageref{linear.likelihood.equation}, we want to elicit priors on $\B$ 
\begin{wrapfigure}[13]{r!}{2.2in}
  \vspace{-1pt}
  \centering
  \parbox[r]{2.1in}{
	\begin{tabular}{lr|lr}
	$\nu=df$	&	$a(\tilde{\X})$ 	& $\nu=df$	&	$a(\tilde{\X})$ \\
	\hline
	3 	& 	2.76			& 12 	&	2.37	\\
	4 	& 	2.62			& 14 	& 	2.36	\\
	5 	& 	2.53			& 16 	& 	2.35	\\
	6 	& 	2.48			& 18 	& 	2.34	\\
	7 	& 	2.45			& 20 	& 	2.33	\\
	8 	& 	2.42			& 30 	& 	2.31	\\
	9 	& 	2.40			& 40 	& 	2.31	\\
	10 	& 	2.39			& 60 	& 	2.30	\\
		&				& $\infty$& 	2.27	\\
	\end{tabular}	
  	\vspace{-4pt}
  	\caption{\textsc{$t$ Ratios}}\label{t.ratios}
  	\vspace{14pt}
  }
  %\vspace{23pt}
\end{wrapfigure} 
from experts 
(we can retain the uninformed approach for $\sigma^2$ or we can elicit for it as well).\index{authorindex}{Kadane, J. B.}
\index{authorindex}{Wolfson, L. J.} Kadane \etal (1980) as well as Kadane and Wolfson (1998) first establish $m$ design points of the 
explanatory variable vector: $\tilde{\X}_1,\tilde{\X}_2,\ldots,\tilde{\X}_m$, where again these represent interesting cases or values 
spanning the range of the $k$ variables.  

These values must be chosen such that stacking the vectors into an $m \times k$ matrix, 
$\tilde{\X}$, gives a positive definite matrix $\tilde{\X}'\tilde{\X}$.  
The elicitees are asked to study each of the $\tilde{\X}_i$ row 
vectors and produce $\y_{50}$, a vector of outcome variable medians whose elements correspond to the design cases.  Such values then 
represent typical responses to the hypothesized design points specified in the $\tilde{\X}_i$.  Now an elicited prior point estimate for 
$\B$ is given by: $\bh_{0.50} = (\tilde{\X}'\tilde{\X})^{-1}\tilde{\X}'\y_{0.50}$. 

\index{subjectindex}{Student's-$t$}
To get a full prior description for $\B$ assume that this distribution is Student's-$t$ around $\bh_{0.50}$ with greater than
$\nu=2$ degrees of freedom (we do not want to worry about the existence of moments).  This is a somewhat conservative prior since
large data size under weak regularity conditions leads to Bayesian posterior normality of linear model coefficients, and
$t$-distributed forms with smaller data size (Berger 1985, p.224; Lindley and Smith 1972).  There is no direct way to set the
degrees of freedom for this $t$-distribution since the $m$ value was established arbitrarily by the researchers.
\index{authorindex}{Berger, J. O.} \index{authorindex}{Lindley, D. V.} \index{authorindex}{Smith, A. F.
M.}\index{subjectindex}{posterior distribution!Student's-$t$}

Kadane\index{authorindex}{Kadane, J. B.} \etal (1980) suggest that after eliciting $\y_{0.50}$ for each $\tilde{\X}_i$, researchers 
should also elicit $\y_{0.75}$ by asking for the median point above the median point just provided.  Repeat this process two more times 
in the same direction to obtain $\y_{0.875}$, and $\y_{0.9375}$.  For each of the $m$ assessments calculate the ratio:
\begin{equation}
	a(\tilde{\X}) = (\y_{0.9375}-\y_{0.50})/(\y_{0.75}-\y_{0.50}),
\end{equation}
where differencing makes the numerator and denominator independent of the center, and the ratio produced is now independent of the spread 
described.  This ratio uniquely describes tail behavior for a $t$-distribution because it is the relative ``drop-off'' in quantiles.  
Kadane \etal give tabulated degrees of freedom against values of this ratio, a subset of which is given in Figure~\ref{t.ratios}.

Values greater than $2.76$ indicate that the researcher should instruct the elicitee to reconsider their responses, and values less than 
$2.27$ imply that a standard normal prior centered at $\bh_{0.50}$ is appropriate.  Other distributions are feasible, but this
tabulation makes the Student's-$t$ particularly easy.  Once the unique degrees of freedom are identified, the elicited prior is fully
identified.  \index{subjectindex}{posterior distribution!Student's-$t$}

\subsubsection{Elicitation Using a Beta Distribution}
Gill and Freeman\index{authorindex}{Freeman, J.} \index{authorindex}{Gill, J.} (2013) developed an elicitation process for updating 
social networks using beta distribution forms.  The process starts with the more general form of the beta probability density function,
\index{subjectindex}{elicitation!beta distribution query}
\begin{equation}\label{general.beta.pdf}
    f(y) = \frac{ \Gamma(\alpha+\beta) }{ \Gamma(\alpha)\Gamma(\beta) }
           \frac{ (y-a)^{\alpha-1}(b-y)^{\beta-1} }{ (b-a)^{\alpha+\beta-1} },
\end{equation}
where: $0 \le a<y<b$, and $\alpha,\beta>0$.  Now the support is over $[a\range b]$ rather than just $[0\range 1]$, but it reduces to the 
standard form with the change of variable: $X = \frac{Y-a}{b-a}$, where $0<x<1$, but $\alpha$ and $\beta$ are unchanged.  This also means 
that $Y = (b-a)X + a$.  This change means that qualitative experts can be queried on the more natural $[0\range 100]$ scale and it is 
converted back to a regular beta form.  Now recall the following beta distribution properties:
\begin{align*}
    \text{mean:}\quad&      \mu_x = \frac{\alpha}{\alpha+\beta} = \frac{\mu_y-a}{b-a}   \9
    %\text{mode:}\quad&        m_x = \frac{\alpha-1}{\alpha+\beta-2} = \frac{m_y-a}{b-a} \9
    \text{variance:}\quad&  \sigma_x^2 = \frac{\alpha\beta}{(\alpha+\beta)^2(\alpha+\beta+1)}
                       = \frac{\sigma_y^2}{(b-a)^2}.
\end{align*}
Rearranging the equations above gives us the following two important relations from Law and Kelton 
(1982, p.205):  \index{authorindex}{Law, A. M.} \index{authorindex}{Kelton, W. D.}
\begin{align}\label{alpha.beta.calc}
    \alpha &= \left[ \frac{\mu_x(1-\mu_x)}{\sigma_x^2} - 1 \right]\mu_x     \nonumber   \9
    \beta  &= \left[ \frac{\mu_x(1-\mu_x)}{\sigma_x^2} - 1 \right](1-\mu_x).
\end{align}
So querying for a mean and variance would produce all the necessary information to obtain the implied $\alpha$ and $\beta$ and thus fully 
describe the beta distribution.  Research to date demonstrates that humans are not very good subjective assessors of variance (O'Hagan 1998), 
\index{authorindex}{O'Hagan, A.} necessitating other means of getting to the parameters.

\index{authorindex}{Freeman, J.} \index{authorindex}{Gill, J.} 
Gill and Freeman propose querying the experts for their lowest but 
reasonable value and highest but reasonable value with the idea that these can be explained as endpoints of a 95\% credible interval.  Some 
training is therefore necessary, but not a substantial amount.  Restricting $\alpha>1$ and $\beta>1$ guarantees that the beta distribution 
will be unimodal, so the credible interval is not broken up into disjoint components.  Combining these, we know that 
$4\sigma_y \approx b-a$. We also know that
\begin{equation*}
    \sigma_y^2 = (b-a)^2 \sigma_x^2 = (b-a)^2 \frac{\alpha\beta}{(\alpha+\beta)^2(\alpha+\beta+1)}.
\end{equation*}
Substituting produces the approximation: $\sigma_x  \approx \frac{1}{4}$, which leads directly to preliminary estimates of the two 
parameters.  \index{subjectindex}{Program Evaluation and Review Technique (PERT)} This draws on the PERT (Program Evaluation and Review 
Technique) analysis for industrial and engineering project planning that uses $\sigma_x = \frac{1}{6}$ when the beta mode is between 
$0.13$ and $0.87$, and is justified by a normal approximation (Farnum and Stanton 1987, Lu 2002).  
\index{authorindex}{Farnum, N. R.} \index{authorindex}{Stanton, L. W.} \index{authorindex}{Lu, M.}
More importantly, the value of $1/4$ 
is just a starting point for a software ``slide'' that the elicitee is allowed to adjust on a computer screen.  The elicitee is shown 
the resulting beta distribution graphically and given two slides: one that changes $\mu_x = \frac{\alpha}{\alpha+\beta}$ and one that 
changes $\sigma_x^2 = \frac{\alpha\beta}{(\alpha+\beta)^2(\alpha+\beta+1)}$.  Subsequently, there are new values of $\mu_x$ and 
$\sigma_x$ that the elicitee prefers.  This can be repeated for any reasonable number of priors.

\subsubsection{Eliciting Some Final Thoughts on Elicited Priors}
\index{authorindex}{Spetzler, C. S.}
\index{authorindex}{Sta\"{e}l von Holstein, C. S.} 
\index{authorindex}{Kadane, J. B.}
\index{authorindex}{Hogarth, R. M.}
\index{authorindex}{Lindley, D. V.}
\index{authorindex}{Tversky, A.}
\index{authorindex}{Brown, D. V.} 
\index{authorindex}{Savage, L. J.}
\index{authorindex}{Tversky, A.}
\index{authorindex}{Kahneman, D.}
\index{authorindex}{Winkler, R. L.} 
Elicited priors in Bayesian studies have been studied for quite some time, going back to the foundational
papers of Kadane (1980), Kadane \etal (1980), Hogarth (1975), Lindley, 
Tversky, and Brown (1979), Savage (1971), Spetzler and Sta\"{e}l 
von Holstein (1975), Tversky (1974), Tversky and Kahneman (1974), and Winkler (1967).  There have been
two main impediments to widespread acceptance and use of elicited priors.  \emph{First}, there has long been
a distrust by some Bayesians and others of overtly subjective priors.  The failed attempt to produce a universally
accepted no-information prior was fueled in part by these sentiments.  \emph{Second}, there remains worry
about the cognitive process that generates elicited priors.  These concerns are summed up by Hogarth 
(1975, p.273):\index{authorindex}{Hogarth, R. M.}
\begin{quote}
	In summary, man is a selective, stepwise information processing system with limited capacity,
	and, as I shall argue, he is ill-equipped for assessing subjective probability distributions.
	Furthermore, man frequently just ignores uncertainty.
\end{quote}
\index{subjectindex}{uncertainty}%
This does not mean that we should never use human experts for generating prior information, but rather
that we should carefully elicit probabilistic statements, knowing that the sources have cognitive
limitations.  A very promising approach is the use of interactive computer queries to elicit priors
(Garthwaite and Dickey 1992).  The software can be written so that inconsistent, illogical, and 
	\index{authorindex}{Garthwaite, P. H.}%
	\index{authorindex}{Dickey, J. M.}%
contradictory answers are rejected at the data-entry stage rather than corrected or ignored later.  
Web-based implementations, though still untried, promise even greater returns given their convenience 
and ubiquity.

There is a close relationship between elicited priors and meta-analysis.\index{subjectindex}{meta-analysis}  It is 
possible to assemble the results from previous studies and treat these as elicitations.  This is a 
fundamentally Bayesian idea and essentially represents a Bayesian treatment of conventional 
meta-analysis where the predictive distribution from previous works forms the basis of a prior on the 
current project (Carlin 1992).\index{authorindex}{Carlin, J. B.}  One caveat is warranted, however.  
Kass and Greenhouse (1989) warn that this approach implies that
the previous studies pooled to form a prior are now assumed to be exchangeable and this might not
be appropriate. \index{subjectindex}{pooling!prior} \index{subjectindex}{prior distribution!informative|)}
\index{authorindex}{Kass, R. E.} \index{authorindex}{Greenhouse, J. B.} 

\section{Hybrid Prior Forms}\index{subjectindex}{prior distribution!hybrid}
Some prior specifications are attempts to \emph{mix} informative and uninformed types in such a way that gradations of information
can be reflected.  Technically this makes them informative priors, but we will treat them as compromises since this is how they
are typically viewed. The key point is that the specification of prior distributions is very flexible, and can be tailored by
researchers to reflect varying levels of qualitative or quantitative information. In this section we describe some recent designs
for prior distributions that focus on addressing specific model problems rather than general prior specifications.

A number of other prior forms are common in applied settings.  We will describe mixture priors in Chapter~\ref{MC.Chapter}, how empirical 
Bayes\index{subjectindex}{empirical Bayes} uses the observed data to establish hyperpriors in a hierarchical model (prior parameters on the 
highest level of priors), and the idea of specifying both hierarchical structure and subjective information into the prior leads to 
hierarchical models in general.  Much work has been done toward finding new criteria for uninformative priors.  These approaches include 
maximally vague entropy priors (Spall and Hill 1990; Berger, Bernardo, and Mendoza 1989), indifference conjugate priors (Novick and Hall 
1965, Novick 1969), and the general idea of proper but very diffuse priors (Box and Tiao 1973).
\index{authorindex}{Spall, J. C.} \index{authorindex}{Hill, S. D.} \index{authorindex}{Berger, J. O.} \index{authorindex}{Bernardo, J. M.}
\index{authorindex}{Mendoza, M.} \index{authorindex}{Novick, M. R.} \index{authorindex}{Hall, W. J.} \index{authorindex}{Box, G. E. P.}
\index{authorindex}{Tiao, G. C.}

\subsection{Spike and Slab Priors for Linear Models}
\index{subjectindex}{prior distribution!spike and slab|(}
Mitchell\index{authorindex}{Mitchell, T. J.} and Beauchamp\index{authorindex}{Beauchamp, J. J.} (1988) introduce a species of
priors designed to facilitate explanatory variable selection in linear regression.  The basic idea is to specify a reasonably
skeptical prior distribution by stipulating density spike at zero surrounded symmetrically by 
\begin{wrapfigure}[14]{r}{3.2in}
  \vspace{-18pt}
  \parbox[c]{3in}{\hspace{0.1in} \epsfig{file=./Images/priors.spike.slab.ps,width=2.8in,height=2.2in,angle=0}}\\[-1pt]
  \caption{\textsc{Spike and Slab Prior}}\label{spike.slab.fig}
  %\vspace{-24pt}
\end{wrapfigure}
a flat slab with specified boundaries.  If desired, other non-zero values can be evaluated by simply offsetting the coefficient
value.  Also, this type of prior has only been applied to linear regression, but it would only be slightly more involved to apply
to generalized linear models. 

Thus the prior makes a reasonably strong claim about zero effect for some regression parameter, but admits 
the possibility of non-zero effects.  Suppose we are seeking a prior for the $j$th regression coefficient 
(from $j \in 1,\ldots,k$) in a standard linear model: $\y = \x\B + \epsilon$, with all of the usual 
Gauss-Markov assumptions, $k<n$, and $\epsilon \sim \mathcal{N}(0,\sigma^2)$.  The prior is then:
\begin{align*}
	\text{spike:} &\quad p(\beta_j = 0) = h_{0j}	\\
	\text{slab:}  &\quad  p(\beta_j \in [-f_j\range f_j], \beta_j \ne 0) = 2h_{1j}f_j, \\
	              &\quad -f_j < \beta_j < f_j,
\end{align*}
where $h_{0j} + 2h_{1j}f_j = 1$.  This is illustrated in Figure~\ref{spike.slab.fig}.  So obviously $h_{0j}$ is the height 
of the spike and $h_{1j}$ is the height of the slab, and varying degrees of skepticism about the veracity of the effect of 
$\x_j$ can be modeled by altering the ratio:
\begin{equation*}
	\gamma_j = \frac{h_{0j}}{h_{1j}} = 2f_j\frac{h_{0j}}{1-h_{0j}}.
\end{equation*}
For terms that should be included in the model with probability one, just set $h_{0j}=0$.  The nice part
of this priors setup is that we can easily reflect our prior belief in complete model specifications.  Miller 
(2002, 203)\index{authorindex}{Miller, A.} notes that taken by itself, $h_{0j}$ is a statement that the $j$th variable
should not be in the model, and taken by itself, $1-h_{0j}$ is a statement that the $j$th variable matters
in some way.  Since all prior density not accounted for by the spike falls to the slab, the prior for the 
$m$th model, $A_m$, is just the product given by:
\begin{equation*}
	p(A_m) = \prod_{j \in A_m}(1-h_{0j}) \prod_{j \not\in A_m}(h_{0j}),
\end{equation*}
which is the product of the slab density for those coefficients in model $A_m$, times the product of the spike
density for those coefficients not in $A_m$ (note that the $j$th coefficient only contributes a single term).  

For the linear regression model Mitchell\index{authorindex}{Mitchell, T. J.} and Beauchamp\index{authorindex}{Beauchamp, J. J.} 
put a prior on $\sigma$ such that $\log(\sigma)$ is uniform between $-\log(\sigma_0)$ and $\log(\sigma_0)$, for some large value 
of $\sigma_0$.  They then derive the resulting posterior for model $A_m$ with $k_m$ number of coefficients (including the 
constant) in the matrix $\X_m$:
\begin{equation*}
	\pi(A_m|\X,\y) \propto \pi^{k_m/2} \Gamma\left(\frac{n-k_m}{2}\right) \left|\X_m'\X_m\right|^{-\half}
			 (S_m^2)^{-(n-k_m)/2} \prod_{j \not\in A_m}\gamma_j, 
\end{equation*}
where $S_m^2 = (\y- \X_m\hat{\B}_m)'(\y- \X_m\hat{\B}_m)$, $\hat{\B} = (\X_m'\X_m)^{-1}\X_m'\y$, and
$c$ is a normalizing constant.  We can see from this how the data informs across all of the
proposed model specifications.  This setup also facilitates coefficient quality assessment as well since we
can average coefficient probabilities across model space.  So the joint posterior distribution of all 
$k$ of the coefficients is given by:
\begin{equation*}
	p(\B|\X,\y) = \sum_m \pi(A_m|\X,\y) p(\B|\A_m,\X,\y)
\end{equation*}
where $p(\B|\A_m,\X,\y)$ is multivariate $t$-distribution centered at $\hat{\B}$ (see 
Chapter~\ref{Linear.Chapter} for more details on posterior distributions from linear models).  This
general approach is called Bayesian model averaging\index{subjectindex}{Bayesian model averaging} and is discussed
in Chapter~\ref{Model.Quality.Chapter}.

\index{authorindex}{Pang, X.}\index{authorindex}{Gill, J.}
Such priors are easy to construct for linear models and can be applied in more general settings (see Pang and Gill
2012).  Several words of caution are warranted, however.  \emph{First}, each coefficient operates on a different 
scale so direct comparison cannot be done without some adjustment like centering and scaling 
(Mitchell\index{authorindex}{Mitchell, T. J.} and Beauchamp's\index{authorindex}{Beauchamp, J. J.} suggestion).  
\emph{Second}, the researcher's choice of $f_j$ and $h_{0j}$ can have a more dramatic effect than intended.
\index{subjectindex}{prior distribution!spike and slab|)}

\subsection{Maximum Entropy Priors}\index{subjectindex}{prior distribution!maximum entropy|(}
Jaynes (1968, 1980, 1983) introduces the idea of an \emph{entropy prior} to describe relative
	\index{authorindex}{Jaynes, E. T.}%
	\index{subjectindex}{prior distribution!entropy}%	
levels of uncertainty\index{subjectindex}{uncertainty!entropy} about the distribution of prior parameters.  
One advantage to the entropy approach is that within the same framework, uncertainty ranging from that provided by the uniform
prior to that provided by absolute certainty given by a degenerate (single point) distribution \index{subjectindex}{degeneracy}
can be modeled in the same way.  Thus the primary advantage to entropy priors is their flexibility.  Unfortunately, however,
entropy priors are not invariant to reparameterization and therefore have somewhat limited applicability.  Nonetheless they are
worthy of study in their own right because of the link to the idea of \emph{information} in the general study of information
theory, which is closely tied to the idea of prior knowledge.

The core idea of entropy is the quantification of uncertainty\index{subjectindex}{uncertainty!entropy} of a 
transmission or observation (Shannon 1948,
Ayres 1994), and this can be interpreted as uncertainty in a PDF or PMF (Rosenkranz 1977).  Initially assume 
	\index{authorindex}{Shannon, C.}%
	\index{authorindex}{Ayres, D.}%
	\index{authorindex}{Rosenkranz, R. D.}%
that we are interested in a discrete parameter $\theta$.  Define the entropy of $\theta$ for a given 
parametric form, $p(\theta)$, as:\index{subjectindex}{entropy!discrete}
\begin{equation}\label{discrete.entropy}
	H(\theta) = -\sum_\Theta p(\theta_i) \log[p(\theta_i)],
\end{equation}
where the sum is taken over the categories of $\theta$.  In the case of discrete distributions, we have a 
wide selection of parametric forms for $p(\theta)$, but consider two very different varieties for
$k$ possible values in the sample space: $\Theta = [\theta_1,\theta_2,\ldots,\theta_k]$.  Following the
discussion of uniform priors, if we assign each outcome a uniform prior probability of occurrence: $1/k$,
then the entropy of this prior is at its maximum:
\begin{equation}
	H(\theta) = -\sum_\Theta \frac{1}{k} \log\left[ \frac{1}{k} \right] = \log[k].
\end{equation}
It is clear from this result that prior uncertainty\index{subjectindex}{uncertainty!prior} increases logarithmically 
with increases in the number
of discrete alternatives.  At the other end of the uncertainty spectrum, we can stipulate that $p(\theta_i) = 1$,
and $p(\theta_{\neg i}) = 0$; that is, $\theta$ equals some value with probability one and every other
value with probability zero.  The entropy of this prior specification is given by:
\begin{equation}
	H(\theta) = -\sum_{\Theta[-i]} 0 \log[0] + 1\log[1] = 0,
\end{equation}
where this calculation requires the assumption that $0 \log[0] = 0$.  Here we are completely certain
about distribution of $\theta$ and the entropy is zero reflecting minimum prior entropy.  

The utility of the entropy prior is that we can take some limited knowledge in the form of a restriction,
and for a given parametric specification of the prior, produce the prior with maximum possible entropy.  
	\index{subjectindex}{prior distribution!maximum entropy}
The goal is to include what we may know but to interject as little else as possible in the creation of the
prior: minimally informative about $\theta$ for the specified $p(\theta)$ (Eaves 1985).\index{authorindex}{Eaves, D. M.}  
So a distribution is produced that is as diffuse as possible given the identified ``side conditions.''
\index{subjectindex}{prior distribution!informative}

Suppose that we could stipulate the first two moments as constraints:
\begin{align}
	E[\theta] &= \sum_\Theta p(\theta)\theta = \mu_1		\nonumber\\
	E[\theta^2] &= \sum_\Theta p(\theta)\theta^2 = \mu_2.		\nonumber
\end{align}
Adding the further constraint that $p(\theta)$ is proper gives the prior density:
\begin{equation}\label{big.entropy.form}
	\tilde{p}(\theta_i) = \frac{ \exp \left[ \lambda_1 \theta_i + \lambda_2 (\theta_i - \mu_1)^2 \right] }
	                  { \sum_{j} \exp \left[ \lambda_1 \theta_j + \lambda_2 (\theta_j - \mu_1)^2 \right] },
\end{equation}
where the constants, $\lambda_1$ and $\lambda_2$, are determined from the constraints.  This is a simplified
version, and in a more general setting there are additional moments than the two estimated, say $M$ in 
$E[g_m(\theta)]$. So producing the constants is more involved than the example here (Robert 2001, p.110; Zellner and Highfield 1988).  
\index{authorindex}{Robert, C. P.} \index{authorindex}{Zellner, A.} \index{authorindex}{Highfield, R. A.}

The continuous case is both more difficult and more straightforward.  It is more difficult because the 
analog to \eqref{discrete.entropy},\index{subjectindex}{entropy!continuous}
\begin{equation}\label{continous.entropy}
	H(\theta) = -\int_\Theta p(\theta) \log[p(\theta)]d\theta,
\end{equation}
leads to different answers depending on alternative definitions of the underlying reference measures (Jaynes 1968; Robert 2001,
p.110).  Essentially this just means that mathematical \index{authorindex}{Robert, C. P.} \index{authorindex}{Jaynes, E. T.}
requirements for getting a usable prior are more difficult to obtain and in some circumstances are actually impossible
(Berger\index{authorindex}{Berger, J. O.} 1985, p.93-94).  For the $M$ moment estimation case, the constraining statement is now:
\begin{equation}\label{continuous.moment.entropy}
	E[g_m(\theta)] = \int_\Theta p(\theta)g_m(\theta)d\theta = \mu_m, \qquad m=1,\ldots,M.	
\end{equation}

Consider the following simple example (O'Hagan\index{authorindex}{O'Hagan, A.} 1994).  The support of $\theta$ is
$[0\range\infty]$ and we specify the constraint that the expected value is equal to some constant: $E[\theta]=c$.  If we further
specify that the prior distribution has the least informative possible exponential PDF form, then
\eqref{continuous.moment.entropy} specifies the prior: $f(\theta|c) = c\,\exp(-c\theta)$.\index{subjectindex}{prior
distribution!entropy!exponential model} \index{subjectindex}{prior distribution!maximum entropy|)}

\subsection{Histogram Priors}\index{subjectindex}{prior distribution!histogram}
Johnson and Albert (1999) suggest the ``histogram prior'' based on the idea that it is easy to \index{authorindex}{Johnson, V. E.}
\index{authorindex}{Albert, J. H.} \index{subject}{prior distribution!histogram} summarize relatively vague, but informative, prior 
information through this graphical device.  Their prior exploits the idea that it is much easier to elicit or generate a small number of 
binned probability statements.  This is essentially a nonparametric approach in that there is no \emph{a priori} structure on the form 
of the resulting histogram and is therefore attractive when the researcher is uncomfortable with specifying a standard form.  The 
estimation process is done in segments across the range of the histogram corresponding to a bin.  Within each bin a separate posterior is 
calculated with a bounded uniform prior producing a noncontinuous posterior weighted by the heights of the histogram bins.  Although the 
discontinuities can be smoothed out, the resulting form may still be unreflective of a compromise between prior and likelihood information.
\index{subjectindex}{smoothing} \index{subjectindex}{prior distribution!informative}

\section{Nonparametric Priors}\index{subjectindex}{prior distribution!nonparametric}\index{subjectindex}{Dirichlet process prior}
Nonparametric priors were originally introduced by Ferguson\index{authorindex}{Ferguson, T. S.} (1973) and Antoniak
\index{authorindex}{Antoniak, C. E.} (1974), but not fully developed until the advent of better computing resources for estimation.  
Dirichlet process priors stipulate that the data are produced by a mixture distribution wherein the Bayesian prior specifications are 
produced by a Dirichlet process, which constitutes a distribution of distributions since each produced parameter defines a 
particular distribution.  These distributions can be made conditional on additional parameterizations (as done in 
Escobar and West [1995]) and thus the models are hierarchical.  This means that realizations of the Dirichlet process are discrete (with 
probability one), even given support over the full real line, and are thus treated like countably infinite mixtures.  
\index{authorindex}{Escobar, M. D.} \index{authorindex}{West, M.} 

This approach is substantially different from the conventional use of the Dirichlet distribution in Bayesian models as a conjugate 
prior distribution for the multinomial likelihood.  First, let us look at the Dirichlet PDF, which is the multivariate generalization
of the beta distribution.  Suppose $\X = [x_1,x_2,\ldots,x_k]$, where each individual $x$ is defined over the support $[0\range 1]$,
and sum to one: $\sum_{i=1}^n x_i = 1$.  The Dirichlet PDF uses a parameter vector, $\A = [\alpha_1,\alpha_2,\ldots,\alpha_k$],
with the condition that all $\alpha_i > 0$.  These structures are then generalizations from the beta for $\X=[x,1-x]$, and
$\A=[\alpha,\beta]$, where $k=2$.  The PDF is:
\begin{equation}
	f(\X|\A) = \frac{ \Gamma\left( \sum_{i=1}^n \alpha_i \right) }{ \prod_{i=1}^n \Gamma(\alpha_i) }
		   \prod_{i=1}^n x_i^{\alpha_i-1}.
\end{equation}
The resemblance to the beta is quite strong here.  This distribution has expected value and variance:
\begin{align}
	E[X_i] 		&= \frac{ \alpha_i }{ \sum_{k=1}^n \alpha_k }	\9
	\Var[X_i] 	&= \frac{ \alpha_i\left( \sum_{k=1}^n \alpha_k - \alpha_i \right) }
			 { \left( \sum_{k=1}^n \alpha_k \right)^2 \left( \sum_{k=1}^n \alpha_k + 1 \right) }   \9
	\Cov[X_i,X_j] 	&= \frac{ -\alpha_i\alpha_j }
			  { \left( \sum_{k=1}^n \alpha_k \right)^2 \left( \sum_{k=1}^n \alpha_k + 1 \right) }.
\end{align}		    

Consider the question of modeling dichotomous individual choices, $Y_i$, like turning out to vote, voting for a specific candidate,
joining a social group, discontinuing education, and so on.  The most common ``regression-style''  modeling specification
is to assume that an underlying smooth utility curve dictates such preferences, providing the unobserved, but estimated threshold
$\theta \in [0,1]$.  The individual's threshold along this curve then determines the zero or one outcome conditional on an
additive right-hand specification, $\X\B$.  Realistically, we should treat this threshold differently for each individual,
but we can apply the reasonable Bayesian approach of assuming that these are different thresholds yet still generated from
a single distribution $G$ which is itself conditional on a parameter $\alpha$, thus $E[nG(\theta_i|\X\B,\alpha)]$ is the
expected number of affirmative outcomes.  Suppose there were structures in the data such as unexplained clustering effects,
unit heterogeneity, autocorrelation, or missingness that cast doubt on the notion of $G$ as a single model.  Note that
this can happen in a Bayesian or non-Bayesian setting, the difference being the distributional or deterministic
interpretation of $\theta$.  The choice of $G$ is unknown by the researcher but determined by custom or intuition.
We suggest, instead, a nonparametric Bayesian approach that draws $\theta$ from a mixture of appropriate prior distributions
conditional on data and parameters (in this simple case a mixture of beta distributions according to
$\mathcal{BE}(\alpha(\X\B),Z-\alpha(\X\B))$ for some prior parameter $Z$).

\index{authorindex}{Gill, J.} \index{authorindex}{Casella, G.} 
Gill and Casella (2007) apply this type of a prior to ordinal regression 
models.  Such outcomes occur frequently in the social sciences, especially in survey research where instruments such as Likert and 
Guttman scale responses are used.  Gill and Casella look at self-reported levels of stress for political executives in the United
States where a five-point ordinal scale is used.
Start with data $Y_1, Y_2,\ldots,Y_n$ assumed to be drawn from a mixture of distributions denoted $p(\psi)$ where the mixing over 
$\psi$ is independent from distribution $G$, and the prior on $G$, $\mathcal{D}$ is a mixture of Dirichlet processes.  The ordered 
probit model assumes first that there is a multinomial selection process:
\begin{equation}
        Y_{i} \sim \mbox{Multinomial}(1,(p_1, p_2, \ldots, p_C)),\quad i=1, \ldots n
        \label{eq:model1}
\end{equation}
where $\sum_j p_j = 1$, and $Y_i=(y_{i1}, \ldots, y_{iC})$ is a $C \times 1$ vector with a $1$ in one position and $0$ elsewhere.
The placement of the $1$ denotes the class that the observation falls into.  In addition, the $p_j$ are ordered by the probit model
\begin{equation}
        p_j = p(\theta_{j-1} \le U_i \le \theta_j)
        \label{eq:model2}
\end{equation}
where these cutpoints between categories have the property that $-\infty = \theta_0 <\theta_1 < \cdots <\theta_C=\infty$.  Define now
the random quantity:
\begin{equation}
        U_i \sim \mathcal{N}(X_i \beta + \psi_i, \sigma^2)
        \label{eq:model3}
\end{equation}
where $X_i$ are covariates associated with the $i^{th}$ observation, $\beta$ is the coefficient vector, and $\psi$ denotes a random 
effect to account for subject-specific deviation from the underlying model.  Here the $U_i$ are unobservable random variables, and we 
could specify the model without them, that is, from (\ref{eq:model2}) and (\ref{eq:model3}),
\begin{equation}
    p_j = \Phi \left( \frac{\theta_{j}-X_i\beta-\psi_i}{\sigma} \right)-\Phi
               \left( \frac{\theta_{j-1}-X_i\beta-\psi_i}{\sigma} \right).
    \label{eq:model4}
\end{equation}

If we do not want to require a particular structure or distribution on this random effect we can make our model semiparametric by
assuming that $\psi$ is an observation from a {\it Dirichlet Process},
\begin{eqnarray}
        \psi_i &\sim& G         \nonumber \\
        G &\sim& \mathcal{D}_{m G_{\mu, \tau^2}}
        \label{eq:model5}
\end{eqnarray}
where $G_{\mu, \tau^2}$ is a {\em base measure} and $m$ is a {\em concentration parameter}.  Thus, $\psi$ is modeled to come
from a distribution that sits in a neighborhood of $G_{\mu, \tau^2}$, with the size of the neighborhood being controlled by
$m$.  For now, we take $m$ to be fixed, but later we will let it vary.  In particular, with the mixture setup we take the prior
on $m$ to be in a discrete set and $G$ to have root density $\mathcal{N}(\mu,\tau^2)$.

It is necessary to stipulate a full set of priors for the other terms, and the following make intuitive sense.
\begin{eqnarray}\label{eq:model6}
    \beta &\sim& \mathcal{N}(\beta_0, \sigma^2_\beta)\nonumber \\
    \mu &\sim& \mathcal{N}(0,d\tau^2)\\
    \frac{1}{\tau^2}&\sim& \mbox{Gamma}(a,b)\nonumber
\end{eqnarray}
It is common to set $\sigma^2_\beta=\infty$, resulting in a flat prior on $\beta$.  The parameters in the priors on $\mu$
and $\tau^2$ can be chosen to make the priors sufficiently diffuse to allow the random effect to have an effect.  The choice of
prior mean zero for $\psi$ does not lose generality, as the $X_i\beta$ term in (\ref{eq:model5}) locates the distribution.

Estimation of this model requires MCMC tools, such as the Gibbs sampler.  So first write:
\begin{eqnarray}\label{eq:Neal1}
    c_i &\sim& \mbox{Discrete}(q_1, \ldots, q_K)\nonumber               \9
    \psi_{c_i}&\sim& g(\psi) = \mathcal{N}(\mu, \tau^2)                           \9
    \mathbf{q} &\sim& \mbox{Dirichlet}(m/K, \ldots,m/K),\nonumber
\end{eqnarray}
where the $c_i$ serve only to group the $\psi_i$, resulting in a common value of $\psi_i = \psi_j$ if $c_i = c_j$.  In the
Gibbs sampler, the $c_i$ are generated conditionally:  
\begin{eqnarray*}
    \bc &=& (c_1, c_2, \ldots, c_n)                                     \9
    \bc_{-i}&=& (c_1, c_2, \ldots, c_{i-1},c_{i+1},\ldots,c_n)          \9
    n_{-i, \ell} &=& \#(c_i = \ell) \\
    f(y_i|\psi_i)&=& p_j\mbox{  see (\ref{eq:model2})},
\end{eqnarray*}
then, for $i=1, \ldots, n$
\begin{equation*}
    p(c_i=\ell|\bc_{-i}) \propto \left\{ \begin{array}{lc} \frac{n_{-i, \ell}}{n-1+m}f(y_i|\psi_i)&
        \mbox{ if } n_{-i, \ell} >0 \\[5pt] \frac{m}{n-1+m}H_i& \mbox{ if } n_{-i, \ell} =0\end{array} \right.,
\end{equation*}
where
\begin{equation*}
        H_i = \int f(y_i|\psi)g(\psi)d\psi.
\end{equation*}
This then sets up the full conditional distributions for Gibbs sampling (see page~\pageref{first.gibbs}).  

\section{Bayesian Shrinkage} \index{subjectindex}{shrinkage|(}
The prior distribution works to move the posterior away from the likelihood and toward its own position.  In cases where sharply defined 
priors are specified in the form of distributions with small variance, Bayesian estimates will have lower variance than corresponding 
classical likelihood-based estimates.  Furthermore, the greater the correlation between coefficients in a given model, the greater the 
extent of the ``shrinkage'' toward the prior mean\index{subjectindex}{shrinkage}.  As we shall see in Chapter~\ref{Hierarchical.Chapter}, 
it is often the case that models with hierarchical specifications (multiple levels of priors) display more shrinkage due to correlations 
between parameters.

As a specific example of shrinkage from Hartigan\index{authorindex}{Hartigan, J. A.} (1983, p.88), consider the normal model in 
Chapter~\ref{Normal.Model.Chapter} with prior mean $m$ and variance $s^2$ for $\mu$ and $\sigma_0$ known.  Rewrite the posterior mean 
according to: \index{subjectindex}{shrinkage!example}
\begin{align}\label{normal.mean.shrinkage}
        \hat{\mu} &=\left(\frac{m}{s^2} + \frac{n\bar{x}}{\sigma_0^2}\right)
		   \Bigg/\left(\frac{1}{s^2} + \frac{n}{\sigma_0^2}\right), 				\nonumber\9
		  &= \frac{\sigma_0^2}{\sigma_0^2 + ns^2}m + \frac{ns^2}{\sigma_0^2 + ns^2}\bar{x} 	\nonumber\9
		  &= (S_f)m + (1-S_f)\bar{x},
\end{align}
where $S_f=\sigma_0^2/(\sigma_0^2 + ns^2)$ is the shrinkage factor that is necessarily bounded by $[0\negthickspace:\negthickspace 1]$.  
This shows that the shrinkage factor gives the \emph{proportional} distance that the posterior mean has shrunk back to the prior mean away 
from the classical maximum likelihood estimate $\bar{x}$.  The form of the shrinkage factor in this case highlights the fact that large 
data variance means that the denominator will dominate and there will be little shrinkage.  

The posterior variance in the normal-normal model can also be rewritten in similar fashion:
\begin{align}\label{normal.variance.shrinkage} \index{subjectindex}{normal variance shrinkage}
	\hat{\sigma}^2 	&= \left(\frac{1}{s^2} + \frac{n}{\sigma_0^2}\right)^{-1} 		\nonumber\9
			&= \frac{\sigma_0^2s^2}{\sigma_0^2 + ns^2} 
			 = \left[ \frac{\sigma_0^2}{\sigma_0^2 + ns^2} \sigma_0^2 \right]^{\half}
			   \left[ \frac{ns^2}{\sigma_0^2 + ns^2} \frac{s^2}{n} \right]^{\half}	\nonumber\9
			&= (S_f\sigma_0^2)^{1/2} \left((1-S_f)\frac{s^2}{n}\right)^{1/2}.
\end{align}
This is interesting because it shows that the posterior variance is a product of the 
square root of the prior variance weighted by the shrinkage factor and the square root of the 
data variance weighted by the complement of the shrinkage factor.  So we see specifically how 
the shrinkage factor determines the compromise between prior uncertainty and data uncertainty.
\index{subjectindex}{shrinkage!relationship to variance} \index{subjectindex}{uncertainty!prior}

It is noteworthy to look at both \eqref{normal.mean.shrinkage} and \eqref{normal.variance.shrinkage}
when the prior distribution is very diffuse.  In the extreme, if we pick an 
improper prior\index{subjectindex}{prior distribution!improper} by
setting $s^2 = \infty$ (or perhaps just some huge value), then it is clear that 
$S_f \rightarrow 0$, and the likelihood model dominates.  Conversely, if $s^2$ is close to
zero, reflecting strong prior knowledge about the distribution of $\mu$, then $S_f$ is close
to one and the prior dominates.

Of course shrinkage is not just a feature of normal models or even models based on location-scale distributions.  
Returning to the Beta-Binomial conjugate setup given in Example~\ref{betabinomial.example}: 
\begin{equation}
	Y \sim \mathcal{BN}(n,p)   \qquad\qquad        p \sim \mathcal{BE}(A,B)
\end{equation}
where $A$ and $B$ are fixed prior parameters.  The posterior distribution for $p$ was shown to be:
\begin{equation}
	p|y \sim \mathcal{BE}(y+A,n-y+B),
\end{equation}
with:
\begin{equation}
        \hat{p} = \frac{ (y+A) }{ (y+A) + (n-y+B) } = \left[\frac{n}{A+B+n}\right] \left(\frac{y}{n} \right)+
                  \left[\frac{A+B}{A+B+n}\right] \left(\frac{A}{A+B} \right).
\end{equation}
Here $\frac{A+B}{A+B+n}$ is the shrinkage estimate where the degree of shrinkage is determined by the magnitude of 
$A+B$ relative to $n$.  So for large $n$ the shrinkage gets small since $n$ is in the denominator of the weight on
the prior mean.\index{subjectindex}{shrinkage|)}  While the shrinkage is overt in the example here, we can also
consider the effect for the more complicated priors forms described in this chapter.

\section{Exercises}
\begin{exercises}
        \item	\label{expo.gamma.exercise} 
		Show that both the exponential PDF and chi-square PDF are special cases of the gamma PDF.
		\index{subjectindex}{distribution!exponential} \index{subjectindex}{distribution!gamma}
		\index{subjectindex}{distribution!chi-square@$\chi^2$}

            % NEW
            \index{subjectindex}{iid}
    \item   Suppose you have an iid sample of size $n$ from the Rayleigh distribution:\index{subjectindex}{distribution!Rayleigh}
            \begin{equation*}
                f(x|\sigma) = \frac{x}{\sigma^2}\exp\left[ -\frac{1}{2\sigma^2}x^2 \right], \quad x \ge 0,\; \sigma > 0.
            \end{equation*}
            Produce the likelihood function for this distribution and assign an appropriate prior distribution for $\sigma^2$.
            %\begin{equation*}
            %   L(\x) = \frac{1}{\sigma^{2n}} \left(\prod_{i=1}^n x_i \right) 
            %       \exp\left[ -\frac{1}{2\sigma^2} \sum_{i=1}^n x_i^2 \right]
            %\end{equation*}

    \item 	(Pearson\index{authorindex}{Pearson, K.} 1920).  An event has occurred $p$ times in $n$ trials.
		    What is the probability that it occurs $r$ times in the next $m$ trials?  How would your answer
		    be interpreted differently in Bayesian terms than in standard frequentist terms?

            % NEW
    \item   Demonstrate that an improper prior distribution is a measure but not a probability measure.

    \item   Suppose that you had a prior parameter with the restriction: $[0<\eta<1]$.  If you believed that $\eta$ had prior 
            mean $0.4$ and variance $0.1$, and wanted to specify a beta distribution, what prior parameters would you 
            assign?\index{subjectindex}{distribution!beta}	

            % NEW
            \index{subjectindex}{iid}
    \item   Suppose $X_1,\ldots,X_n$ are iid $\mathcal{G}(\alpha,\beta)$, $Y_1,\ldots,Y_n$ are iid $\mathcal{G}(\alpha,\gamma)$
            and independent of the $X_i$.  Produce the distribution of $\bar{X}/\bar{Y}$.  % Shao p.56

    \item   Derive the Jeffrey's prior for a normal likelihood model under three circumstances: \emph{(1)} $\sigma^2=1$,
		    \emph{(2)} $\mu=1$, and \emph{(3)} both $\mu$ and $\sigma^2$ unknown (nonindependent).

            % NEW
    \item   For $X \sim \mathcal{N}(\mu,1)$, show that the prior distribution $\mu \sim \mathcal{N}(m,s^2)$ gives the 
            posterior distribution:
            \begin{equation*}
                \mu|X \sim \mathcal{N}\left( \frac{1}{1+s^2}m + \frac{s^2}{1+s^2}X, \frac{s^2}{1+s^2} \right).
            \end{equation*}.

	\item   \label{posterior.calculation.exercise}
	        (Robert 2001) Calculate the marginal posterior distributions for the following setups:\\[-3pt]
				\begin{bayeslist}	
					\item	$x|\sigma^2 \sim \mathcal{N}(0,\sigma^2)$,
						$1/\sigma^2 \sim \mathcal{G}(1,2)$.
					\item	$x|\lambda \sim \mathcal{P}(\lambda)$,
						$\lambda \sim \mathcal{G}(2,1)$.
					\item	$x|p \sim \mathcal{NB}(10,p)$,
						$p \sim \mathcal{BE}(0.5,0.5)$.
				\end{bayeslist}
				\vspace{6pt}
				\index{subjectindex}{distribution!normal} \index{subjectindex}{distribution!gamma}%
				\index{subjectindex}{distribution!Pareto} \index{subjectindex}{distribution!beta}%
				\index{subjectindex}{distribution!negative binomial}%
				
                % NEW
        \item   \label{exercise.haldane.prior}
                The Haldane prior\index{subjectindex}{prior distribution!Haldane} for parameter $\zeta \in (0\range 1)$ is given
                by $p(\zeta) \propto \zeta^{-1}(1-\zeta)^{-1}$.  Suppose we had data $X_1,\ldots,X_n$ Bernoulli distributed according to
                $\mathcal{BR}(x|\zeta) = \zeta^x(1-\zeta)^{1-x}$.  Show that the resulting posterior distribution is
                $\pi(\zeta|\X) \propto \zeta^{\sum x_i -1}(1-\zeta)^{n-\sum x_i -1}$.  What is the resulting posterior mean?  Can
                the Haldane prior be expressed as a special case of a beta distribution?  What happens if there are no zeros or no ones in
                the $\X$ data?  % posterior distribution is improper

        \item   Calculate the Jeffreys prior for the distributional forms in
                Exercise~\ref{Prior.Chapter}.\ref{posterior.calculation.exercise}.\index{subjectindex}{prior distribution!Jeffreys}%
	
                % NEW
        \item   The triangular distribution\index{subjectindex}{prior distribution!triangular} for $\theta$ is given with limits
        $(a,b)$ and mode $m$ by the expression: \begin{equation*} p(\theta|a,b,m) \begin{cases} 0                               &
        \theta < a \\ \frac{2(\theta-a)}{(b-a)(m-a)}  & a \le \theta \le m \\ \frac{2(b-\theta)}{(b-a)(b-m)}  & m \le \theta \le b
        \\ 0                               & \theta > b.  \end{cases} \end{equation*} Find the mean and variance of this
        distribution.  Produce a posterior distribution for $\theta$ assuming data that are distributed $\mathcal{N}(\theta,1)$.

	    \item   \label{example.time.series}
                The Bayesian framework is easily adapted to problems in time-series.
				One of the most simple time-series specifications is the 
				\begin{normalfont}AR(1)\end{normalfont}, which
				assumes that the previous period's outcome is important in the current
				period estimation. \index{subjectindex}{time-series!Bayesian}
				\index{subjectindex}{AR(1) model} 

				Given an observed outcome variable vector, $\mathbf{Y}_t$ measured at time $t$, 
				\begin{normalfont}AR(1)\end{normalfont} model for $T$ periods is:\\[-3pt]
				\vspace{-5pt}
				\begin{align*}
        				\mathbf{Y_t}    &= \mathbf{X}_t \B + \epsilon_t  \nonumber \\
        				\epsilon_t      &= \rho \epsilon_{t-1} + u_t, \qquad |\rho|< 1 \nonumber \\
        				u_t             &\sim \text{\begin{normalfont}iid\end{normalfont}} 
							\; \mathcal{N}(0,\sigma_u^2). \label{AR1.specification}
				\end{align*}
				\vspace{6pt}
				Here $\mathbf{X}_t$ is a matrix of explanatory variables at time $t$, $\epsilon_t$ 
				and $\epsilon_{t-1}$ are residuals from period $t$ and $t-1$, respectively, $u_t$ 
				is an additional zero-mean error term for the autoregressive relationship, and 
				$\B$, $\rho$, $\sigma_u^2$ are unknown parameters to be estimated by the 
				model.  Backward substitution through time to arbitrary period $s$ gives 
				$\epsilon_t = \rho^s\epsilon_{t-s} 
				+ \sum_{j=0}^{T-1}\rho^j u_{t-j}$, and since $E[\epsilon_t] = 0$, then 
				$\text{var}[\epsilon_t] =  E[\epsilon_t^2] = \frac{\sigma_u^2}{1-\rho^2}$, and the 
				covariance between any two errors is: $\Cov[\epsilon_t,\epsilon_{t-j}] = 
				\frac{\rho^j \sigma_u^2}{1-\rho^2}$.  Assuming asymptotic normality, this setup 
				leads to a general linear model with the following tridiagonal $T \times T$ weight 
				matrix (Amemiya 1985, p.164):\index{authorindex}{Amemiya, T.}
				\begin{equation*}
				    \mathbf{\Omega} = \frac{1}{\sigma_u^2}\left[ \begin{array}{rrrrrrr}
        				1       & -\rho & & & & & \\
        				-\rho   & (1+\rho^2) & -\rho & & & 0 & \\
                				& & & \ddots & & & \\
                				& \multicolumn{1}{l}{0} & & & -\rho & (1+\rho^2) & -\rho \\
                				& & & & & -\rho & 1 \\
    				    \end{array} \right].
				\end{equation*}

				Using the following data on worldwide fatalities from terrorism compared
				to some other causes per 100,000 (source: Falkenrath\index{authorindex}{Falkenrath, R.} 
				2001), develop posteriors for $\B$, $\rho$, and
				$\sigma_u$ for each of the causes ($Y$) as a separate model stipulating the date 
				(minus 1983) as the explanatory variable ($X$).  Specify conjugate priors
				(Berger and Yang\index{authorindex}{Berger, J. O.} \index{authorindex}{Yang, R.} [1994] show some 
				difficulties with nonconjugate priors here), or use a truncated normal for the prior on $\rho$  
				(see Chib and Greenberg [1998]). \index{authorindex}{Chib, S.} \index{authorindex}{Greenberg, E.}    
				\index{subjectindex}{terrorism exercise} Can you reach some conclusion about the differences in 
				these four processes?\\

                \parbox[c]{\linewidth}{
				\blsref \begin{normalfont} %\begin{small} 
				\begin{tabular}{lcccc}
					    &		&		&		&		\7
				    Year	& Terrorism	& Car Accidents	& Suicide 	& Murder	\\
				    \hline
					    &		&		&		&		\7
				    1983	& 0.116		& 14.900	& 12.100	& 8.300		\\
				    1984	& 0.005		& 15.390	& 12.420	& 7.900		\\
				    1985	& 0.016		& 15.150	& 12.380	& 8.000		\\
				    1986	& 0.005		& 15.920	& 12.870	& 8.600		\\
				    1987	& 0.003		& 19.106	& 12.710	& 8.300		\\
				    1988	& 0.078		& 19.218	& 12.440	& 8.400		\\
				    1989	& 0.006		& 18.429	& 12.250	& 8.700		\\
				    1990	& 0.004		& 18.800	& 11.500	& 9.400		\\
				    1991	& 0.003		& 17.300	& 11.400	& 9.800		\\
				    1992	& 0.001		& 16.100	& 11.100	& 9.300		\\
				    1993 	& 0.002		& 16.300	& 11.300	& 9.500		\\
				    1994	& 0.002		& 16.300	& 11.200	& 9.000		\\
				    1995	& 0.005		& 16.500	& 11.200	& 8.200		\\
				    1996	& 0.009		& 16.500	& 10.800	& 7.400		\\
				\end{tabular} %\end{small} 
                \end{normalfont} \bls
				}
                \vspace{11pt}

            % NEW
    \item   The number of cases needed to give a $1-\alpha$ confidence interval of width $\omega$ for a proportion $p$ is
            $n = (z_{\alpha})^2 p(1-p)/\omega^2$.  Plot $n$ versus $\omega$ at $\alpha = 0.05$ for $p=(0.01, 0.3, 0.5, 0.8)$.

	\item Laplace\index{authorindex}{Laplace, P. S.} (1774) wonders what the best choice is for a posterior 
				point estimate.  He sets three conditions for the shape of the posterior: 
				symmetry, asymptotes, and properness (integrating to one).  In addition, 
				Laplace tacitly uses uniform priors.  He proposes two possible criteria for 
				selecting the estimate:

				 \begin{bayeslist}
                		\item   La primi\'{e}re est l'instant tel qu'en \'{e}galement probable 
					que le v\'{e}ritable instant du ph\'{e}nom\`{e}ne tombe avant ou 
					apr\`{e}s; on pourrait appeler cet instant milieu de probabilit\'{e}.\\[7pt]
                        		{\bf Meaning:} use the median.

                		\item   Le seconde est l'instant tel qu'en le prenant pour milieu, la somme 
					des erreurs \`{a} craindre, multipli\'{e}es par leur probabilit\'{e}, 
					soit un minimum; on pourrait l'appeler milieu d'erreur ou milieu 
					astronomique, comm \'{e}tant celui auquel les astronomes doivent 
					s'arreter de pr\'{e}f\'{e}rence.\\[7pt]
                        		{\bf Meaning:} use the quantity at the ``astronomical center of mass'' 
					that minimizes: $f(x) = \int|x-V|f(x)dx$.  In modern terms, this is 
					equivalent to minimizing the posterior expected loss: 
					$E[L(\theta,d)|x] = \int_\Theta L(\theta,d)\pi(\theta|x)d\theta$,
					which is the average loss defined by the posterior distribution
					and $d$ is the ``decision'' to use the posterior estimate of $\theta$
					(see Berger [1985]).  \index{subjectindex}{posterior expected loss}
					\index{subjectindex}{loss function}\index{authorindex}{Berger, J. O.}
				\end{bayeslist}
 
				Prove that these two criteria lead to the same point estimate.

            % NEW
    \item   In their popular text Gelman and Hill (2007)\index{authorindex}{Gelman, A.} \index{authorindex}{Hill, J.} often
            specify variance components priors according to the following two steps:
            \begin{align*}
                \sigma \sim \mathcal{U}(0,100) \\
                \tau  = \sigma^{-2},
            \end{align*}
            where $\sigma$ is a standard error and $\tau$ is a precision.  Explain why this might not be a good idea for
            probit regression models.

    \item Review one body of literature in your area of interest and develop a detailed plan for creating elicited priors.

            % NEW
    \item   Kadane and Wolfson (1996)\index{authorindex}{Kadane, J. B.} \index{authorindex}{Wolfson, L. J.} specify a
            normal linear model for elicitation according to:
            \begin{align*}
                \Y|\X,\B,\sigma^2  & \sim \mathcal{N}(\X\B,\sigma^2\I) \\
                \B                 & \sim \mathcal{N}(b,\sigma^2\mathbf{R}^{-1}) \\
                \frac{1}{\sigma^2} & \sim \frac{1}{w\delta}\mathcal{G}(\delta/2,2).
            \end{align*}
            Develop a program to elicit scalars $b$, $w$, $\delta$, and matrix $\mathbf{R}$ using a series of quantile
            questions. % Berry and Stangle, Bayesian Biostastics p.163

    \item   \label{executions.example}
            Test a Bayesian count model for the number of times that capital punishment is implemented on a state level 
            in the United States for the year 1997.  Included in the data below (source: United States Census Bureau, 
            United States Department of Justice) are explanatory variables for: median per capita income in dollars, the 
            percent of the population classified as living in poverty, the percent of Black citizens in the population, 
            the rate of violent crimes per 100,000 residents for the year before (i.e., 1996), a dummy variable to indicate 
			whether the state is in the South, and the proportion of the population with a college degree of some kind.  
            The data are given below and available in the \texttt{BaM} dataset \texttt{executions}.
            \index{subjectindex}{example!death penalty}

            In 1997, executions were carried out in 17 states with a national total of 74.  The model should be developed 
            from the Poisson link function, $\T =\text{\normalfont{log}}(\M)$, with the objective of finding the best 
            $\B$ vector in:
            \begin{equation*}
                \underbrace{g^{-1}(\T)}_{17 \times 1} = 
				 \exp\bigg[ \texttt{1}\beta_0 + \texttt{INC}\beta_1 + \texttt{POV}\beta_2 
                    + \texttt{BLK}\beta_3 + \texttt{CRI}\beta_4 + \texttt{SOU}\beta_5 + \texttt{DEG}\beta_6
                           		\bigg].  
            \end{equation*}
            Specify a suitable prior with assigned prior parameters, then summarize the resulting posterior distribution.

            \parbox[c]{\linewidth}{
			    \blsref \begin{normalfont} \begin{center} \begin{footnotesize} 
                \hspace{-33pt}
			    \begin{tabular}{p{0.42in}cccccp{0.22in}c}
					            &        &        &         &         &        &                 & \7
                        		& Exe-  & Median & Percent & Percent & Violent &       & Prop.\\
        			State       & cutions & Income & Poverty & Black & Crime   & \centering{South} & Degrees\\
					            &        &        &         &         &        &                 & \7
        			\hline
					            &        &        &         &         &        &                 & \7
        			Texas       &  37    &  34453 &  16.7   &  12.2   & ~644   & \centering{~~1} & 0.16 \\[2pt]
        			Virginia    &  ~9    &  41534 &  12.5   &  20.0   & ~351   & \centering{~~1} & 0.27 \\[2pt]
        			Missouri    &  ~6    &  35802 &  10.6   &  11.2   & ~591   & \centering{~~0} & 0.21 \\[2pt]
        			Arkansas    &  ~4    &  26954 &  18.4   &  16.1   & ~524   & \centering{~~1} & 0.16 \\[2pt]
        			Alabama     &  ~3    &  31468 &  14.8   &  25.9   & ~565   & \centering{~~1} & 0.19 \\[2pt]
        			Arizona     &  ~2    &  32552 &  18.8   &  ~3.5   & ~632   & \centering{~~0} & 0.25 \\[2pt]
        			Illinois    &  ~2    &  40873 &  11.6   &  15.3   & ~886   & \centering{~~0} & 0.25 \\[2pt]
        			S.\,Carolina&  ~2    &  34861 &  13.1   &  30.1   & ~997   & \centering{~~1} & 0.21 \\[2pt]
        			Colorado    &  ~1    &  42562 &  ~9.4   &  ~4.3   & ~405   & \centering{~~0} & 0.31 \\[2pt]
        			Florida     &  ~1    &  31900 &  14.3   &  15.4   & 1051   & \centering{~~1} & 0.24 \\[2pt]
        			Indiana     &  ~1    &  37421 &  ~8.2   &  ~8.2   & ~537   & \centering{~~0} & 0.19 \\[2pt]
        			Kentucky    &  ~1    &  33305 &  16.4   &  ~7.2   & ~321   & \centering{~~0} & 0.16 \\[2pt]
        			Louisiana   &  ~1    &  32108 &  18.4   &  32.1   & ~929   & \centering{~~1} & 0.18 \\[2pt]
        			Maryland    &  ~1    &  45844 &  ~9.3   &  27.4   & ~931   & \centering{~~0} & 0.29 \\[2pt]
        			Nebraska    &  ~1    &  34743 &  10.0   &  ~4.0   & ~435   & \centering{~~0} & 0.24 \\[2pt]
        			Oklahoma    &  ~1    &  29709 &  15.2   &  ~7.7   & ~597   & \centering{~~0} & 0.21 \\[2pt]
        			Oregon      &  ~1    &  36777 &  11.7   &  ~1.8   & ~463   & \centering{~~0} & 0.25 \\
					            &        &        &         &         &        &                 & \7
        	    \end{tabular} \end{footnotesize} \end{center} \end{normalfont} \bls
            }

            % NEW
    \item   \label{zellner.g.prior.exercise} Zellner's\index{authorindex}{Zellner, A.} g-prior can be expressed most simply 
            in a linear regression context as:
            \begin{align*}
                \B|\tau &\sim \mathcal{N}\left( \mathbb{B}, \frac{g}{\tau}(\X'\X){-1} \right) \\
                p(\tau) &\propto \frac{1}{\phi},
            \end{align*}
            \noindent where $\mathbb{B}$ is the prior mean, and $\tau$ is the usual homoscedastic precision parameter, meaning that the
            covariance matrix of the $\B$ is given by $\I/\tau$.  Common choices for $g$ include $g=n$ (the data size), $g=k^2$
            (the square of the number of explanatory variables in the model), and $g=\max(n,k^2)$.  Rerun the model in
            Exercise~\ref{Linear.Chapter}.\ref{meier.keiser.exercise} with this prior using your preferred value of $g$.  How do
            the results differ?  \index{subjectindex}{prior distribution!Zellner's g-prior}

\end{exercises}

\thispagestyle{empty}
\chapter{Some Markov Chain Monte Carlo Theory}\label{MCMC.Theory.Chapter}
\setcounter{examplecounter}{1}

\section{Motivation}
This chapter revisits the theoretical basis for Markov chain Monte Carlo but with greater detail and more attention to issues of 
convergence.  Most of the technical content is intended to give a greater appreciation for the underlying mathematical process 
of Markov chains and therefore an understanding of the issues involved in convergence and mixing.  Both convergence and mixing 
behavior of Markov chains affect the final inferences made with Bayesian models using MCMC.  Readers who wish to move onto more 
practical considerations can proceed to Chapter~\ref{MCMC.Utilitarian.Chapter} and return to this information as needed.

\section{Measure and Probability Preliminaries}\label{measure.prob.section}
\index{subjectindex}{measure space} \index{subjectindex}{probability measure}\index{subjectindex}{sigma-algebra@$\sigma$-algebra}
\index{authorindex}{Billingsley, P.} \index{authorindex}{Doob, J. L.} \index{authorindex}{Ross, S.}  
\index{subjectindex}{sigma-algebra@$\sigma$-algebra} 
First we return to the idea of a \emph{stochastic process}.\index{subjectindex}{stochastic process} A stochastic process is a set 
of observed values $\theta^{[t]}$ ($t \ge 0$) on the probability space: $(\Omega,\mathcal{F},\mathfrak{P})$ where the superscript $[t]$ 
denotes the serial order of occurrence, $\Omega$ is the non-empty outcome space with the associated $\sigma$-algebra $\mathcal{F}$ 
and probability measure $\mathfrak{P}$ (Billingsley 1995, Doob 1990, Ross 1996).  \index{subjectindex}{probability measure space}
We say that $\mathcal{F}$ is 
the associated \emph{field} or \emph{algebra} of subsets of $\Omega$ if: $\Omega \in \mathcal{F}$, it is closed under 
complementation as well as intersections of its subsets, and a $\sigma$-field or $\sigma$-algebra if it is also closed under 
countable unions of subsets.  These subsets can be individual elements, denoted $\theta$, or sets of 
such elements, denoted $A,B,C,\ldots$.  A function $\mathfrak{P}$ is a probability measure on $\Omega$ if it maps subsets of 
$\Omega$ to $[0\range 1]$ according to the Kolmogorov axioms: \emph{(1)} $p(A) \in [0\range 1] \;\;\forall A \in \mathcal{F}$, 
\emph{(2)} $p(\Omega) = 1$, $p(\varnothing) = 0$, and \emph{(3)} for non-overlapping multiple $A_i$, 
$P\left(\bigcup_{i=1}^{n}A_i\right) = \sum_{i=1}^{n}p(A_i)$ (even if $n=\infty$).  \index{subjectindex}{Kolmogorov axioms of probability}
\index{authorindex}{Billingsley, P.}\index{authorindex}{Gill, J.} These were stated more colloquially on page~\pageref{first.kolmogorov}, 
but see also Billingsley (1995, Chapter~2) for an in-depth explanation or Gill (2006, Chapter~7) for an accessible introduction.  Now 
$(\Omega,\mathcal{F},\mathfrak{P})$ is the \emph{probability measure space} for $\theta$ often shortened to the \emph{state space}.
\index{subjectindex}{state space}\index{subjectindex}{probability measure space} 

\index{subjectindex}{stochastic process!random elements} 
\index{authorindex}{Karlin, S.}\index{authorindex}{Taylor, H. M.}
\index{authorindex}{Hoel, P. G.}\index{authorindex}{Port, S. C.}\index{authorindex}{Stone, C. J.}
The sequence of $\Omega$-valued $\theta^{[t]}$ random \emph{elements}, indicated by $t=0,1,\ldots$, defines the $\Omega$-valued
stochastic process.  Typically this is just $\Re$-valued with restrictions (Karlin and Taylor 1981, 1990, Hoel, Port, and Stone
1987).  By convention, the sequence is labeled with consecutive even-spaced intervals: $T\negmedspace :\{\theta^{[t=0]},
\theta^{[t=1]}, \theta^{[t=2]}, \ldots\}$.  This configuration can be generalized to non-consecutive labels or non-equal time
periods, although it does not provide any additional utility for current purposes.

At time $t$ the \emph{history}\index{subjectindex}{stochastic process!history} of the stochastic process is the increasing 
series of sub-$\sigma$-algebras defined by: $\mathcal{F}_0 \subseteq \mathcal{F}_1 \subseteq \ldots \mathcal{F}_t$ where 
$\theta$ is measurable on each one.  A $T$-valued stochastic process,\index{subjectindex}{stochastic process!$T$-valued} 
with transition probability $p$, and initial (starting) value $\theta_0$, is a \emph{Markov chain}, 
$\mathfrak{M}(\T_t,t \ge 0)$, \index{subjectindex}{Markov chain!definition} if at the arbitrary $(t+1)$st time point in 
$T$, the following is true:
\begin{equation}
    p(\theta^{[t+1]}|\mathcal{F}_t) =  p(\theta^{[t+1]}|\theta^{[t]}), \; \forall t\ge 0
\end{equation}
(Zhenting and Qingfeng\index{authorindex}{Zhenting, H.} \index{authorindex}{Qingfeng, G.} [1978, Chapter~6]).
As stated previously in Chapter~\ref{MCMC.Chapter}, the only component of the history that matters in 
determining movement probabilities to an arbitrary set $A$, which we define as a collection of individual
elements, for Markov chains at the current step, is the present realization of the stochastic process:  
\index{subjectindex}{Markov chain!memory-less property}\index{subjectindex}{Markovian property}
\begin{equation}
    p(\theta^{[t+1]} \in A|\theta^{[0]}, \theta^{[1]},\ldots, \theta^{[t-1]}, \theta^{[t]})
        = p(\theta^{[t+1]} \in A|\theta^{[t]}).
\end{equation}
This is the defining \emph{Markovian property} that distinguishes a Markov chain from the general class of stochastic 
processes.  \index{subjectindex}{Markov chain Monte Carlo (MCMC)!theory|)}\index{subjectindex}{Markovian property}

Retain $\Omega$ as the space giving the support of $\theta$, a random variable where components are denoted with 
subscripts: $\theta_i, \theta_j, \theta_k,\ldots$.  Here $\mathcal{F}$ is again the associated $\sigma$-algebra and $\mathfrak{P}$ 
is a probability measure on $\Omega$.  Now use $f, g, h,\ldots$ to denote real-valued measurable functions defined on 
this state space,  \index{subjectindex}{measurable functions} 
and define $\mathcal{M}$ as the full collection of signed measures\index{subjectindex}{measures!signed} on measure space 
$(\Omega,\mathcal{F})$ where $\lambda$ and $\mu$ denote elements by notational convention.  The class of \emph{positive} 
measures\index{subjectindex}{measures!positive} is given by: $\mathcal{M}^+ = \{\lambda \in \mathcal{M}:\lambda(T)>0)$, 
and includes $\mathfrak{P}$.  

A Markov chain transition kernel,\index{subjectindex}{transition kernel} $K(\theta,A)$, is the mechanism for describing the 
probability structure of the Markov chain\index{subjectindex}{Markov chain!movement}, $\mathfrak{M}(\T_t,t \ge 0)$: a 
probability measure for all $\theta$ points in the state space to the set $A \in \mathcal{F}$.  Thus, it is a 
mapping of the potential transition events\index{subjectindex}{Markov chain!transitioning} to their probability of 
occurrence (Robert and Casella 2004, p.208).\index{authorindex}{Robert, C. P.}\index{authorindex}{Casella, G.}  
Formally, $K$ is a non-negative, $\sigma$-finite kernel on the state space $(\Omega,\mathcal{F})$ that provides the mapping 
$\Omega \times \mathcal{F} \rightarrow \Re^+$ given the following three conditions:
\begin{enumerate}
    \item   for every finite subset $A \in \mathcal{F}$, $K(\bullet,A)$ is measurable,
    \item   for every point $\theta_i \in \Omega$: $K(\theta_i,\bullet)$ is a measure on the state space,
    \item   there is a positive measurable function, $f(\theta_i,\theta_j),\; \forall 
	    \theta_i, \theta_j \in \Omega$ where $\int K(\theta_i,d\theta_j)f(\theta_i,\theta_j) < \infty$.
\end{enumerate}
\index{subjectindex}{Markov chain!measure conditions}
\index{subjectindex}{Markov chain!sigma finite kernel@$\sigma$-finite kernel}

This preliminary discussion sets up the discussion of Markov chain properties and characteristics.  Importantly, a Markov 
chain is just a regular probability mechanism with two added features: time-seriality, and a memory-less decision-process.

\section{Specific Markov Chain Properties}
Having defined a stochastic process of interest, we now return to the core properties of Markov chains given in 
Chapter~\ref{MCMC.Chapter} but with greater attention to underlying theoretical details.  These properties are important
because they lead us to convergence of the Markov chain, without which no practical statistical inference can be made.

\index{subjectindex}{Markov chain!properties!irreducible}\index{subjectindex}{psi-irreducibility@$\psi$-irreducibility}%
\subsection{$\psi$-Irreducibility}
Irreducibility is a description of accessibility.  A set, $A$, is \emph{irreducible} if every point or collection of points 
in $A$ can be reached from every other point or collection of points in $A$. 
The associated Markov chain is irreducible if it operates on an irreducible set as its state space.  
\index{subjectindex}{Markov chain!irreducible set}  Define now $\psi$ as a positive $\sigma$-finite measure on $(\Omega,\mathcal{F})$ 
with arbitrary $A \in \mathcal{F}$ such that $\psi(A)>0$.  Given the transition kernel $K(\theta,A)$, if every positive 
$\psi$-subset, $A' \subseteq A$, can be reached from every part of $A$, then $A$ is called \emph{$\psi$-communicating}.  
When the full state space for the Markov  chain $T$ is $\psi$-communicating, then the kernel that defines the Markov chain is 
$\psi$-irreducible.  Typically, we assume that $\psi$-irreducible here is maximally $\psi$-irreducible: for any \emph{other} 
positive $\sigma$-finite measure on $(\Omega,\mathcal{F})$, $\psi'$, it is necessarily true that $\psi > \psi'$ (Meyn and 
Tweedie 1993, pp.88-89). \index{authorindex}{Meyn, S. P.}\index{authorindex}{Tweedie, R. L.}
\index{subjectindex}{Markov chain!psi-irreducible!maximally@Markov chain!$\psi$-irreducible!maximally}
\index{subjectindex}{Markov chain!psi-communicating@$\psi$-communicating}  

\subsection{Closed and Absorbing Sets}
\index{subjectindex}{Markov chain!absorbing condition}\index{subjectindex}{Markov chain!unobtainable set} 
\index{subjectindex}{Markov chain!closed set}\index{subjectindex}{Markov chain!obtainable set} 
A non-empty set $A \in \mathcal{F}$ is \emph{obtainable} from $\theta$ for the Markov chain defined at time $t$ by the 
kernel $K^t$ if $K^t(\theta,A) > 0$, for some $t \ge 1$, and \emph{unobtainable} if $K^t(\theta,A) = 0,\, 
\text{for all} \;t \ge 1$.  Here, $A$ is called \emph{closed} for $K$ if $A^c$ is not obtainable from $A$: 
$K^t(\theta,A^c) = 0,\, \text{for all} \;\theta \in A, \text{and all} \;t \ge 1$.  The condition of \emph{absorbing}
(Revuz 1975) is more restrictive than closed: $K(\theta,A) = K(\theta,\Theta) = 1, \; \text{for all} \;\theta \in A$, 
since it is possible under the closed condition, but impossible under the absorbing condition, that $K(\theta,A) = 
K(\theta,\Theta) \ne 1$, for some $\theta \in A$.  So a closed set can have subsets that are unavailable but an absorbing 
state fully communicates with all of its subsets.
\index{subjectindex}{Markov chain!properties!communicating} 
\index{subjectindex}{Markov chain!absorbing set}\index{authorindex}{Revuz, D.} 

\subsection{Homogeneity and Periodicity} \index{subjectindex}{Markov chain!chain period}  
A Markov chain is \emph{homogeneous}\index{subjectindex}{Markov chain!properties!homogeneous} at the $t$th step if the 
transition probabilities at this step do not depend on the value of $t$.  The \emph{period} of a Markov chain is
the length of time to repeat an identical cycle of values, and it is desirable that the Markov chain not have such 
a defined cycle.\index{subjectindex}{Markov chain!cycle}   A chain that does not repeat in this fashion is called 
\emph{aperiodic}.\index{subjectindex}{Markov chain!properties!aperiodic}  We can easily deduce that a \emph{periodic} 
Markov chain is non-homogeneous because the cycle of repetitions defines transitions based on specific times.
\index{subjectindex}{Markov chain!properties!periodic}

Define now $T(A)$ as the first \emph{hitting time} (shortest return time to an arbitrary sub-space) for the Markov chain 
$\mathfrak{M}(\theta_t,t \ge 0)$ with invariant distribution $\pi$ to the set $A \in \mathcal{F}$ not including time 0: 
$T(A) = \inf(t\ge 1\range\theta_t \in A)$.  Athreya, Doss, and Sethuraman (1996, p.72) stipulate two conditions that 
lead to convergence towards the invariant distribution, which is our eventual goal.  
\index{authorindex}{Athreya, K. B.} \index{authorindex}{Doss, H.} \index{authorindex}{Sethuraman, J.}
The first condition says that for every element in $A$ the probability that the hitting time is less than infinity is 
greater than zero for all of these elements, with probability $1$ under the invariant distribution.  The second condition 
forces the transition probability from one of these elements to any value in the space to be greater than or equal to the 
probability under the measure up to a constant.  Periodicity violates the second condition because it means that at any 
arbitrary step of the chain we can define a set $D$ as the next value in the sequence specified by a period of moves.  

\subsection{Null and Positive Recurrence}
Putting these previous definitions together, a  homogeneous, $\psi$-irreducible Markov chain on a closed set (discrete
or continuous and bounded) is \emph{recurrent} or persistent with regard to the set, $A$, if the probability that the 
chain occupies each subset of $A$ infinitely often over unbounded time is one.  
\index{subjectindex}{Markov chain!properties!recurrent} \index{subjectindex}{Markov chain!properties!persistent} 
When a chain moves into a recurrent state, it stays there forever and eventually visits every sub-state an infinite number of times.  
Furthermore, a recurrent Markov chain is \emph{positive recurrent} if the average time to return to $A$ is bounded, 
otherwise it is a \emph{null recurrent} Markov chain (Doeblin 1940).\index{authorindex}{Doeblin, W.}
\index{subjectindex}{Markov chain!properties!positive recurrent} 
\index{subjectindex}{Markov chain!properties!positive recurrent}

In the case of unbounded continuous state spaces, we have to work with a slightly more complicated version of 
recurrence.  \index{subjectindex}{Markov chain!unbounded and continuous state spaces} Define first for a set $A$ and all 
elements $x$ in $A$, the \emph{expected number of visits by the Markov chain to $x$ in the limit}: 
$\eta_x = \sum_{n=1}^{\infty} I_{(\theta_n \in x)}$, which is a function that counts visits to $x$.  
\index{subjectindex}{Markov chain!expected number of visits} Now we say a $\psi$-irreducible Markov chain is 
\emph{Harris recurrent} if there is  $\sigma$-finite probability measure $\psi$ for $(\Omega,\mathcal{F})$ such that at 
time $n$ it has the property: $\psi(A) > 0,\; \forall A \in \mathcal{F}$ (Harris 1956, Athreya and Ney 1978).  
The new definition for unbounded continuous state 
spaces is required because a $\psi$-irreducible chain with an invariant distribution on an unbounded continuous state 
space that is \emph{not} Harris recurrent has a positive probability of getting stuck indefinitely in an area bounded 
away from convergence, given a starting point there.  The Harris definition allows us to avoid worrying about the 
existence of a pathological null set in $\Re^k$.  The standard MCMC algorithms implemented on finite state computers 
are Harris recurrent (Tierney 1994).  \index{authorindex}{Tierney, L.}  
\index{subjectindex}{Markov chain!properties!Harris recurrent}\index{authorindex}{Harris, T. E.}
\index{authorindex}{Athreya, K. B.}\index{authorindex}{Ney, P.}  

\subsection{Transience}
Now consider the number of visits to the set itself, rather than specific elements in the set: for a set $A$, the 
\emph{expected number of visits by chain $\theta_n$ to $A$ in the limit}: $\eta_A = \sum_{n=1}^{\infty} 
I_{(\theta_n \in A)}$, where the $I()$ function counts hits on $A$.  Transience and recurrence can both be defined 
in terms of expectation for this $\eta_A$:\index{subjectindex}{Markov chain!properties!transient} $A$ is 
\emph{uniformly transient} if there exists a scalar $M <\infty$ such that $E[\eta_A] \le M \;\forall\theta\in A$.  A 
single state, $\theta$, in the discrete state space case is \emph{transient} if: $E[\eta_\theta] < \infty$.  
Conversely, $A$ is \emph{recurrent} if: $E[\eta_A] = \infty \;\forall \theta \in A$.  A single state in the discrete 
state space case is \emph{recurrent} if: $E[\eta_\theta] = \infty$.  For proofs see: Meyn and Tweedie (1993, pp.182-3) 
and Nummelin (1984, p.28).  \index{authorindex}{Meyn, S. P.}\index{authorindex}{Tweedie, R. L.}\index{authorindex}{Nummelin, E.} 

The important theorem here is:
\begin{quote}
    {\bf Theorem.}  If $\mathfrak{M}(\T_t,t \ge 0)$ is a $\psi$-irreducible Markov chain with transition kernel $K(\theta,A)$, then 
		    it must either be transient or recurrent depending on whether it is defined on a transient or 
		    recurrent set $A$.\\ \index{subjectindex}{Markov chain!properties!recurrent}
		    \index{subjectindex}{Markov chain!transient}
\end{quote}
The proof is a direct consequence of Kolmogorov's zero-one law, and details are given in Billingsley (1995, 120).
\index{subjectindex}{Kolmogorov's zero-one law}\index{authorindex}{Billingsley, P.}

This means that there is a two-state world to worry about: the chain is either recurrent and we know that it will 
eventually settle into a stable distribution, or it is transient and it will never achieve stability.
\index{subjectindex}{Markov chain!stable distribution} 

We can also define the \emph{convergence parameter} of a kernel $rK$ as the real number $0 \le R < \infty$
on a closed set $A$ such that: $\sum_{0}^{\infty}r^nK^n < \infty$ for every $0 \le r < R$, and 
$\sum_{0}^{\infty}r^nK^n = \infty$ for every $R \ge r$.  \index{subjectindex}{Markov chain!convergence parameter}
It turns out that for $\psi$-irreducible Markov chains there always exists a finite $R$ that defines whether or not the 
kernel for this Markov chain is \emph{$R$-transient} if the first condition holds, and \emph{$R$-recurrent} if the 
second condition holds (Meyn and Tweedie 1993).  \index{authorindex}{Meyn, S. P.}\index{authorindex}{Tweedie, R. L.}
\index{subjectindex}{Markov chain!properties!R-recurrent@Markov chain!properties!$R$-recurrent}

\subsection{Markov Chain Stability}
Label $\pi(\theta)$ as the stationary (limiting) distribution of the Markov chain for $\theta$ on the state space $\Omega$,
\index{subjectindex}{Markov chain!stationary distribution} with transition probability $p(\theta_i,\theta_j)$ 
that gives the probability that the chain will move from arbitrary point $\theta_i$ to arbitrary point $\theta_j$,
as stipulated by the transition kernel $K$ (i.e., from the $i$th row of the transition matrix for a discrete state space).  
Naturally, this stationary distribution, $\pi(\theta)$, is really the posterior distribution of interest from some 
Bayesian model where marginalization is not possible or convenient analytically.  This stationary or invariant 
distribution satisfies the following condition from Chapter~\ref{MCMC.Chapter}:
\index{subjectindex}{Markov chain!invariant distribution (see stationary)}
\begin{align}\label{stationarity.requirement2}
        \sum_{\theta_i}\pi^{t}(\theta_i)p(\theta_i,\theta_j) = \pi^{t+1}(\theta_j)
            &\qquad \text{Discrete State Space}  \nonumber\\
    \int\pi^{t}(\theta_i)p(\theta_i,\theta_j)d\theta_i = \pi^{t+1}(\theta_j)
            &\qquad \text{Continuous State Space},
\end{align}
(see specifically page~\pageref{marginal.distribution.definition}).  Multiplication by the transition kernel and evaluating for 
the current point (summation for discrete sample spaces, integration for continuous sample spaces) produces the same 
marginal distribution, $\pi(\theta) = \pi(\theta) P$.  
\index{subjectindex}{Markov chain!marginal distribution}\index{subjectindex}{Markov chain!stationary distribution}%
Therefore the marginal distribution remains fixed when the chain reaches the stationary distribution and we can ignore 
superscripts giving iteration number for iteration purposes.  Once the chain reaches the stationary distribution 
(synonymously the equilibrium distribution, limiting distribution), its movement is dictated by 
the marginal distribution, $\pi(\theta)$ from that point on.  
\index{subjectindex}{Markov chain!limiting distribution}
\index{subjectindex}{Markov chain!equilibrium distribution (see stationary)}
A $\psi$-irreducible, aperiodic Markov chain is guaranteed to have exactly one such stationary distribution 
(H\"{a}ggstr\"{o}m 2002, p.37).\index{authorindex}{Haggstrom, O.@H\"{a}ggstr\"{o}m, O.} This is the critical theoretical basis for 
estimation with MCMC: if the stationary distribution of the Markov chain is the posterior distribution of interest, then 
we are certain to eventually get samples from this posterior.

\subsection{Ergodicity}
The ergodic theorem is the key link between the mechanical process of the Monte Carlo simulation and the inferential 
result of MCMC.  If a chain is positive (Harris if necessary) recurrent, and aperiodic on some state $A$, it is an 
\emph{ergodic} Markov chain (Tweedie 1975).\index{authorindex}{Tweedie, R. L.}  Importantly, ergodic Markov chains have 
the property that:\index{subjectindex}{Markov chain!ergodicity}
\index{subjectindex}{Markov chain!ergodicity}\index{subjectindex}{ergodic theorem}
\begin{equation}
    \underset{n\rightarrow \infty}{\text{lim}}\bigg|P^n(\theta_i,\theta_j) - \pi(\theta_j)\bigg| = 0,
\end{equation}
for all possible $\theta_i$, and $\theta_j$ in the subspace (Norris 1997, p.53).\index{authorindex}{Norris, J. R.}  So the 
transition probabilities of the chain have converged to those of the limiting distribution and therefore all future draws 
are treated as if from this marginal distribution of interest.  This means that once a specified chain is determined to 
have reached this ergodic state, inference comes from running the chain for some length of time and summarizing the 
empirical draws.  We have thus replaced analytical work (i.e., integrating over some difficult form) with empirical analysis.

Ergodic Markov chains provide the following result:
\begin{equation}
    \underset{t \rightarrow \infty}{\lim} \frac{1}{t} \sum f(\T_t) = \int_{\boldsymbol{\Theta}} f(\T)\pi(\T)d\T,
\end{equation}
proven originally by Doeblin (1940).  \index{authorindex}{Doeblin, W.} This result means that empirical averages for the 
function $f()$ converge to a probabilistic average of the function over the limiting distribution.  In fact, it is this 
principle that underlies and justifies all MCMC for Bayesian stochastic simulation; it is exactly the link between 
``Markov chain'' and ``Monte Carlo.'' 

Ergodicity gives a means of asserting eventual convergence (although not the only one), but it does not provide 
a firm bound on the time required to reach convergence to the limiting distribution.  There are actually 
multiple ``flavors'' of ergodicity that provide differing rates of convergence for the Markov chain, and these are 
described in Section~\ref{rates.of.convergence.section}.

\section{Defining and Reaching Convergence}\label{defining.reaching.convergence}
Return to the abstract measure space\index{subjectindex}{measure space} $(\Omega,\mathcal{F})$ with events $A, B, C,\ldots$ 
in $\Omega$, real-valued measurable functions $f, g, h,\ldots$ on $\mathcal{F}$, and signed measure
\index{subjectindex}{signed measure} $\mathcal{M}^+$ with elements $\lambda$ and $\mu$.  \emph{First}, define an appropriate norm 
operator.  The elementary form for a bounded signed measure, $\lambda$, is:
\begin{equation}\index{subjectindex}{norm operator}
    ||\lambda|| \equiv \underset{A \in \Omega}{\sup}\lambda(A)
             - \underset{A \in \Omega}{\inf}\lambda(A)
\end{equation}
which is just the total variation of $\lambda$.  \emph{Second}, assume that $K$ is $R$-recurrent given by probability 
measure $\mathfrak{P}$, and the stationary distribution is normed such that $\pi(h) = 1$ for $h$ on $\mathcal{F}$.  In addition, assume 
also that $$\mathfrak{M}(\T_t,t \ge 0)$$ is \index{subjectindex}{Markov chain!properties!R-recurrent@Markov chain!properties!$R$-recurrent}%
$R$-recurrent (discrete or bounded continuous space) or Harris $R$-recurrent \index{subjectindex}{Markov chain!properties!Harris 
$R$-recurrent} (continuous unbounded space) Markov chain with transition kernel $K$.  If $K$ has period $p \ge 2$ then by definition the associated 
Markov chain cycles between the states: $\{A_0, A_1, A_2,\ldots, A_{p-1}\}$.  The \emph{$\psi$-null set}, 
$\Psi=\{A_0 \cup A_1 \cup \cdots \cup A_{p-1}\}^c$, defines the collection of such states not visited in the $p$-length 
iterations, and we want to drive this to a set of size zero.

Let the (positive) signed measures $\lambda$ and $\mu$ be any two initial distributions of
the Markov chain at time zero, and therefore before convergence to any other distribution.
Nummelin (1984, Chapter 6) shows that if $\mathfrak{M}(\T_t,t \ge 0)$ is aperiodic, then:\index{authorindex}{Nummelin, E.}
\begin{equation}\label{oreys.theorem}
    \underset{n\rightarrow\infty}{\lim}||\lambda P^n - \mu P^n|| = 0.
\end{equation}
This is essentially Orey's (1961)\index{authorindex}{Orey, S.} \emph{total variation norm theorem}
\index{subjectindex}{total variation norm} applied to an aperiodic, recurrent Markov chain (Orey's result
was more general but not any more useful for our endeavors; see also Athreya and Ney [1978, p.498]
\index{authorindex}{Athreya, K. B.}\index{authorindex}{Ney, P.} for a proof).  However, we know that any
$\psi$-irreducible and aperiodic Markov chain has one and only one stationary distribution and
$\psi$-irreducibility is implied here by recurrence.\index{subjectindex}{Markov chain!stationary distribution}
Therefore we can substitute into \eqref{oreys.theorem} the stationary distribution $\pi$ to get:
\begin{equation}\label{aperiodic.theorem}
    \underset{n\rightarrow\infty}{\lim}||\lambda P^n - \pi|| = 0,
\end{equation}
which gives ergodicity.  This shows in greater detail than before the conditions by which
convergence in distribution to the stationary distribution is justified.
\index{subjectindex}{Markov chain!ergodicity}

Convergence to stationarity is distinct from convergence of the empirical averages, which are usually the primary 
substantive interest.  \index{subjectindex}{Markov chain!empirical averages} Consider the limiting behavior of a
statistic of interest, $h(\theta)$, from an aperiodic Harris recurrent Markov chain.
\index{subjectindex}{Markov chain!partial sums} We typically obtain empirical summaries of this statistic
using the partial sums such as: \index{subjectindex}{Markov chain!properties!Harris recurrent}
\begin{equation}
        \bar{h} = \frac{1}{n}\sum_{i=1}^{n} h(\theta_i).
\end{equation}
The expected value of the target $h$ is $E_fh(\theta)$, so by the established properties of Harris recurrent Markov chains 
(Br\'{e}maud 1999, p.104),\index{authorindex}{Bremaud, Pierre@Br\'{e}maud, Pierre} it is known that 
$\bar{h} \rightarrow E_fh(\theta)$ as $n \rightarrow \infty$.  Equivalently, it is true that:
\begin{equation}\label{stationarity.setup}
        \frac{1}{n}\sum_{i=1}^{n} h(\theta_i) - E_fh(\theta)
        \underset{n \rightarrow \infty}{\longrightarrow} 0.
\end{equation}
We can also consider the true distribution of $h(\theta)$ at time $n$ (even if it is not directly observed) from a 
chain with starting point $\theta_0$. The interest here is in $E_{\theta_0}h(\theta)$, where the expectation is with 
respect to the distribution of $\theta_n$ conditional on $\theta_0$.  In the next step add and subtract this term on 
the left-hand side of \ref{stationarity.setup} to obtain:
\begin{equation}
    \biggl[ \frac{1}{n}\sum_{i=1}^{n} h(\theta_i) - E_{\theta_0}h(\theta) \biggr] -
        \biggl[ E_fh(\theta) -  E_{\theta_0}h(\theta) \biggr]
        \underset{n \rightarrow \infty}{\longrightarrow} 0.
\end{equation}
The second bracketed term is obviously the difference between the expected value of the target $h(\theta)$ in the 
true distribution at time $n$ and the expected value of $h(\theta)$ in the stationary distribution.  For any ergodic 
Markov chain, \index{subjectindex}{Markov chain!ergodicity} this quantity will eventually to converge to zero.  The 
first bracketed term is the difference between the current empirical average and its expectation at time $n$.  Except 
at the uninteresting starting point, these quantities are \emph{never} non-asymptotically equivalent, and so even in 
stationarity, the empirical average has not converged.  This is not bad news, however, since we know for certain by 
the central limit theorem that:\index{subjectindex}{central limit theorem}
\begin{equation}\label{central.limit.theorem}\index{subjectindex}{central limit theorem}
    \frac{ \frac{1}{n}\sum_{i=1}^{n} h(\theta_i) -  E_{\theta_0}h(\theta) } { \sigma/\sqrt{n} }
    \stackrel{d}{\rightarrow} \mathcal{N}(0,1).
\end{equation}
(Meyn and Tweedie 1993, p.418, Jones 2004).  Therefore as $\sqrt{n} \delta^n \rightarrow 0$, convergence to stationarity 
proceeds at a much faster rate, but does not bring along convergence of empirical averages.  Note also that 
\eqref{central.limit.theorem} explains the shape of post-convergence marginal density plots.
\index{authorindex}{Meyn, S. P.}\index{authorindex}{Tweedie, R. L.}\index{authorindex}{Jones, G.}
\index{subjectindex}{Markov chain!convergence of empirical averages}
\index{subjectindex}{Markov chain!convergence to stationarity}

\section{Rates of Convergence}\label{rates.of.convergence.section}
So far little has been said about the actual rate of convergence, merely that chains are or are not in a state of 
convergence.  Ergodicity, resulting from positive (Harris if necessary) recurrence and aperiodicity, is merely an 
asymptotic property and thus a rather indeterminate statement for Markov chains run in finite time, that is, every 
Markov chain ever run in actual practice (Rosenthal 1995c).  \index{authorindex}{Rosenthal, J. S.}

We now provide the first improvement on basic ergodicity to produce a more rapid transition to Markov chain stability.  
If the Markov chain has invariant distribution, $\pi(\theta)$, and  $\pi(A) > 0\; \forall A \in \mathcal{F}$ it is 
\emph{ergodic of degree 2} if:\index{subjectindex}{Markov chain!ergodicity!degree 2}
\begin{equation}\label{ergodic.degree.2}
	\int_A \pi(d\theta)E_\theta[T(A)^2] < \infty
\end{equation} 
(Nummelin 1984, p.118).\index{authorindex}{Nummelin, E.} In other words, the condition is that the second moment of 
the first hitting times must be finite.  What does this buy us?  It turns out that if the functions $f(\theta)$ 
(arbitrary) and $\pi(\theta)$ are regular (finite total density and finite expectations over all sub-regions of the 
support, see Billingsley [1995, p.174] for details), then the rate of convergence is proportional to $n^{-2}$:
\index{authorindex}{Billingsley, P.}
\begin{equation}
	\underset{n \rightarrow \infty}{\lim} n^2 \|f(\theta_t) - \pi(\theta)\| \longrightarrow 0.
\end{equation}
This is an interesting result but unfortunately it is difficult to assert degree 2 ergodicity with many practical 
problems.

A more useful and stronger type of ergodic convergence is \emph{geometric ergodicity}.  
\index{subjectindex}{Markov chain!ergodicity!geometric}
Make the same assumptions as those above about the Markov chain but substitute the hitting time assumption with the 
following requirement:
\begin{equation}
	\|f(\theta_t) - \pi(\theta)\| \le m(\theta)\rho^t,\; \forall \theta,\; 0 < \rho < 1, 
\end{equation}
where $m(\theta)$ is any finite, non-negative function.  Under these conditions the $t$th step transition probability
converges to the invariant distribution at a geometric rate, which can be very quick depending on the value of $\rho$.  
If instead of specifying the function $m(\theta)$ we find a constant $m$ such that:
\begin{equation}
	\|f(\theta_t) - \pi(\theta)\| \le m\rho^t,\; \forall \theta,\; 0 < \rho < 1, 
\end{equation}
then the chain is \emph{uniformly ergodic}, which means it converges even faster.  
The value of these properties is two-fold.  \emph{First}, knowing that 
a chain is geometrically or uniformly ergodic is comforting in that it is an assurance of convergence in some reasonably 
practical amount of time (depending of course on the complexity of the model and the structure of the data).  \emph{Second}, 
it allows the derivation of bounds on the number of iterations to convergence for some Markov chains.  These claims are 
usually made by analyzing minorization and drift conditions.  The \emph{minorization} condition means that for any 
sub-space $A$, the $\sigma$-finite measure $\varphi$ in $\Omega$ with $\varphi(A)>0$ contains a \emph{small set} $C$ 
with the property that for any $\theta \in C$: \index{subjectindex}{Markov chain!small set}
\index{subjectindex}{Markov chain!ergodicity!uniform} \index{subjectindex}{ergodic!uniform}
\index{subjectindex}{drift condition}\index{subjectindex}{minorization condition}
\index{subjectindex}{Markov chain!ergodicity!geometric}
\begin{equation}
	K_t(\theta',\theta) \ge \delta v(\theta')
\end{equation}
for $\theta'$ in $\Omega$, $\delta > 0$, and time $t > 0$, where $v$ is a probability measure concentrated on $C$.  See 
Meyn and Tweedie (1993, Chapter 5) for details on small sets for Markov chains.  A Metropolis-Hastings chain always 
meets these conditions if $q_t(\theta|\cdot)$ and $\pi(\theta)$ are both positive and continuous.  (Roberts and Rosenthal 
1998).  \index{authorindex}{Meyn, S. P.}\index{authorindex}{Tweedie, R. L.}
\index{authorindex}{Roberts, G. O.} \index{authorindex}{Rosenthal, J. S.} 

We can now catalog some popular variants of MCMC algorithms by their ergodic properties.  These are given with references 
for the associated proofs.  It is assumed in this list that every Markov chain is at least ergodic as well as meeting the 
small set (minorization) condition above.  As a reminder, we will indicate $\theta$ for a finding in the single-dimensional 
case, and $\T$ for a finding in the multi-dimensional case, and all statements refer to continuous state spaces unless 
otherwise stated.  This is not a complete listing of the numerous variants, by any means, but represents most of the more 
important and relevant results.  See also the discussion in Roberts and Smith (1994).\\
\index{authorindex}{Roberts, G. O.} \index{authorindex}{Smith, A. F. M.} \index{subjectindex}{minorization condition}

\noindent {\bf Gibbs Sampling}\index{subjectindex}{Gibbs sampler!convergence properties}
\index{subjectindex}{Markov chain!ergodicity!geometric}
\begin{bayeslist}
	\item	A chain operating on a finite state space, with a positive invariance distribution is {\bf geometrically
		ergodic} (Geman and Geman 1984, Besag 1974).  The positivity condition means 
		that the support of the invariant distribution must be the Cartesian product of the marginal supports as 
		a way of guaranteeing irreducibility.
        	\index{authorindex}{Geman, S.} \index{authorindex}{Geman, D.} \index{authorindex}{Besag, J.}
		\index{subjectindex}{Markov chain!ergodicity!geometric}

	\item	A chain cast as two-parameter data augmentation is {\bf geometrically ergodic} (Tanner and Wong 1987,
		Rosenthal 1993). 
            	\index{authorindex}{Tanner, M. A.} \index{authorindex}{Wong, W. H.} \index{authorindex}{Rosenthal, J. S.}
		\index{subjectindex}{Markov chain!ergodicity!geometric}

	\item	A chain meeting the Geman and Geman conditions with purely systematic scan (the order of the 
		parameter-by-parameter updating is unchanged over Gibbs iterations): $1,2,3,...,d$ is
		{\bf geometrically ergodic} (Schervish and Carlin 1992).
        	\index{authorindex}{Schervish, M. J.} \index{authorindex}{Carlin, B. P.}
		\index{subjectindex}{Markov chain!ergodicity!geometric}

	\item	A chain meeting the Geman and Geman conditions with random scan (at the $t$'th step only one of the 
		$\theta_i^{[t]}$ is randomly selected, usually uniformly $p=1/d$, and updated) is {\bf geometrically ergodic} 
		(Liu, Wong, and Kong 1994).
        	\index{authorindex}{Liu, J. S.} \index{authorindex}{Wong, W. H.} \index{authorindex}{Kong, A.} 
		\index{subjectindex}{Markov chain!ergodicity!geometric}

	\item	Suppose there exists a non-negative function $K^*()$ on $\Re^d$ with $K^*(\T')>0$ on $\Omega$,
		where for some $t>0$ we have $K^{[t]}(\T,\T') \ge K^*(\T'), \forall \T$ ($\T$ in the continuous domain 
		of the Markov chain), and $K(\T,\T')$ is positive over all $(\T,\T') \in \Re^d \times \Re^d$.  
		Then the chain is {\bf uniformly ergodic} (Roberts and Polson 1994). %lemma 2 plus theorem 1
        	\index{authorindex}{Roberts, G. O.} \index{authorindex}{Polson, N. G.}
		\index{subjectindex}{Markov chain!ergodicity!uniform}

	\item	A chain where $\pi(\T)$ is produced from improper priors may lead to improper posteriors (Hobert and Casella 
		1996, 1998, Cowles 2002), so there is no longer a justification for geometric ergodicity (Chan 1993), or even
		degree 2 ergodicity since it may be possible to come up with sub-spaces where 
		$\int_A \pi(d\T)E\T[T(A)^2] = \infty$.
                \index{authorindex}{Hobert, J. P.} \index{authorindex}{Casella, G.} \index{authorindex}{Chan, K. S.}
		\index{authorindex}{Cowles, M. K.} 
\end{bayeslist}

\noindent {\bf Metropolis-Hastings}\index{subjectindex}{Metropolis-Hastings algorithm!convergence properties}
\begin{bayeslist}
	\item	Suppose $\pi(\T) = h(\T)\exp(p(\T))$ where $p(\T)$ is a (exponential family form) polynomial of order 
		$m \ge 2$, and $p_m(\T) \rightarrow -\infty$ as $|\T| \rightarrow \infty$, where ($p_m(\T)$ is the 
		sub-polynomial consisting of only the terms in $p(\T)$ of order $m$). There is actually a subtlety 
			lurking here in the multivariate case.  Take the term with the highest total order across terms 
			as $m(\Theta)$, and determine if there is a case whereby setting all terms to zero but one does 
			not result in $-\infty$.  So the bivariate normal ($\propto \exp[-\half(x^2 -2xy +y^2)]$) passes 
			but a function like $\exp[-\half(x^2 + 2x^2y^2 + y^2)]$ fails. 
		Note that this characterizes the normal distribution and those with lighter tails.  Then for a symmetric 
		candidate distribution bounded away from zero, the chain is {\bf geometrically ergodic} (Roberts and 
		Tweedie 1996).  \index{authorindex}{Roberts, G. O.} \index{authorindex}{Tweedie, R. L.}
		\index{subjectindex}{Markov chain!ergodicity!geometric}

	\item	A chain with $\pi(\T)$ log-concave in the tails (meaning there is an $\alpha>0$ such that for $y \ge x$,
		$\log\pi(x) - \log\pi(y) \ge \alpha(y-x)$ and for $y \le x$, $\log\pi(x) - \log\pi(y) \ge \alpha(x-y)$) 
		and symmetric candidate distribution where $q(\T'|\T) = q(\T - \T') = q(\T' - \T)$ 
		is {\bf geometrically ergodic} (Mengersen and Tweedie 1996).
        	\index{authorindex}{Mengersen, K. L.} \index{authorindex}{Tweedie, R. L.}
		\index{subjectindex}{Markov chain!ergodicity!geometric}

	\item	A chain with $\sigma$-finite measure $\varphi < \infty$ on $(\Omega,\mathcal{F})$, where $q_t(\T|\cdot)$ 
		and $\pi(\T)$ are bounded away from zero is {\bf uniformly ergodic} (Tierney 1994).  Practically, this
		means truncating the posterior support such that $\pi(\T) > 0$ (strictly!), which may be challenging in 
		high dimensions.  \index{authorindex}{Tierney, L.}
		\index{subjectindex}{Markov chain!ergodicity!uniform}

	\item	An independence chain (defined later on page~\pageref{independence.chain.definition}) with the bounded weight 
            function, $w(\T)= \pi(\T)/f(\T)$ and $\pi(\T)$ bounded away from zero is {\bf uniformly ergodic} with $\rho 
            \le 1-\text{sup}(w(\T))^{-1}$ (Tierney 1994).\\ 
            \index{authorindex}{Tierney, L.}
		    \index{subjectindex}{Markov chain!ergodicity!uniform}
\end{bayeslist}

This listing also highlights another important point.  It is not necessarily worth changing the structure of the simulation 
in regular practice to produce a uniformly ergodic Markov chain from a geometrically ergodic version, but it is almost
always worth the trouble to obtain a geometrically ergodic setup (discarding degree 2 ergodicity as analytically
difficult to assert in almost all cases).  Without geometric ergodicity convergence can take dramatically longer and,
for the purposes of practical MCMC work, essentially infinite time for ergodic chains.  Fortunately, for many of the
model types encountered in the social sciences these conditions are met with little trouble using the two standard
algorithms.  

\index{authorindex}{Jones, G. L.} \index{authorindex}{Hobert, J. P.}%
\index{authorindex}{Roberts, G. O.} \index{authorindex}{Sahu, S. K.}%
\index{authorindex}{Jarner, S. F.}%
\index{authorindex}{Roberts, G. O.}%
\index{subjectindex}{Gibbs sampler!blocked}%
\index{subjectindex}{Gibbs sampler!grouped}%
\index{subjectindex}{drift condition}%
Where problems may arise is in the use of hybrid chains, such as Metropolis-within-Gibbs and 
\index{subjectindex}{Metropolis-within-Gibbs}\index{subjectindex}{Variable-at-a-Time Metropolis-Hastings}
Variable-at-a-Time Metropolis-Hastings (Roberts and Rosenthal 1998), that combine features of more basic algorithms and
are usually specified because of posterior irregularities in the first place (note, actually, that ``Metropolis-within-Gibbs'' 
is a misnomer since Gibbs sampling is a special case of Metropolis-Hastings in which a candidate is always accepted).  
\index{authorindex}{Roberts, G. O.}\index{authorindex}{Tweedie, R. L.}  These can sometimes be checked with the 
property that a Markov chain satisfying a minorization condition and a drift condition with $V(\theta) > 2b/(1-\delta)$
is geometrically ergodic (Rosenthal 1995a).\index{authorindex}{Rosenthal, J. S.}\index{subjectindex}{minorization condition} 
For instance, Jones and Hobert (2004, 2001: Appendix A) give specific minorization and drift conditions for a block Gibbs 
sampler to be geometrically ergodic (block or grouped Gibbs sampler update parameters in blocks where the joint updatings 
are presumed to be marginalizable post-MCMC, i.e., something like drawing $\theta_1^{[j]},\theta_2^{[j]} \sim 
\pi(\theta_1,\theta_2|\theta_3^{[j-1]}), \theta_3^{[j]} \sim \pi(\theta_3|\theta_1^{[j]},\theta_2^{[j]})$, see Roberts 
and Sahu [1997]).  Also, Jarner and Roberts (2002) connect the drift condition to polynomial rate convergence measured by 
standard norms.  

From a utilitarian standpoint knowing that the Markov chain is geometrically or uniformly ergodic is not enough.  
It is only part of the complete process to worry about when running the chain to obtain reliable results.
While it is obviously important to demonstrate ergodic properties, it does not actually confirm any set of applied 
results.  Rosenthal (1995b, p.741) makes this particularly clear:\\ \index{authorindex}{Rosenthal, J. S.}
%\parbox[c]{0.9\linewidth}{\blsref\small 

\begin{quote}
	It is one thing to say that the variation distance to the true posterior distribution after $k$ steps will
	be less than $A\alpha^k$ for some $\alpha<1$ and $A>0$.  It is quite another to give some idea of how 
	much less than $1$ this $\alpha$ will be, and how large $A$ is, or equivalently to give a quantitative
	estimate of how large $k$ should be to make the variation distance less than some $\epsilon$.
\end{quote}

\noindent In other words, it is necessary to assert at least geometric ergodicity but it is not going to directly help 
the practitioner make decisions about the length of the runs.  In the next two sections we provide findings that lead 
to explicit advice about how to treat convergence for the Metropolis-Hastings algorithm and the Gibbs sampler.

\section{Implementation Concerns}\index{subjectindex}{Markov chain!convergence|(}
In applied work there are two practical questions that a user of Markov chain Monte Carlo algorithms must ask: 
(1) how long should I run the chain before I can claim that it has converged to its invariant (stationary) 
distribution, and (2) how long do I need to run the chain in stationarity before it has sufficiently mixed 
throughout the target distribution?  The key factor driving both of these questions is the \emph{rate} at which
the Markov chain is mixing through the parameter space: slow mixing means that the definition of ``long'' 
gets considerably worsened.  

There is also a difference between \emph{being} in the state of convergence and \emph{measuring} the state of 
convergence.  A Markov chain for a single dimension has converged at time $t$ to its invariant distribution 
(the posterior distribution of interest for correctly set up Bayesian applications) when the transition kernel 
produces univariate draws arbitrarily close to this distribution and the process therefore generates only 
legitimate values from a distribution in proportion to the actual target density.  For a given measure of 
``closeness'' (i.e., for some specified threshold, see below), a Markov chain is either in its invariant 
distribution or it is not.  For single-dimension chains, there is no such thing as ``somewhat converged'' or 
``approaching convergence'' (this gets more complicated for chains operating in multiple dimensions).
In fact, Rosenthal (1995a) gives an example Markov chain that converges in exactly one step.  So diagnostics 
and mathematical proofs that make claims about convergence are thus analyzing only a two-state world.  
\index{authorindex}{Rosenthal, J. S.}

To put more precision on such statements, define the vector $\T_t \in S \subseteq \Re^d$ as the $t$th empirical 
draw (reached point) from the chain $\mathfrak{M}(\T_t,t \ge 0)$, operating on $d$-dimensional measure space 
$(\Omega,\mathcal{F})$, having the transition operator $f$ defined on the Banach space of bounded measurable 
functions, and having $\pi(\T)$ as its invariant distribution.\index{subjectindex}{Banach space}  A normed vector space is 
called a Banach space if it is complete under this metric.  Completeness means that for a given \emph{probability} 
measure space, $(\Omega, \mathcal{F}, P)$, if $A \subset B$, $B \in \Omega$, $p(B)=0$, then $A \in \Omega$ and 
$p(A)=0$.  This condition allows us to ignore a set of measure problems that can otherwise occur.  It also 
provides results on a general state space as well as the easier case of a finite countable state space.  
Invariance in this context means that $\pi$ is a probability measure on $(\Omega,\mathcal{F})$ such that 
$\pi(s) = \int f(\T,s)\pi(d\T),\; \forall s \in \Omega$.  The transition kernel of the Markov chain, $f()$ 
(generalizing $K$ above), is the mechanism that maps $\Omega \times \mathcal{F} \rightarrow [0,1]$ such that for 
every $A \in \mathcal{F}$, the function $f(\cdot,A)$ is measurable and for every $\T \in \Omega$  the function 
$f(\T,\cdot)$ is a valid probability function, as noted previously.

A chain that is positive recurrent or positive Harris recurrent (whichever appropriately applies) and aperiodic is 
also $\alpha$-mixing, meaning that:
\begin{equation}
	\alpha(t) = \underset{A,B}{\sup} \bigg| p(\T_t \in B,\T_0 \in A) - p(\T_t \in B)p(\T_0 \in A) \bigg| 
		    \underset{t \rightarrow \infty}{\longrightarrow} 0
\end{equation}
(Rosenblatt 1971).  \index{authorindex}{Rosenblatt, M.}
This means that for sub-spaces $A$ and $B$ that produce the largest difference, the joint
probability of starting at some point in $A$ and ending at some point in $B$ at time $t$ converges to the 
product of the individual probabilities.  This means that these events are asymptotically (in $t$) independent 
for any definable sub-spaces.  This second result from ergodicity justifies our treatment of Markov chain 
iterations as iid samples (Chan 1993).  \index{authorindex}{Chan, K. S.} \index{subjectindex}{iid}

There are actually some additional measure-theoretic nuances and extensions of these properties, such as the implied 
assumption that $\pi(\T)$ is not concentrated on a single point as in a Dirac delta function, but the definitions 
given here are sufficient for the present purposes.  \index{subjectindex}{Dirac delta function} Also, it is important 
to remember that ergodicity is just one way to assert convergence.  It turns out, for instance, that a \emph{periodic} 
Markov chain can also converge under a different and more complicated set of assumptions (Meyn and Tweedie 1993, 
Chapter 13), and we can even define ergodicity without an invariant distribution (Athreya and Ney 1978).  
\index{authorindex}{Meyn, S. P.}\index{authorindex}{Tweedie, R. L.}
\index{authorindex}{Athreya, K. B.}\index{authorindex}{Ney, P.} 

A Markov chain that has converged has the property that repeated applications of the transition kernel produce an 
identical distribution: $\pi f = \pi$.   By far the most commonly used method of claiming such convergence is the 
\emph{total variation norm}, which is restated from \eqref{oreys.theorem}:
\index{subjectindex}{Orey's Theorem}\index{subjectindex}{total variation norm}
\begin{equation} \label{total.variation.norm}
	\|f(\T_t) - \pi(\T)\| = \half \underset{\T \in A}{\text{sup}\, A} 
				        \int_{\boldsymbol{\Theta}} \left| f(\T_t) - \pi(\T) \right| d\T.
\end{equation}
This is half of the well-known $L_1$ distance, although the $L_2$ ``chi-square'' distance is useful as well, see Diaconis and 
Saloff-Coste (1996).   Another suggestion is the infinity norm $\|f\|_\infty = \underset{\theta \in \Re^d}{\sup}\|f(\theta)\|$ 
(Roberts and Polson 1994).
\index{authorindex}{Diaconis, P.} \index{authorindex}{Saloff-Coste, L.}%
\index{authorindex}{Roberts, G. O.} \index{authorindex}{Polson, N. G.}%
\index{authorindex}{Zellner, A.} \index{authorindex}{Min, C-K.}%
The vector $\T$ is a $d$-dimensional random variable lying within $A$, the sub-space that makes the difference within the
integral as great as possible.  When we integrate over $\T \in A$ it produces a supremum over the measurable sub-space
$A$ for the set of all measurable functions on $A$ (a set that includes $f(\T_t)$ and $\pi(\T)$).  So there are two
important operations occurring in the statement of \eqref{total.variation.norm}.  \emph{First},  there is selection of a sub-space that makes
the resulting quantity as large as possible.  \emph{Second}, there is integration of the distributional difference over this sub-space.
Thus one gets the most pessimistic view of the difference between $f(\T_t)$ and $\pi(\T)$ as possible.  

The $1/2$ in \eqref{total.variation.norm} constant comes from limit theory: as $t\rightarrow\infty$, the total variation norm for $A$ 
converges to twice the empirical difference for all such sub-spaces (Meyn and Tweedie 1993, p.311):
\index{authorindex}{Meyn, S. P.}\index{authorindex}{Tweedie, R. L.}
\begin{equation}
	\underset{t\rightarrow\infty}{\lim}\|f(\T_t,\cdot) - \pi(\T)\| =
	2\underset{t\rightarrow\infty}{\lim} \underset{A}{\text{sup}}\|f(\T_t,A) - \pi(A)\|.
\end{equation}
Another way to write the total variation norm first defines $\mu(A)$ as a signed measure on the state space $S$ for the sub-space $A$.  
In the notation above, $\mu(A)$ is the integrated difference of two distributional statements over all of $A$.  Now the total variation 
norm can be expressed as:
\begin{equation}\label{total.variation.norm2}
        \|\mu\| = \underset{A \in S}{\text{sup\;}}\mu(A) - \underset{A \in S}{\text{inf\,}}\mu(A),
\end{equation}
which shows the same principle as \eqref{total.variation.norm} due to the explicit statement of the integral.

Zellner and Min (1995) propose three potentially useful alternatives as well, such as: the \emph{anchored ratio 
convergence criterion}, which selects two arbitrary points in the sample space to calculate a posterior ratio 
baseline for comparison to reached chain points, the \emph{difference convergence criterion}, which compares 
different analytical equivalent conditionals from the joint distribution, and the \emph{ratio convergence 
criterion}, which uses the last idea in ratio form rather than differencing (see particularly p.922).
We will return to the Zellner-Min diagnostics in Chapter~\ref{MCMC.Utilitarian.Chapter}.
\index{subjectindex}{anchored ratio convergence criterion}
\index{subjectindex}{difference convergence criterion}
\index{subjectindex}{ratio convergence criterion}
Thus if $\|f(\T_t) - \pi(\T)\| \rightarrow 0$ as $n \rightarrow \infty$, the distribution of $\T$ converges to 
that of a random variable from $\pi(\T)$, and this convergence is actually stronger than standard convergence 
in distribution (i.e., convergence of CDFs).  When $\|f(\T_t) - \pi(\T)\|$ reaches values close to zero (say 
$\delta$ for now) we are willing to assert convergence.  The problem, of course, is that $\pi(\T)$ is a difficult 
form to work with analytically, which is why we are using MCMC in the first place.  Theoretical work that puts 
explicit bounds on convergence includes: Lawler and Sokal (1988), Frieze, Kannan, and Polson (1994), Ingrassia 
(1994), Liu (1996b), Mengersen and  Tweedie (1996), Robert (1995), Roberts and  Tweedie (1996), Rosenthal (1995a), 
as well as Sinclair and Jerrum (1988). 
\index{authorindex}{Lawler, G. F.} \index{authorindex}{Sokal, A. D.} \index{authorindex}{Liu, J. S.}
\index{authorindex}{Frieze, A.} \index{authorindex}{Kannan, R.} \index{authorindex}{Polson, N. G.}
\index{authorindex}{Mengersen, K. L.} \index{authorindex}{Tweedie, R. L.}
\index{authorindex}{Sinclair, A. J.} \index{authorindex}{Jerrum, M. R.}
\index{authorindex}{Roberts, G. O.} \index{authorindex}{Rosenthal, J. S.}

For discrete problems it turns out that the converge rate can be established in proportion to the absolute value 
of the second eigenvalue of the transition matrix (kernel) (Diaconis and  Stroock 1991, Fill 1991, Fulman and 
Wilmer 1999, Sinclair and Jerrum 1989) but this can also be quite difficult to produce for realistic problems 
(Frigessi, Hwang, Di Stefano, and Sheu 1993).  For examples where these approaches work in practice see: Amit 
(1991), Amit and Grenander (1991), Cowles and  Rosenthal (1998), Goodman and Sokal (1989), Meyn and Tweedie 
(1994), Mira and Tierney (2001b), Polson (1996), Roberts and Rosenthal (1999), and Rosenthal (1995b, 1996).  
Usually these solutions are particularistic to the form of the kernel and can also produce widely varying or 
impractical bounds.  
\index{authorindex}{Mira, A.} \index{authorindex}{Tierney, L.}
\index{authorindex}{Goodman, J.} \index{authorindex}{Sokal, A. D.}
\index{authorindex}{Diaconis, P.} \index{authorindex}{Stroock, D. W.} \index{authorindex}{Fill, J. A.}
\index{authorindex}{Fulman, J.} \index{authorindex}{Wilmer, E. L.}
\index{authorindex}{Sinclair, A. J.} \index{authorindex}{Jerrum, M. R.}
\index{authorindex}{Frigessi, A.} \index{authorindex}{Hwang, C-R.} 
\index{authorindex}{Di Stefano, P.} \index{authorindex}{Sheu, S.-J.}
\index{authorindex}{Amit, Y.} \index{authorindex}{Grenander, U.}
\index{authorindex}{Cowles, M. K.} \index{authorindex}{Rosenthal, J. S.}

\subsection{Mixing}\index{subjectindex}{Markov chain!mixing|(}
Markov chain mixing is a related but different concern than convergence.  Mixing is the rate at which a Markov 
chain traverses about the parameter space, before or after reaching the stationary distribution.  Thus
slow mixing causes two problems: it retards the advance towards the target distribution, and once there, it
makes full exploration of this distribution take longer.  Both of these considerations are critical to 
providing valid inferences since the pre-convergence distribution does not describe the desired marginals
and failing to mix through regions of the final distribution biases summary statistics.  Mixing problems generally come
from high correlations between model parameters or weakly identified model specifications, and are often more
pronounced for model precision parameters.  

Detailed guidance about assessing mixing properties for particular applications is given later in 
Chapter~\ref{MCMC.Utilitarian.Chapter}, but several general points are worth mentioning here.  When a
Metropolis-Hastings chain is mixing poorly it usually has a very low acceptance rate and therefore stays
in single locations for long periods.  Usually this is obvious, like acceptance ratios over some period of 
time less that 1\%.  This view is unavailable for a Gibbs sampler chain since it moves on every iteration.
Often, though, if one has an indication of the range of the high density area for the posterior (for instance,
a rough idea of the 90\% HPD region), then poor mixing is observed by reasonable chain periods that 
traverse a very limited subset of this interval.  Often such problems with the Gibbs sampler are caused by
high correlations between parameters (also discussed in Chapter~\ref{MCMC.Utilitarian.Chapter}).
\index{subjectindex}{Markov chain!mixing|)}

\subsection{Partial Convergence for Metropolis-Hastings}
As the number of dimensions increases, the sensitivity (and complexity) of the Metropolis-Hastings algorithm
increases dramatically since the measure space $(\Omega,\mathcal{F})$ is defined such that each abstract point 
in $\Omega$ is $d$-dimensional and the $\sigma$-field of subsets $\mathcal{F}$ is generated by a countable 
collection of sets on $\Re^d$.  An ergodic Markov chain $\mathfrak{M}(\T_t,t \ge 0)$ has the property:
$\|f(\T_t) - \pi(\T)\| \rightarrow 0\quad \text{as}\; n \rightarrow \infty,\; \forall \T \in \Theta$,
but the size of $d$ is critical in determining the rate since each step is $d$-dimensional.
The primary complexity introduced by dimensionality here has to do with the strictness by which we apply
$f(\T_t) - \pi(\T)$.  Suppose now that there is a subset of these dimensions $e<d$ that are of primary interest
and the remaining $d-e$ are essentially a result of nuisance parameters.  Is it then reasonable to require
only evidence of \emph{partial convergence}?  That is, at time $t$ for some small $\delta$:
\begin{equation}\label{partial.convergence.measure}
	\|f(\theta^*_t) - \pi(\theta^*)\| \approx \delta,\quad \forall \theta^* \in \Re^e
\end{equation}
but, \index{subjectindex}{Markov chain!partial nonconvergence measure}
\begin{equation}\label{partial.nonconvergence.measure}
	\|f(\theta^\dagger_t) - \pi(\theta^\dagger)\| \gg \delta \quad \forall \theta^\dagger \in \Re^{d-e},
\end{equation}
where decisions are made one at a time for each of these dimensions using standard empirical diagnostics of 
stationarity.  Even though our evidence is derived from these diagnostics it is important to note that they measure 
Markov chain \emph{stability} rather than actual convergence and so the sum of each of these across dimensions is 
used to just \emph{assert} convergence (convergence in total variation norm gives stationarity but the converse is 
not similarly guaranteed).

There is one \emph{very} important distinction to be made here.  The Markov chain $\mathfrak{M}(\T_t,t \ge 0)$ 
is assumed ergodic over all of $\Omega$ and is thus guaranteed to \emph{eventually} converge across all of $\Re^d$.  
What we see by observing \eqref{partial.convergence.measure} and \eqref{partial.nonconvergence.measure} 
at time point $t$ is a lack of evidence to say that there is full dimensional convergence.  Can a Markov chain 
operating in $d$ dimensions be drawing from the true invariant distribution in $e$ sub-dimensions but not in $d-e$ 
sub-dimensions?  The standard empirical diagnostics in \winbugs, \coda, and \boa\ (Brooks and
Gelman, 1998a, 1998b; Geweke 1992; Heidelberger and Welch 1981a, 1981b; Raftery and Lewis, 1992, 1996, all described in 
Chapter~\ref{MCMC.Utilitarian.Chapter}), as well as others used in practice (Brooks, Dellaportas, and Roberts [1997] 
develop one based on the total variation norm discussed above), provide a series of parameter-by-parameter tests of 
\emph{non}-convergence.  
\index{authorindex}{Brooks, S. P.}%
\index{authorindex}{Gelman, A.}%
\index{authorindex}{Geweke, J.}%
\index{authorindex}{Heidelberger, P.}%
\index{authorindex}{Welch, P. D.}%
\index{authorindex}{Raftery, A. E.}%
\index{authorindex}{Lewis, S. M.}%
Hence they indicate when a single dimension chain is sufficiently trending as to violate 
specific distributional assumptions that reflect stability, but they do assert convergence in the opposite case.  
\index{subjectindex}{Markov chain!convergence diagnostic}
\index{authorindex}{Brooks, S. P.}  \index{authorindex}{Dellaportas, P.} \index{authorindex}{Roberts, G. O.} 

These diagnostics operate on marginal distributions individually since the output of the MCMC process is a set of
\emph{marginal} empirical draws.  Unfortunately the total variation norm given above only shows us if the chain 
has converged simultaneously across every dimension, providing a strong disconnect between theoreticians who derive
convergence properties for specific chains under specific circumstances and the masses who want to 
run simple empirical diagnostics in easy-to-use software environments.  To see this disconnect more clearly,
consider the right-hand-side of \eqref{total.variation.norm} written out with more detail:
\begin{align}\label{total.variation.norm.2}
	\half \underset{\theta \in A}{\sup A}  & \Bigg|
	\int_{\theta_1}\cdots\int_{\theta_e}\int_{\theta_{e+1}}\cdots\int_{\theta_d}
	\left[ f(\theta_1,\ldots,\theta_d)_t - \pi(\theta_1,\ldots,\theta_d) \right] \nonumber\\[-2pt]
	& \qquad\qquad\qquad\qquad\qquad\qquad\qquad
	d\theta_1 \cdots d\theta_e d\theta_{e+1} \cdots d\theta_{d} \Bigg|.
\end{align}
If $f()$ and $\pi()$ were able to be expressed as independent products (i.e., $f(x,y) = f(x)f(y)$) for the $\theta_i$,
then this would be a straightforward integration process.  As stated this cannot be true for $\pi$ since we are doing
MCMC for this very reason.  But what about $f()$?  Consider the \emph{actual} transition probability for the 
Metropolis-Hastings algorithm from $\T$ to $\T'$ from \eqref{actual.MH.transaction.function}:
\begin{equation}\label{MH.transition.prob}
        A(\T,\T')=\min \left\{\frac{\pi(\T')g(\T|\T')}{f(\T)g(\T'|\T)} , 1 \right\}
                                g(\T'|\T) +(1-r(\T))\delta_\T(\T'),
\end{equation}
where $g()$ is the proposal distribution, 
\begin{equation*}
	r(\T) = \int \min \left\{\frac{f(\T')g(\T|\T')}{f(\T)g(\T'|\T)} , 1 \right\} g(\T'|\T) d\T', 
\end{equation*}
and 
\begin{equation*}
	\delta_\T(\T')=1 \;\text{if}\; \T=\T' \;\text{and zero otherwise} 
\end{equation*}
(the Dirac delta function).  \index{subjectindex}{Dirac delta function} It is clear from looking at 
\eqref{MH.transition.prob} that we cannot generally disentangle dimensions.  In particular, note the conditionals that 
exist across $\T$ and $\T'$.   What this means is that decisions to jump to a proposal point in $\mathbf{d}$-space 
$(\Omega,\mathcal{F})$ are made based on the current position in every dimension for the Metropolis-Hastings algorithm.  
So if the chain has not converged in the $i$th dimension, $i \in [e+1\range d]$, its current placement effects the single 
acceptance ratio and therefore the probability of making a complete $d$-dimensional jump.  And this is all under the 
assumption of ergodicity.

So now that we know that non-convergence in at least one dimension affects decisions to move in all dimensions, the 
natural question is how does this work?  A Metropolis-Hastings chain dimension that has not converged is producing on
average lower density contributions in the acceptance ratio.  Therefore in cases where the conditionality on the
current state is explicit (all general forms except the independence chain Metropolis-Hastings where jumping values are
selected from a convenient form as in the random walk chain,\index{subjectindex}{random walk} but ignoring the current 
position completely:\label{independence.chain.definition} 
$g(\T'|\T) = f(\T')$) it retards the mixing of the whole chain.  Because the chain is ergodic, it is alpha mixing 
($\underset{A,B}{\sup} \left| p(\T_t \in B,\T_0 \in A) - p(\T_t \in B)p(\T_0 \in A) \right|$ goes to zero in the limit) 
but inefficiently so (slowly) since non-convergence for the $d-e$ dimensions implies poorer mixing and greater distance 
between $p(\T_t \in B,\T_0 \in A)$ and $p(\T_t \in B)p(\T_0 \in A)$.

A chain that is completely in its stationary distribution mixes better (Robert and Casella 2004, Chapter~12).  
\index{authorindex}{Robert, C. P.} \index{authorindex}{Casella, G.}
So even in the case where $\theta_i$, the un-converged dimension here, is not a parent node in the model specified by 
$\pi$, there is a negative effect: a Markov chain that has not sufficiently mixed through the target distribution produces 
biased empirical summaries because collected chain values will be incomplete, having had insufficient time to fully 
explore the target.

\subsection{Partial Convergence for the Gibbs Sampler}
Robert and Richardson (1998) show that when a Markov chain, $\mathfrak{M}(\theta_t,t \ge 0)$, is derived from another 
Markov chain, $\mathfrak{M}(\phi_t,t \ge 0)$, by simulating from a distribution according to $\pi(\theta|\phi_t)$, 
the properties of the first chain inherit that of the conditional.  
Critically, this conditionality defines new sub-spaces of $(\Omega,\mathcal{F})$ with new measure properties.
\index{authorindex}{Robert, C. P.} \index{authorindex}{Richardson, S.}  

For our purposes the important point is that if $\mathfrak{M}(\phi_t,t \ge 0)$ is geometrically ergodic, then 
$\mathfrak{M}(\theta_t,t \ge 0)$ is as well, which is easy to demonstrate using the data augmentation principle.  The 
marginal distribution for the geometrically ergodic chain at time $t$ is $\pi_t(\phi)$ with invariant distribution 
$\pi(\phi)$.  We can now express the invariant distribution of $\theta$ in conditional terms: 
$\pi(\theta) = \int_\phi \pi(\theta|\phi)\pi(\phi)d\phi$, with the marginal distribution at time $t$: 
$\pi_t(\theta) = \int_\phi \pi(\theta|\phi)\pi_t(\phi)d\phi$.  The conditional $\pi(\theta|\phi)$ appears in the 
second expression without reference to time since $\theta_t$ is simulated \emph{at each step} from $\pi(\theta|\phi)$.  
These define the total variation norm for $\theta_t$:
\begin{align}\label{pushing.proof}
	\| \pi_t(\theta) - \pi(\theta) \| &= \half \underset{\theta \in A}{\sup A}
			     \left| \int_\theta \int_\phi \pi(\theta|\phi)\pi_t(\phi) d\phi d\theta
	                          - \int_\theta \int_\phi \pi(\theta|\phi)\pi  (\phi) d\phi d\theta \right| \nonumber\\[9pt]
			  &= \half \underset{\theta \in A}{\sup A} \left| \int_\theta \int_\phi \pi(\theta|\phi)
						\left[ \pi_t(\phi) - \pi(\phi) \right] d\phi d\theta \right| \nonumber\\[9pt]
			  &\le \| \pi_t(\phi) - \pi(\phi) \|,
\end{align}
where the inequality comes from $\pi(\theta|\phi) \le 1$ by the integration of a probability function over the measure
space for $\phi$ (the rate $\rho$ carries through as well).  Switching the order of integration comes from stated 
regularity conditions on probability functions.  Note that this process is related to, but distinct from, so-called 
Rao-Blackwellization where intentional conditioning is imposed to reduce the variance of computed expectations or 
marginals (Casella and Robert 1996).  The result in \eqref{pushing.proof} is that a non-convergent dimension to the
Gibbs sampler ($\theta$ here) ``pushes'' the others ($\phi$ here) away from stationarity as well, even if these pass 
an empirical diagnostic for convergence.  \index{authorindex}{Robert, C. P.} \index{authorindex}{Casella, G.}

One utility of this result is that if we can intentionally augment a target chain with a simple form that is known to be 
geometrically ergodic, then we can \emph{impose} this property even though we increase the dimension of $(\Omega,\mathcal{F})$ 
(Diaconis and Saloff-Coste 1993, Fill 1991).  Robert and Richardson (1998) point out that this is particularly useful when 
a target chain of unknown convergence characteristics is conditioned on a simple discrete Markov chain known to be 
geometrically ergodic with specific $\rho$ and $m(\theta)$ (alternately $m$).  Also, if the chain that is conditioned on is
$\alpha$-mixing, the target chain will be as well.
\index{authorindex}{Fill, J. A.} \index{authorindex}{Diaconis, P.} \index{authorindex}{Saloff-Coste, L.}

The big point comes from the structure of the Gibbs sampler (the default engine of \bugs).  Since the kernel 
is an iteration of full conditionals, $\pi(\theta_j|\T_{-j})$ for $j=1,\ldots,d$, then according to the logic just discussed, 
either the Gibbs sampler is geometrically ergodic in every dimension or it is not geometrically ergodic in any dimension.  
Importantly, since the sub-chains share the same geometric rate of convergence, $\rho$, then one should be cautious with 
empirical diagnostics since they provide evidence of \emph{non-convergence} not evidence of \emph{convergence} (see Asmussen, 
Glynn, and Thorisson [1992] for a detailed discussion on this point).  
\index{authorindex}{Asmussen, S. P.} \index{authorindex}{Glynn, P.} \index{authorindex}{Thorisson, H.}
Recall that at any given time $t$ a Markov chain is either converged to its invariant distribution or it has not: there is a 
specific time when $\|f(\T_t) - \pi(\T)\| < \delta$ for some chosen $\delta$, we just do not necessarily know the moment.  

Suppose for a two-parameter Gibbs sampler $\theta_1$ passes some empirical diagnostic and $\theta_2$ does not.  Since they 
share the same rate of convergence, for $\theta_1$ to be in its invariant distribution while $\theta_2$ is not means that 
you are testing the chain for convergence during the very small number of intervals where the differing results are due to 
probabilistic features of the chain or the test.  Given the standard number iterations expected for MCMC work in the social
sciences (generally tens or hundreds of thousands), the probability that you have stumbled up this exact interval is 
essentially zero.  Conversely, the test fails for $\theta_2$ because the Markov chain for this dimension is either not
yet in stationarity, or is in stationarity but has failed to sufficiently explore the target distribution to produce
a stable summary statistic for the chosen diagnostic.  The latter condition only exists for a relatively short period of 
time, even with poor mixing.  Moreover, the faster the convergence rate (i.e., geometric or uniform), the 
smaller the numerator in the calculation of this probability making it even less likely that the user caught the interval 
of intermediate results using the Gibbs samplers with listed properties above, where the size of this effect is notably a 
function of $m(\theta)$ (or $m$) and $\rho$.  \index{subjectindex}{Markov chain!ergodicity}
\index{subjectindex}{Markov chain!convergence diagnostic} Therefore, for the Gibbs sampler, evidence of non-convergence in 
\emph{any} dimension is evidence of non-convergence in \emph{all} dimensions.  So for users of the usual diagnostic 
packages, \coda\ and \boa, the standard for multidimensional convergence needs to be high.  For a 
relatively small number of parameters, one would expect broad consensus across diagnostics.  However, for large numbers
of model parameters, we need to be aware that the diagnostics are built on formal hypothesis tests at selected
$\alpha$ levels and therefore $1-\alpha$ tests for large numbers of parameters will fail for about $\alpha$ proportion
of dimensions, even in full convergence.

Note that this same logic applies to Metropolis-Hastings MCMC for parameters with conditions formed by hierarchies,
which are a natural and common feature of Bayesian model specifications.  This inheritance of convergence properties
does not necessarily occur, however, for every parameter as in the perfectly symmetric case of the Gibbs sampler.
It also is not reciprocal in that the conditions in a Bayesian hierarchical model flow downward from founder nodes
to dependent nodes.  Note that these conditionals result explicitly from the model rather than through 
algorithmic conditioning in the Gibbs sampler sense.  In addition, parameters can be highly correlated without these
structural relationships.  The difficulty posed by all of these characteristics is that they generally slow the
mixing of the chain, making convergence and full exploration more difficult.
\index{subjectindex}{Markov chain!convergence|)}


\section{Exercises}
\begin{exercises}
    \item 	In Section~\ref{measure.prob.section}, three conditions were given for $\mathcal{F}$ to
		    be an associated field of $\Omega$.  Show that the first condition could be replaced
		    with $\varnothing \in \mathcal{F}$ using properties of one of the other two conditions.
		    Similarly, prove that the Kolmogorov axioms can be stated with respect to the probability
		    of the null set or the probability of the complete set.

            % NEW
    \item   Given a Markov chain on $\Omega$ and two sub-states $A,B \in \Omega$, where all elements of $B$ can be reached from $A$:
            $A$ is called \emph{essential for $B$} if all elements of $A$ can also be reached from $B$, otherwise
            \emph{inessential}.  Show that if $A$ is essential for $B$, then $B$ is essential for $A$.

    \item	Suppose we have the probability space $(\Omega,\mathcal{F},P)$, sometimes called a \emph{triple}, 
		    and $A_1,A_2,\ldots,A_k \in \mathcal{F}$.  Prove the finite sub-additive property that:
		    \begin{equation*}
			    P\left( \bigcup\limits_{i=1}^{k}A_i \right) \le \sum_{i=1}^{k}p(A_i),
		    \end{equation*}
		    (Boole's Inequality).\index{subjectindex}{Boole's Inequality}

            % NEW
    \item   Using the transition matrix from Example~\ref{Example.Cards} in Chapter~\ref{MCMC.Chapter} 
            (page~\pageref{Example.Cards}),
            \begin{equation*}
                K  = \left[ \begin{array}{cccccccccccc}
                \third &\;\qquad& 0 &\;\qquad& \third &\;\qquad& \third &\;\qquad& 0 &\;\qquad& 0 \\ 
                    0 &\;& \third &\;&      0 &\;&      0 &\;& \third &\;& \third \\ 
                \third &\;& \third &\;& \third &\;&      0 &\;&      0 &\;&      0 \\ 
                    0 &\;&      0 &\;&      0 &\;& \third &\;& \third &\;& \third \\ 
                \third &\;& \third &\;&      0 &\;&      0 &\;& \third &\;&      0 \\ 
                    0 &\;&      0 &\;& \third &\;& \third &\;&      0 &\;& \third \\
                \end{array} \right], 
            \end{equation*}
            construct a directed graph showing the immediate paths between these six states.  Does this reveal anything about
            moving from states that are not immediately connected.

    \item	Prove that uniform ergodicity gives a faster rate of convergence than geometric ergodicity and
		    that geometric ergodicity gives a faster rate of convergence than ergodicity of degree 2.

            % NEW
    \item   Write a Metropolis-Hastings algorithm in \R\ that is purposely transient using the data from 
            Example~\ref{hit.run.example} in Chapter~\ref{MCMC.Chapter} (page~\pageref{hit.run.example}).  Plot the first 500 
            iterations.

    \item	Construct a stochastic process without the Markovian property and construct deliberately 
		    non-homogeneous Markov chain.

            % NEW
    \item   A Markov chain operates on a $k$-cycle if it moves clockwise and counterclockwise on a loop of $1,2,\ldots,k$ 
            ordered values.  If $\theta$ moves clockwise with probability $p$ and counterclockwise with probability $1-p$,
            then give the stationary distribution for $p=\half$.  Is this different for $p\ne\half$?

    \item	Give an example of a maximally $\psi$-irreducible measure and one that it dominates. 

            % NEW
    \item   One urn contains $k$ black marbles and another urn contains $k$ white marbles.  At each iteration
            of a process one ball is uniformly randomly selected from each urn and exchanged between the urns.  Define 
            $\theta^{[t]}$ as the number of white balls in urn number 1 after the $t$th step.  Obtain the following:
            \begin{enumerate}[a.]
                \item   Prove that this is a Markov chain.
                \item   Derive the transition probabilities for $\theta^{[t]}$.
                \item   Produce the stationary distribution for $\theta^{[t]}$.
            \end{enumerate}

    \item	A single-dimension Markov chain, $\mathfrak{M}(\theta_t,t \ge 0)$, with transition matrix $P$ 
		    and unique stationary distribution $\pi(\theta)$ is \emph{time-reversible} iff:
		    \index{subjectindex}{Markov chain!time-reversible}
		    \begin{equation*}
			    \pi(\theta_i) p(\theta_i,\theta_j) = \pi(\theta_j) p(\theta_j,\theta_i).
		    \end{equation*}
		    for all $\theta_i$ and $\theta_j$ in $\Omega$.  Prove that the reversed chain, defined by
		    $\mathfrak{M}(\theta_s,s \le 0) = \mathfrak{M}(\theta_t,-t \ge 0)$, has the identical
		    transition matrix. %Grimmmett and Stirzaker p.219

            % NEW
    \item   \label{AR1.chain.exercise2} Consider a Markov chain based on an AR(1) specification: 
            \begin{equation*}
                \theta^{[j+1]} = \varepsilon \theta^{[j]} + \epsilon^{[j]},
            \end{equation*}
            where $\epsilon^{[j]} \sim \mathcal{N}(0,1)$, for all $j=1,\ldots,J$ iterations, and $\varepsilon \in (0:1)$.
            See also Exercise~\ref{example.time.series} (page~\pageref{example.time.series}).
            Show that the stationary distribution of this Markov chain is $\mathcal{N}(0,1/(1-\varepsilon^2))$.

	\item	Meyn and Tweedie (1993, p.73) \index{authorindex}{Meyn, S. P.} \index{authorindex}{Tweedie, R. L.} define an n-step 
		    \emph{taboo probability}\index{subjectindex}{taboo probability} as:
		    \begin{equation*}
			    {\color{white} \sum \normalcolor}_{\mathcal{A}}
			    P^n(x,B) = P_x(\Phi_n \in B, \tau_A \ge n),\qquad x \in \mathsf{X}, A,B \in B(\mathsf{X}),
		    \end{equation*}
		    meaning the probability that the Markov chain, $\Phi$, transitions to set $\mathcal{B}$ in $n$ steps avoiding
		    (not hitting) set $\mathcal{A}$.  Here $\mathsf{X}$ is a general state space with countably generated $\sigma$-field
		    $B(\mathsf{X})$, and $\tau_A$ is the return time to $\mathcal{A}$ (their notation differs slightly from that in
		    this chapter).  Show that:
		    \begin{equation*}
			    {\color{white} \sum \normalcolor}_{\mathcal{A}} P^n(x,B) = \int_{\mathcal{A}^c} P(x,dy)_{\mathcal{A}} P^{n-1}(y,B),
		    \end{equation*}
		    where $\mathcal{A}^c$ denotes the complementary set to $\mathcal{A}$.
		
            % NEW
    \item   For Exercise~\ref{AR1.chain.exercise2}, implement the AR(1) Markov chain in \R\ and graph the trajectory of the chain
            for three different values of $\varepsilon$ in the same plot.

	\item	The Langevin algorithm\index{subjectindex}{Langevin algorithm} is a Metropolis-Hastings variant where on each
		    step a small increment is added to the proposal point in the direction of higher density.  Show that making 
		    this increment an increment of the log gradient in the positive direction produces an ergodic Markov chain by
		    preserving the detailed balance equation.

            % NEW
    \item   A transition matrix $K$ is defined on the binary set $\{0,1\}$ by:
            \begin{equation*}
                K = \left[ \begin{array}{cc} 1-a & a \\ b & 1-b \\ \end{array} \right]
            \end{equation*}
            with $0 < a,b \le 1$ and at least one of these less than one.  Show that the stationary distribution of the Markov
            chain defined by this transition kernel is given by $p(0) = b/(a+b), p(1) = a/(a+b)$.

	\item	Consider a stochastic process, $\theta^{[t]}$ ($t \ge 0$), on the probability space $(\Omega,\mathcal{F},P)$.  If:
		    $\mathcal{F}_t \subset \mathcal{F}_{t+1}$, $\theta^{[t]}$ is measurable on $\mathcal{F}$ with finite first 
		    moment, and with probability one $E[\theta^{[t+1]}|\mathcal{F}_t] = \theta^{[t]}$, then this process is called
		    a \emph{Martingale}\index{subjectindex}{Martingale} (Billingsley 1995, p.458).  Prove that Martingales do or do not
		    have the Markovian property.

            % NEW
    \item   Replacing $E[\theta^{[t+1]}|\mathcal{F}_t] = \theta^{[t]}$ in the last exercise with
            $E[\theta^{[t+1]}|\mathcal{F}_t] \ge \theta^{[t]}$ produces a \emph{supermartingale}, and replacing with
            $E[\theta^{[t+1]}|\mathcal{F}_t] \le \theta^{[t]}$ produces a \emph{submartingale}.  If $t \ge 0$, show that
            $\theta^{[t]}$ is a supermartingale if and only if $-\theta^{[t]}$ is a submartingale, and vice versa.
            \index{subjectindex}{Martingale} \index{subjectindex}{supermartingale}, and \index{subjectindex}{submartingale}

	\item	Prove that for a Markov chain that is positive Harris recurrent, $\exists\,\sigma-$finite probability measure, 
		    $\mathfrak{P}$, on $S$ such that for an irreducible Markov chain, $X_t$, at time $t,\; p(X_n \in A)=1,\; 
		    \forall A \in S$ where $\mathfrak{P}(A)>0$, and aperiodic is also $\alpha$-mixing,
		    \begin{equation*}
        		\alpha(t) = \underset{A,B}{\sup} \bigg| p(\T_t \in B,\T_0 \in A) - p(\T_t \in B)p(\T_0 \in A) \bigg|
                    		\underset{t \rightarrow \infty}{\longrightarrow} 0.
		    \end{equation*}

            % NEW
    \item   A random walk Markov chain on $\mathcal{I}$ has the following transition kernel with fixed $0 < p < 1$:
            \begin{equation*}
                K(i,j) = \begin{cases}
                            \begin{array}{ll}
                                p   & \text{if} \; j-1=i \\
                                1-p & \text{if} \; j+1=i \\
                                0   & \text{otherwise.} \\
                            \end{array}
                         \end{cases}
            \end{equation*}    
            Show that this Markov chain is irreducible with period $2$. Suppose now that $p$ is randomly drawn
            on each iteration of the Markov chain from the distribution $\sim \mathcal{U}(0,1)$ (exclusive of the endpoints).
            Is this still an irreducible Markov chain?
\end{exercises}

\thispagestyle{empty}
\chapter{Bayesian Hypothesis Testing and the Bayes Factor} \label{Testing.Chapter}
\setcounter{examplecounter}{1}

\section{Motivation}
This chapter systematically explores Bayesian hypothesis testing, and describes the Bayes Factor as the evidence of the quality of one 
model specification over another.  We start with a general discussion of the state of hypothesis testing in the social and behavioral 
sciences and outline where Bayesian posterior descriptions can replace a number of problematic practices.  The emphasis will be mainly 
on using Bayesian tools for determining the strength of evidence, rather than the details of the many differences between Bayesian and 
non-Bayesian approaches.  For a very clear, and succinct, discussion of the historical developments in this regard, see Marden (2000).
\index{authorindex}{Marden, J. I.}

The material in this chapter assumes knowledge of generalized linear models, although basic likelihood theory is briefly reviewed.
Appendix~\ref{GLM.Chapter} contains a review of McCullagh\index{authorindex}{McCullagh, P.} and Nelder (1989)
\index{authorindex}{Nelder, J. A.} style GLM theory.  In particular we will see their treatment of the exponential family form
when discussing the structure of prior distributions.  Excellent and accessible works on likelihood theory abound.  Among the most
commonly assigned are Berger\index{authorindex}{Berger, J. O.} and Wolpert\index{authorindex}{Wolpert, R. L.} (1988), Cramer
\index{authorindex}{Cramer, J. S.} (1994), Edwards \index{authorindex}{Edwards, A. W. F.} (1992), Eliason
\index{authorindex}{Eliason, S. R.} (1983), and King\index{authorindex}{King, G.} (1989).  There are also an increasing number of
texts on generalized linear models.  Dobson\index{authorindex}{Dobson, A. J.} (1990) is particularly accessible.  The classic and
enduring work is McCullagh\index{authorindex}{McCullagh, P.} and Nelder \index{authorindex}{Nelder, J. A.} (1989), and more recent
additions include Lindsey\index{authorindex}{Lindsey, J. K.} (1997) and Fahrmeir\index{authorindex}{Fahrmeir, L.} and
Tutz\index{authorindex}{Tutz, G.} (2001).

More topically, this is a chapter about \emph{model testing}: determining why we should prefer one model specification over another.  
Obviously practically inclined social science researchers have many potential specifications to choose from, and it turns out that the
Bayesian paradigm has powerful and sophisticated tools for comparing alternatives.  The core advantage over non-Bayesian approaches is
that \emph{everything} unknown is given a probability assessment, including model choices.  This facilitates clear comparisons and 
stands in direct contrast to the methods in the last chapter, which focused on \emph{model adequacy} for a single specification.

Bayesian hypothesis testing is often less formal than the non-Bayesian varieties.  By far, the most common procedure for summarizing 
results in social science research is to simply describe the posterior distribution rather than to apply a rigid decision process.  
\index{subjectindex}{decision theory}
However, Bayesian decision theory (Chapter~\ref{Decision.Chapter}) is a well-developed area, particularly in those fields where costs, 
risks, and consequences in general are measurable (Cyert\index{authorindex}{Cyert, R. M.} and DeGroot\index{authorindex}{DeGroot, M. H.} 
1987).  More basically, the Bayes Factor described in this chapter allows a very general and directly implementable means of model 
comparison that is already popular in the Bayesian social sciences (Meeus, \etal 2010, Pang 2010, Quinn, Martin, and Whitford 1999, 
Raftery 1995, Sened and Schofield 2005, Zhang and Luck 2011).
\index{authorindex}{Meeus, W.} \index{authorindex}{Pang, X.}
\index{authorindex}{Quinn, K. M.} \index{authorindex}{Martin, A.}
\index{authorindex}{Whitford, A. B.} \index{authorindex}{Raftery, A. E.}
\index{authorindex}{Sened, I.} \index{authorindex}{Schofield, N.}
\index{authorindex}{Zhang, W.} \index{authorindex}{Luck, S. J.}

The null hypothesis significance testing\index{subjectindex}{hypothesis testing!NHST} paradigm that currently 
dominates tests of statistical reliability in the social sciences is seriously defective and widely misunderstood.  
A fixation with arbitrary thresholds and unjustified frequentist assumptions has damaged empirical research in many fields 
for quite some time (Bakan 1960, Cohen 1994, Hunter 1997, Meehl 1978, Pollard 1993, Rozeboom 1960, Serlin and Lapsley 1993, Schmidt 1996).  
	\index{authorindex}{Bakan, D.}%
	\index{authorindex}{Cohen, J.}%
	\index{authorindex}{Hunter, J. E.}%
	\index{authorindex}{Meehl, P. E.}%
	\index{authorindex}{Rozeboom, W. W.}%
	\index{authorindex}{Serlin, R. C.}%
	\index{authorindex}{Lapsley, D. K.}%
	\index{authorindex}{Schmidt, F. L.}%
The use of p-values and ``stars'' (asterisks on tables) as evidence for or against 
the null hypothesis is flawed in many social science studies since the long-run 
probabilistic assumption of Neyman-Pearson, upon which these measures are built, 
generally does not apply to single-point cross-sectional studies with uniquely 
occurring data.  Social scientists employ standard Neyman-Pearson frequentist 
standards and practices without meeting the key underlying assumption of long-run 
replicability, and therefore average over unobserved and unlikely events.  

There is a lengthy but frustrated literature attempting to reconcile frequentist 
and Bayesian approaches with the idea that the resulting inferences should be the 
same if one could just carefully restrict the \emph{a priori} model specifications 
(Bartholomew 1965; Casella and Berger 1987a; Berger, Brown, and Wolpert 1994; 
Berger, Boukai, and Wang 1997; DeGroot 1973; Good 1983a, 1992; Good and Crook 1974; 
Jeffreys 1961; Pratt 1965).  
	\index{authorindex}{Bartholomew, D. J.}%
	\index{authorindex}{Casella, G.}% 
	\index{authorindex}{Berger, R. L.}%
	\index{authorindex}{Berger, J. O.}%
	\index{authorindex}{Brown, L. D.}%
	\index{authorindex}{Wolpert, R. L.}%
	\index{authorindex}{Boukai, B.}%
	\index{authorindex}{Wang, Y.}%
	\index{authorindex}{DeGroot, M. H.}%
	\index{authorindex}{Good, I. J.}%
	\index{authorindex}{Crook, J. F.}%
	\index{authorindex}{Jeffreys, H.}%
	\index{authorindex}{Pratt, J. W.}%
Detailed and sophisticated works have still produced only limited success in a series 
of special cases that are not of general interest.

Also, there is much less controversy about Bayesian inference and Bayesian hypothesis testing than there was a few decades ago.
This is because of an increasing recognition that the Bayesian approach, which treats all unknown quantities with distributional
statements, is closer to common scientific intuition. Social and behavioral scientists typically do not have the unending stream
of iid data that canonical Neyman-Pearson frequentist inference requires. Consider that the standard confidence interval means
that on average 19 times out 20, for $\alpha=0.05$, the interval covers the true parameter value. Yet we generally have only one
sample of observational data, often collected by others.
Also recall that maximum likelihood analysis is equivalent to 
a Bayesian setup with the appropriately bounded uniform distribution prior.  It 
is also true that the two approaches lead to identical inferences asymptotically 
for \emph{any} proper prior distribution specification (i.e., the data will 
eventually overwhelm the prior knowledge).  \index{subjectindex}{frequentism!similarity to Bayes}
Rather than treat the previous facts as a comforting reason to continue advocating 
traditional likelihoodist practices, along with the accompanying null hypothesis 
significance test, we observe that Bayesians have shown that uniform prior 
specifications can lead to a number of problematic posterior results.  The fact 
\index{subjectindex}{prior distribution!uniform} that uniform priors are not invariant under nonlinear 
transformation was critical to Fisher's wholesale (and vitriolic) rejection of 
Bayesian inference (Fisher\index{authorindex}{Fisher, R. A.} 1930; 1956, Chapter 2).
\index{subjectindex}{Bayesian inference} In addition, there are many situations where uniform priors are unreasonable starting
points like elections with two major party candidates and a third fringe candidate. Why would a researcher pretend that these
three have equal probability of winning the election before collecting data?

\section{Bayesian Inference and Hypothesis Testing}\label{section:hypo}\index{subjectindex}{hypothesis testing}
Bayesian modes of inference can be divided into two basic approaches: one in which basic descriptions of the posterior are provided as 
evidence of some effect (as we have been doing up until this point), and one in which explicit testing mechanisms are performed.  The 
second approach is due mainly to Jeffreys (1961).  Often posterior description and articulated testing are not provided as exclusive 
demonstrations of evidence of some effect, but are given in conjunction.  When there are multiple competing model specifications 
arising either from theoretical propositions or from alternative specifications of the same theory,  a set of posterior distributions is 
produced, requiring some method for comparison.  The most straightforward is the ratio of the posterior probability of some 
specification relative to another.  This posterior odds ratio\index{subjectindex}{posterior odds ratio} gives the odds of one model 
relative to another and is called the Bayes Factor\index{subjectindex}{Bayes Factor} (discussed in detail in this chapter).  

\subsection{Problems with Conventional Hypothesis Testing}\label{Hypo.Intro.Section}
\index{subjectindex}{hypothesis testing!NHST} \index{subjectindex}{iid}
The standard process for hypothesis testing in the social sciences is an odd mix that can be called \emph{quasi-freqentist}.
Suppose we observe $X_1, X_2, \ldots, X_n$ iid $f(x|\theta)$, where $\theta$ is some unknown value on the parameter space $\Theta$.  
A one-sided (non-nested) test is defined by:\index{subjectindex}{hypothesis testing!one-sided}
\begin{equation}
        \Hz \theta \le 0 \quad vs. \quad \Ho \theta > 0,
        \label{one.sided.test}
\end{equation}
and a two-sided (nested) test is similarly defined by: \index{subjectindex}{hypothesis testing!two-sided}
\begin{equation}
        \Hz \theta = 0 \quad vs. \quad \Ho \theta \ne 0.
        \label{two.sided.test}
\end{equation}
In the standard setup used in empirical social and behavioral science analysis, a test statistic ($T$), some function of $\theta$ and the 
data, is calculated and compared with its known distribution under the assumption that $H_0$ is true.  Commonly used test statistics are 
sample means ($\bar{X}$), chi-square statistics ($\chi^2$), and t-statistics in linear (OLS) regression analysis.  The test procedure 
assigns one of two decisions ($D_0$, $D_1$) to all possible values in the sample space of $T$, which correspond to supporting either $H_0$ 
or $H_1$, respectively.  The p-value (``associated probability'')\index{subjectindex}{p-value} is equal to the area in the tail (or tails;
we will illustrate with a one-tailed discussion for now) of the assumed distribution under $H_0$ ($\theta=0$ fixed), which starts at the 
point designated by the placement of $T$ on the horizontal axis and continues to infinity:	
\index{subjectindex}{distribution!chi-square@$\chi^2$}
\begin{equation}
        p(x) = p(T(x) \ge T|\theta=0) = \int_T^{\infty} f(t|\theta=0)dt.
\end{equation}
The sample space of $T$ is segmented into two complementary regions ($S_0$, $S_1$), whereby the probability that $T$ falls in $S_1$, causing 
decision $D_1$, is either a predetermined null hypothesis\index{subjectindex}{null hypothesis} cumulative distribution function (CDF) level: 
the probability of getting this or some lower value given a specified parametric form such as normal, F, t, etc.  ($\alpha =$ size of the 
test, Neyman and Pearson 1928a, 1928b, 1933a, 1933b),\index{authorindex}{Neyman, J.} \index{authorindex}{Pearson, E. S.} or the cumulative 
distribution function level corresponding to the value of the observed test statistic under $H_0$ is reported (i.e., the p-value = 
$\int\limits_{S_1}P_{H_0}(T=t)dt$, Fisher [1925a]).

There are many criticisms of the use of p-values in empirical work.  \index{subjectindex}{hypothesis testing!p-values} Hwang \etal
(1992) point out that since the p-value is the density under the null hypothesis starting at the test statistic and continuing to
infinity on the support for $\theta$, that is an average over unlikely sample values that have not actually occurred.  In
addition, Berger \index{authorindex}{Berger, J. O.} and Wolpert\index{authorindex}{Wolpert, R. L.} (1984) note that this
definition violates the likelihood principle (Birnbaum\index{authorindex}{Birnbaum, A.} 1962) that inferences must come from
observed, not hypothetical, data.  Casella \index{authorindex}{Casella, G.} and Berger\index{authorindex}{Berger, R. L.} (1987a)
observe that there is nothing ``frequentist'' about a p-value since it is not the probability of a Type I error.
Koop\index{authorindex}{Koop, G.} (1992) shows that classic frequentist analysis with p-value evidence fails to provide evidence
of unit root problems in time-series analysis.\index{subjectindex}{time-series}  While Bayesian time-series models are not within
the main scope of this text (see Pole, West, and Harrison [1994] as a good starting point), it should be mentioned that asymptotic
analysis for the unit root problem is a serious problem in the non-Bayesian setting and is well-behaved for Bayesian models, a
qualitative difference that is even greater when the estimated parameters occupy restricted space.  \index{authorindex}{Pole, A.}
\index{authorindex}{West, M.} \index{authorindex}{Harrison, J.}      

A core problem (there are many) with the null hypothesis significance test as practiced is that researchers pretend to select $\alpha$ levels 
\emph{a priori} as in experiments based on Neyman-Pearson, but actually report p-values (or worse yet, ranges of p-values indicated by 
asterisks) as the strength of evidence: quasi-frequentism.\index{subjectindex}{hypothesis testing!NHST!problems}  This is because the 
social sciences are encumbered with Fisher's arbitrary thresholds (even he later recanted), despite the fact that there has \emph{never} been 
a theoretical justification to support 0.01, 0.05, and 0.10 levels.  Aitkin\index{authorindex}{Aitkin, M.} (1991) observes that atheoretic 
use of fixed test sizes leads to an ``unreasonable test in completely specified models,'' and Barnard\index{authorindex}{Barnard, G. A.} 
(1991, discussion of Aitkin) points out that these conventions originated from the lack of ready computing (i.e., the propagation of 
Fisher's\index{authorindex}{Fisher, R. A.} tables).  Because the test is typically performed once on a set of social data in time and will 
not reoccur in the same fashion, the reported p-value is not a long run frequentist probability.  Furthermore, since only one model 
specification is tested (at least as far as the reader ever gets to know!), an infinite number of alternate specifications are not ruled out.

A second problem with the null hypothesis significance test that pertains directly to research in applied settings such as public policy 
analysis is that there is no explicitly modeled consequence of making the wrong decision (Pollard\index{authorindex}{Pollard, W. E.} and 
Richardson\index{authorindex}{Richardson, J. T. E.} 1987).  Unlike purely academic research, decisions taking place in policy analysis and 
implementation have direct consequences for citizens, employees, managers, and agencies in general.  Yet hypothesis testing confuses 
inference and decision making since it ``does not allow for the costs of possible wrong actions to be taken into account in any precise way'' 
(Barnett\index{authorindex}{Barnett, V.} 1973).  Decision theory\index{subjectindex}{decision theory} (Raiffa and Schlaifer 1961) is the 
logical adjunct to hypothesis testing that formalizes the cost of alternatives by explicitly defining the cost of making the wrong decision 
by specifying a loss function\index{subjectindex}{loss function} and associated risk\index{subjectindex}{risk!function} for each alternative 
(Berger\index{authorindex}{Berger, J. O.} 1985, Pollard\index{authorindex}{Pollard, P.} 1986).  These principles are discussed on
Chapter~\ref{Decision.Chapter}.  Despite the utility of this extension to settings where decision making is required, it is 
rare to see applications in policy studies.  What makes this surprising is that loss functions have a natural role in applied settings since 
obvious asymmetries occur in political and social decision-making: peace versus war, election victory versus loss, social group acceptance 
or rejection, and so on.

\subsubsection{One-Sided Testing}\index{subjectindex}{hypothesis testing!one-sided!Bayesian}
One-sided Bayesian hypothesis testing for a specified parameter is fairly basic and a Bayesian version of the standard p-value can
be produced once the posterior distribution is obtained.  For the simple one-sided case in \eqref{one.sided.test}, the specified
prior distribution of $\theta$ provides an \emph{a priori} probability over the two regions of the sample space of $\theta$: $\Hz
p(-\infty < \theta \le 0) = \pi_0$, $\Ho p(0 < \theta < \infty) = \pi_1 = 1 - \pi_0.$  While this can take on an obviously large
number of forms, the uninformative uniform distribution is particularly useful, and many authors have suggested that lacking
specific information $\pi_0 = p(H_0 \;\text{is true}) = \frac{1}{2}$ is a useful value (Berger\index{authorindex}{Berger, J. O.}
and Sellke\index{authorindex}{Sellke, T.} 1987, Jeffreys\index{authorindex}{Jeffreys, H.} 1961).

Once prior probabilities are assigned, the Bayesian posterior probability is 
derived from the non-normalized region defined by the null hypothesis divided by 
the total non-normalized region, which can be derived as follows: 
\begin{align}
          p(H_0|\x) &= \int_{-\infty}^{\infty} p(H_0,\theta|\x)d\theta                                                         \nonumber\9
                    &= \int_{-\infty}^{\infty} \frac{ p(\x|H_0,\theta)p(H_0,\theta) }{ p(\x) }d\theta                           \nonumber\9
                    &= \int_{-\infty}^{\infty} p(\x|H_0,\theta)p(H_0,\theta) d\theta/p(\x)	                              \nonumber\9
                    &= \frac{ \int_{-\infty}^{\infty} p(\x|H_0,\theta)p(H_0,\theta)d\theta }
                            { \int_{-\infty}^{\infty} [p(\x|H_0,\theta)p(H_0,\theta) + p(\x|H_1,\theta)p(H_1,\theta)] d\theta } \nonumber\9
                    &= \frac{ \int_{-\infty}^{0} p(\x|\theta)\pi_0 d\theta }
                            { \int_{-\infty}^{0} p(\x|\theta)\pi_0 d\theta + \int_{0}^{\infty} p(\x|\theta)\pi_1 d\theta }      \nonumber\9
                    &= \frac{ \int_{-\infty}^0 p(\mathbf{\x}|\theta)\pi_0 d\theta }
                            { \int_{-\infty}^{\infty} p(\mathbf{\x}|\theta)\pi_0 d\theta },
				\label{one.sided.testing.probability}
\end{align}
where the part of the integral in the numerator from $0$ to $\infty$ contributes zero to this calculation since $H_0$ is on the
right-hand-side of the conditionals (and the same logic holds for the $H_1$ part).  The terms inside the integrals are modified
using the definition of conditional probability: $p(\x|H_0,\theta)p(H_0,\theta) = p(\x|H_0,\theta)p(\theta|H_0)p(H_0) =
p(\x|H_0,\theta)p(\theta|H_0)\pi_0$.  In the last step, the simplification occurs only if $\pi_0 = \pi_1$ (otherwise the
denominator is a weighting of the two integrals seen in the penultimate state).

More generally, this is just the slice of the density that corresponds to the one-sided restriction defining the null hypothesis calculated 
over the posterior distribution.  This posterior probability, while slightly more difficult to construct than the standard p-value, is far 
more useful because it is the value that many people mistake a p-value for: the probability that the null hypothesis is true, given the data 
and the model.  Conversely, the standard p-value is the far less revealing probability of seeing these or more extreme data, given the 
model and \emph{an assumed true null hypothesis}.

Casella\index{authorindex}{Casella, G.} and 
Berger\index{authorindex}{Berger, R. L.} (1987a) showed that when $\X$ is generated by a symmetric location density with monotone 
likelihood 
    ratio\footnote{Suppose we have a family of probability density functions $h(t|\theta)$ in which the random variable $t$ is 
    conditional on some unknown $\theta$ value to be tested.  This family has a monotone likelihood ratio 
	if for every $\theta_1 > \theta_2$, the corresponding $\frac{h(t|\theta_1)}{h(t|\theta_2)}$ 
	is a nondecreasing function of the random variable $t$.\index{subjectindex}{monotone likelihood ratio}}
(a condition greatly aided by the central limit theorem\index{subjectindex}{central limit theorem} as the data set size increases), 
and if the prior distribution of $\theta$ is symmetric about zero, then $\text{inf}\;\pi(H_0|\X) \le p(x)$, where the infimum 
(parameter or structural minimization, see Gill [2006]) is taken over the class of suitable priors.  Yet there is no theoretical 
justification for picking the prior distribution that leads to the infimum over any other justifiable prior.  Also, in the less 
common cases (at least with relatively large-$n$ social science research) where the sampling density of $\X$ does not have a 
monotone likelihood ratio, then $p(H_0|\X) < p(x)$.

In fact, Casella\index{authorindex}{Casella, G.} and 
Berger's\index{authorindex}{Berger, R. L.} proof 
shows that frequentist p-values are radically biased against the null.  
\index{subjectindex}{bias!frequentist p-values}
For example (Berger\index{authorindex}{Berger, J. O.} and 
Sellke\index{authorindex}{Sellke, T.} 1987, p.113, 
Casella\index{authorindex}{Casella, G.} and 
Berger\index{authorindex}{Berger, R. L.} 1987a, p.110), if a 
random variable $X$ distributed $\mathcal{N}(\theta,1)$ is observed to be 
$1.645$, then the one-sided p-value is 0.05.  However, for all prior 
distributions assigning mass of $\frac{1}{2}$ at zero and $\frac{1}{2}$ 
elsewhere, $\text{inf}\:\pi(H_0|x=1.645) = 0.21$.  This example demonstrates that
concentrating non-zero mass on point null position (zero here) leads to 
unreasonable (and downwardly biased) posterior inferences, and are thus not 
``impartial'' expressions of prior ignorance.  And it gets worse for the p-value.  
Casella\index{authorindex}{Casella, G.} and 
Berger\index{authorindex}{Berger, R. L.} also show that 
equality of the p-value and the Bayesian posterior quantile is achieved under 
the same circumstances but with the extreme constant pseudo-density improper 
prior that gives constant density for all values in $\Re$; yet a
persistent frequentist criticism of Bayesian inference is the use of these 
improper priors in estimation as unreasonable probability constructs.
\index{subjectindex}{hypothesis testing!Bayesian!improper priors}
\index{subjectindex}{Bayesian inference}
\index{subjectindex}{prior distribution!improper}

\begin{examplelist}
    \item   \label{example:French.strikes} {\bf Example: One-Sided Testing with French Labor Strike Data} 
            \index{subjectindex}{example!French labor strikes}
            One characteristic of labor strikes in France is imitative behavior by unions: news of other strikes 
            can stimulate additional strikes by signaling that the conditions are amenable.  
            Conell\index{authorindex}{Conell, C.} and Cohn\index{authorindex}{Cohn, S.} (1995) look at French 
            Third Republic coal mining strikes with particular attention to follow-on strike behavior by unions.  
            Their data are given in Table~\ref{French.Strikes}

            \begin{table}[h]
            \begin{small}
            \begin{center}
            \tabletitle{\textsc{French Coal Strikes, by Year}}\label{French.Strikes}
            \begin{tabular}{rrrrrrrrrrr}
	            1902 & 1906 & 1912 & 1914 & 1919 & 1921 & 1923A & 1923B & 1926 & 1930 & 1933\\
   	            9 &    8 &   13 &   23 &   15 &   23 &    13 &     6 &   13 &   15 &   10
            \end{tabular}
            \end{center}
            \end{small}
            \end{table}
            \vspace{11pt}

            There are two periods assigned to 1923 because there were two distinct ``salary offensives'' during 
            this year.  Since these are counts, it is natural to consider a Poisson model for the data.  However, 
            the Poisson model assumes that the mean and variance are equal and this is not the case here.  
            Consequently we specify a negative binomial model with a Jeffreys prior.  

            Here we use an equivalent variant of the negative binomial PMF as that given in Appendix~\ref{distribution.appendix}:
            \index{subjectindex}{Jeffreys prior!negative binomial}
            \begin{equation}
	            \mathcal{NB}(y|r,p) = \binom{r+y-1}{y}p^r(1-p)^{y},
            \end{equation}
            where the interpretation is that $y$ represents the number of failures before reaching the $r$th success.  
            Recall that the Jeffreys prior is calculated from the negative expected value of the second derivative of 
            the log-likelihood: 
            $(-E_{\X|\theta} \frac{d^2}{d\theta^2}\log f(\x|\theta) )^\half = r^\half p^{-1}(1-p)^{-\half}$.  
            This turns out here to be the kernel of a beta distribution with parameters $(0,1/2)$, which is not strictly 
            an allowable parameterization of the beta (both parameters are constrained to be positive).
            \index{subjectindex}{distribution!beta}  Interestingly, this does not harm the inference process in this case, 
            although alternatively we could specify a Poisson/gamma model.  In fact, from Table~\ref{conjugate.table} we 
            know that the beta distribution is the conjugate prior for the negative binomial.  The resulting posterior is 
            therefore also beta distributed for known $r$:
            \begin{align}
	            \pi(p|\y,r) &\propto r^{\frac{1}{2}}p^{-1}(1-p)^{-\frac{1}{2}}
			                p^{nr}(1-p)^{\sum y_i}\prod_{i=1}^{n}\binom{r+y_i - 1}{y_i}
			                \nonumber\\
		                &\propto p^{nr-1} (1-p)^{\sum y_i - \frac{1}{2}}
			                \nonumber\\
		                &\sim    \mathcal{BE}\left(nr,\sum y_i + \frac{1}{2}\right).
            \end{align}

            \begin{figure}[h]
                  \hspace{-0.5in}
                  \epsfig{file=Images/testing.figure01b.ps,height=2.75in,width=5.90in,clip=,angle=0}  
	            \vspace{-18pt}
	            \caption{\textsc{One-Sided Testing for the French Strikes Data}} \label{French.Strikes.Fig}
	            \vspace{18pt}
            \end{figure}

            An initial or follow-on strike is considered a ``failure'' in the model and the corresponding ``success'' is an 
            end to the series of strikes for that period.  Therefore for the purpose of this analysis, we set $r=1$.  The 
            hypothesis of interest is:
            \begin{align}
	            \Hz p \le 0.05		\nonumber\\
	            \Ho p > 0.05,		\nonumber
            \end{align}
            meaning that the posterior probability of a cessation to the series of strikes is one in twenty or less under 
            the null.  By 
            \eqref{one.sided.testing.probability}:
            \begin{equation}
                    p(H_0|\y) = \frac{ \int_{0}^{0.05} \pi(p|\y)p(p) dp }
                                    { \int_{0}^{1} \pi(p|\y)p(p) dp } = 0.171.
            \end{equation}
            This fraction is depicted in Figure~\ref{French.Strikes.Fig} where the shaded region represents the numerator and the
            entire PDF is the denominator.  We could integrate to get this quantity but it is actually easier, and just as
            accurate, to simulate the result.  In fact it is nearly trivial in \R: we just randomly generate 1,000,000 values
            distributed according to $\text{beta}(nr,\sum y_i + \frac{1}{2})$ and count the proportion that are less than 0.05
            (actually not this many simulated values are needed for an accurate estimate but the calculation is extremely fast).  
\end{examplelist}

\subsubsection{Two-Sided Testing}\index{subjectindex}{hypothesis testing!two-sided!Bayesian}
While the standard, semi-frequentist approach naturally accommodates both one-sided and 
two-sided hypothesis tests, the Bayesian framework does not.  While one-sided hypothesis 
testing is very straightforward in Bayesian hypothesis testing, two-sided hypothesis 
testing is quite difficult and remains fairly controversial in practice.  This is not 
surprising since a Bayesian is likely to be uncomfortable placing prior mass on a point 
null hypothesis ($\Hz \theta=0$, for example).  While this appears to be a flaw in the 
Bayesian construct, it is actually an indication of how much more reasonable the approach 
is: \emph{nobody} actually believes that some parameter of interest is \emph{exactly} 
zero.  Instead, most researchers are either truly interested in a directional conclusion 
(direct and regular communication reduces hostility, countries ruled by dictators are 
\emph{more} likely to go to war, smaller classes lead to \emph{better} student 
performance, etc.), or whether some effect size is approximately equal to zero.\\

Despite the evidence that point null hypothesis testing\index{subjectindex}{hypothesis testing!point null} is antithetical to the
Bayesian philosophy, there has been considerable effort expended trying to find a reconcilable Bayesian approach (Berger and
Sellke 1987, Berger, Brown, and Wolpert 1994; Berger, Boukai, and Wang 1997, Lehmann 1993, Meng 1994a, Rubin 1984).  Furthermore,
unlike the non-nested case, posterior probability quantiles in the nested case are often substantially different than frequentist
p-values (Lee\index{authorindex}{Lee, P. M.} 2004).  
\index{authorindex}{Meng, X-L.}
\index{authorindex}{Berger, J. O.} \index{authorindex}{Sellke, T.} \index{authorindex}{Brown, L. D.}
\index{authorindex}{Wolpert, R. L.} \index{authorindex}{Boukai, B.} \index{authorindex}{Wang, Y.}
\index{authorindex}{Lehmann, E. L.} \index{authorindex}{Rubin, D. B.}

In testing $\Hz \theta=0$ versus $\Ho \theta \ne 0$, we cannot assign a continuous prior distribution for $\theta$ since this would assign 
zero mass at the null point, thus providing an infinite bias against the nesting.  One alternative is to specify a small interval 
around the null point, creating a focused null region:\index{subjectindex}{bias!model specification} $\Hz -\epsilon \le \theta \le \epsilon$.

\subsection{Attempting a Bayesian Approximation to Frequentist Hypothesis Testing}
There is not a general manner in which evidence from Bayesian posterior quantiles can be calibrated with p-values since the two
measures are fundamentally different in theory:
\begin{equation}
    \int_{T}^{\infty} p(\theta) L(\theta|\mathbf{\x})d\theta \ne p = p(T(\x) \ge T|\theta=0) 
                                                                   = \int_{T}^{\infty} f(\x|\theta=0)dx
\end{equation}
for some $\alpha$-driven critical value $T$ and some test statistics $T(\x)$
(Casella\index{authorindex}{Casella, G.} and Berger\index{authorindex}{Berger, R. L.} 1987b, p.133;
Hinkley\index{authorindex}{Hinkley, D. V.} 1987, p.128).  This does not mean that for a given frequentist model some Bayesian
parameterization that is forced to coincide cannot be found.  Particular cases include Severini\index{authorindex}{Severini, T.
A.} (1991, 1993) for HPD regions, Stein\index{authorindex}{Stein, C.} (1965) in a repeat-sample context,
DiCiccio\index{authorindex}{DiCiccio, T. J.} and Stern \index{authorindex}{Stern, S. E.} (1994) in a multivariate setting,
Thatcher\index{authorindex}{Thatcher, A. R.} (1964) for the binomial, Chang\index{authorindex}{Chang, T.} and
Villegas\index{authorindex}{Villegas, C.} (1986) for the multivariate normal, and Nicolaou\index{authorindex}{Nicolaou, A.} (1993)
for dealing with nuisance parameters.  
    
Since there does not exist a default prior that is subsumed to \emph{any} subsequently observed posterior except in the limit,
then any Bayesian setup designed to agree with quasi-frequentist results is by definition a subjective assessment of the structure
of the data.  In addition, the notion of a Bayesian p-value analog has been described as a ``paradox'' (Meng,
X-L.\index{authorindex}{Meng, X-L.} 1994a) since the quasi-frequentist averages over data that do not exist compared to the
Bayesian approach of averaging over the allowable parameter space.

The first attempt to develop a Bayesian procedure that agrees with a two-sided classical 
test is that of Lindley\index{authorindex}{Lindley, D. V.} (1961).  If the prior information is 
sufficiently vague so that one has no
particular belief that $\theta=\theta_0$ versus $\theta=\theta_0 \pm \epsilon$, where $\epsilon$ is
some small value, then a reference (ignorance-expressing, Chapter~\ref{Prior.Chapter}) prior can be 
used to obtain a posterior, and $H_0$ is rejected for values that fall out of the $(1-\alpha)100$ 
HPD region.  Highest posterior density regions are preferred over credible intervals for asymmetric 
distributions since credible intervals simply space out a specified distance from the mean 
regardless of overlying density.\index{subjectindex}{HPD region}

\section{The Bayes Factor as Evidence}\index{subjectindex}{Bayes Factor} \index{subjectindex}{model comparison}
Bayes Factors have dominated the literature on Bayesian model testing because they are often easy to calculate and have a naturally intuitive
interpretation (likelihood ratio tests are a special case).  The central notion is that prior and posterior information should be combined in 
a ratio that provides evidence of one model specification over another.  Bayes Factors are also very flexible in that multiple hypotheses can 
be simultaneously compared.  Moreover, \emph{model nesting is not required in order to make comparisons}, addressing a major deficiency with 
classical approaches (Cox\index{authorindex}{Cox, D. R.} 1961).  \index{subjectindex}{model nesting}

The most general form of the Bayes Factor can be described as follows.  Suppose we observe data $\x$ and wish to test two competing models, 
$M_1$ and $M_2$, relating these data to two different sets of parameters, $\T_1$ and $\T_2$.  This is a problem of deciding between two 
families of density specifications:
\begin{equation}
        M_1\negthickspace: f_1(\x|\T_1)
        \qquad M_2\negthickspace: f_2(\x|\T_2)
\end{equation}
where $\T_1$ and $\T_2$ are either nested within a larger set of alternative parameters, $\mathbf{\Theta}$, or drawn from distinct parameter 
spaces, $\mathbf{\Theta_1}$ and $\mathbf{\Theta_2}$.  The standard Bayesian setup specifies a prior unconditional distribution for the 
parameter vectors: $p_1(\T_1)$ and $p_2(\T_2)$, and therefore a prior probability of the two models: $p(M_1)$ and $p(M_2)$.  The posterior 
odds ratio in favor of Model~1 versus Model~2 are therefore produced by Bayes' Law:
\begin{equation}\label{bayes.Factor.1}
        \underbrace{ \frac{\pi(M_1|\x)}{\pi(M_2|\x)} }_{ \text{posterior odds} } =
        \underbrace{ \frac{p(M_1)/p(\x)}{p(M_2)/p(\x)} }_{ \text{prior odds/data} }
        \times \underbrace{ 
	    \frac{\int_{\theta_1}f_1(\x|\T_1) p_1(\T_1)d\T_1}
                 {\int_{\theta_2}f_2(\x|\T_2) p_2(\T_2)d\T_2} }_{ \text{Bayes Factor} }.
\end{equation}
So the quantity of interest turns out to be the ratio of marginal likelihoods (page~\pageref{Bayes.Law.2}) from the two models.  This 
expression equates the posterior odds ratio on the left-hand side to the product of the prior odds ratio and the ratio of integrated 
likelihoods.  Note that with fairly complicated models, the integrals in \eqref{bayes.Factor.1} can be quite challenging to compute, 
even with the Markov chain Monte Carlo procedures introduced starting in Chapter~\ref{MCMC.Chapter}.  By rearranging we get the 
standard form of the Bayes Factor, which can be thought of as the magnitude of the evidence for Model~1 over Model~2, contained in 
the data:\index{subjectindex}{posterior odds ratio}\index{subjectindex}{prior odds ratio}\index{subjectindex}{Bayes Factor!definition} 
\begin{equation}\label{bayes.Factor.2}
        BF_{(1,2)} = \frac{\pi(M_1|\x)/p(M_1)}{\pi(M_2|\x)/p(M_2)},
\end{equation}
which is also called the posterior to prior odds ratio for the obvious reason revealed in this form.  In the case where we are
willing to put equal prior probability on the two models ($p(M_1) = p(M_2) = \frac{1}{2}$) and the models share the same parameter
space but hypothesize differing levels, then the Bayes Factor reduces to the common likelihood ratio.
\index{subjectindex}{likelihood ratio}  This is equivalent to assigning simple point mass through the priors.  It is also possible
to rearrange \eqref{bayes.Factor.2} since the $p(\x)$ is the same for both models:
\begin{align}
	\frac{\pi(M_1|\x)/p(M_1)}{\pi(M_2|\x)/p(M_2)} 
		     &=	\frac{\pi(M_1,\x)/(p(\x)p(M_1))}{\pi(M_2,\x)/(p(\x)p(M_2))} \nonumber\9
		     &= \frac{\pi(M_1,\x)/p(M_1)}{\pi(M_2,\x)/p(M_2)} 
		      = \frac{\pi(\x|M_1)}{\pi(\x|M_2)},
\end{align}	
which gives another general form provided by some authors.  Commonly, the natural log of the Bayes Factor is calculated for reasons of
numerical stability.

Bayes Factors are also \emph{transitive}\index{subjectindex}{Bayes Factor!transitive property} in that multi-way comparisons
are relative.  So if we have $BF_{(1,2)}$ and $BF_{(2,3)}$, then:
\begin{equation}
    BF_{(1,2)}BF_{(2,3)} = \frac{\pi(M_1|\x)/p(M_1)}{\pi(M_2|\x)/p(M_2)}\frac{\pi(M_2|\x)/p(M_2)}{\pi(M_3|\x)/p(M_3)} = BF_{(1,3)},
\end{equation}
which is useful for multiple model comparisons using the same data. This property also means that if there exists a null model, then
a series of alternatives can be tested against it and the resulting values are comparable on the same relative scale.

Bayes Factors do not have an inherent \emph{scale}, exactly in the manner that likelihood ratios do not either.  A fundamental criticism 
of Bayes Factors is that because they lack an underlying metric, \emph{all} results are therefore arbitrary and subjective.  This is not
quite right since they are an overt \emph{relative} comparison of model fit.  Clearly we would rather see extremely large or extremely
small values of the Bayes Factor since that indicates obvious superiority of one specification over another.

While the Bayesian approach typically eschews arbitrary decision thresholds, Jeffreys\index{authorindex}{Jeffreys, H.} (1961,
p.432) gives the following typology for comparing Model~1 versus Model~2:\index{subjectindex}{Jeffreys' Bayes Factor typology}
\begin{align}
  BF_{(1,2)} > 1                                & \quad \text{model 1 supported} \nonumber\\
  1 > BF_{(1,2)} \ge 10^{-\frac{1}{2}}          & \quad \text{minimal evidence against model 1} \nonumber\\
  10^{-\frac{1}{2}} > BF_{(1,2)} \ge 10^{-1}    & \quad \text{substantial evidence against model 1} \nonumber\\
  10^{-1} > BF_{(1,2)} \ge 10^{-2}              & \quad \text{strong evidence against model 1} \nonumber\\
  10^{-2} > BF_{(1,2)}                          & \quad \text{decisive evidence against model 1,} \nonumber
\end{align}
where Model~1 is assumed to be a null model.  Kass\index{authorindex}{Kass, R. E.} and Raftery\index{authorindex}{Raftery, A. E.}
(1995) modify these categories slightly and provide a more intuitive logarithmic scale for decision criteria (also discussed in
Raftery \index{authorindex}{Raftery, A. E.} 1996).  Note that there is no explicit ``acceptance'' or ``rejection'' of hypotheses
as in the Neyman-Pearson context.  Instead the Bayes Factor (or the log of the Bayes Factor) is considered simply the weight of
evidence for Model~1 over Model~2 provided by the data, given the prior and the model specification 
(Good\index{authorindex}{Good, I. J.} 1985).  Good\index{authorindex}{Good, I. J.} points out elsewhere (1980b) that this is not 
a very new idea since Pierce first used ``weight of evidence''\index{subjectindex}{weight of evidence} in comparing hypotheses as
early as 1878, and Turing\index{authorindex}{Turing, A.} (a contemporary of Jeffreys) in 1940 used the expression ``factor in
favor of a hypothesis'' (reported in Good 1972, p.15) to mean nearly the same thing.  Karl Pearson\index{authorindex}{Pearson, K.}
also uses this phraseology, but in a less formal comparative manner (1914 and elsewhere).

It is important to remember that Jeffreys'\index{subjectindex}{Jeffreys' Bayes Factor typology} typology is still an arbitrary
designation of levels.  However, under specific circumstances the Bayes Factor relates directly to standard posterior quantities.
For instance, if we are willing to take the classical stance that there are only two plausible alternative hypotheses, then it
follows that $p(H_0|data) + p(H_1|data) = 1$ (we could also denote this $p(M_1|\x) + p(M_2|\x) = 1$ if we wanted to be specific
that each hypothesis is represented by a model).  Starting with this we can use Bayes' Law and the definition of the Bayes Factor
to produce:
\begin{align}\label{bayes.factor.relationship}
	p(H_0|\data) &= 1 - p(H_1|\data)										\nonumber\9
		    &= 1 - p(\data|H_1) \frac{p(H_1)}{p(\data)}								\nonumber\9
		    &= 1 - \frac{p(\data|H_0)}{BF_{(1,2)}} \frac{p(H_1)}{p(\data)}						\nonumber\9
		    &= 1 - \frac{1}{BF_{(1,2)}} \left[\frac{p(\data)}{p(H_0)}p(H_0|\data)\right]\frac{p(H_1)}{p(\data)} 	\nonumber\9
		    &= \left[ 1 + \frac{1}{BF_{(1,2)}} \frac{p(H_1)}{p(H_0)} \right]^{-1}.
\end{align}
Naturally other posterior quantities can be related in similar fashion to the Bayes Factor as well.  This 
result shows that the posterior probability of the null hypothesis is a function of the Bayes Factor scaled 
by the ratio of priors, and highlights quite clearly the strong influence that prior specifications can 
have on Bayesian hypothesis testing.  Lavine and Schervish (1999) provide cautionary advice when extending the Bayes Factor beyond 
such basic comparisons.            \index{authorindex}{Lavine, M.} \index{authorindex}{Schervish, M. J.}

\subsection{Bayes Factors for a Mean}\index{subjectindex}{Bayes Factor!mean test}
Consider a simple setup where $X \sim \mathcal{N}(\mu,\sigma^2)$, where the population mean $\mu$ is unknown and the 
population variance $\sigma^2$ is known.  We are interested in a two-sided test of $\Ho \mu = \mu_0$ versus 
$\Hz \mu \ne\mu_0$ (where $\mu_0$ is often 0).  We specify a normal prior under the research hypothesis ($H_1$) 
with mean $m$ and variance $s^2$.  A sample of size $n$ is collected with mean $\bar{x}$.  This leads to the Bayes Factor:
\begin{equation}\label{bayes.factor.mean}
    BF_{(H_1,H_0)} = \left(1 + \frac{ns^2}{\sigma^2}\right) 
                     \exp\left[ -\frac{n}{2} \left( \frac{1}{\sigma^2} - \frac{1}{\sigma^2+ns^2} \right)
                                \left(\bar{x} - \mu_0\right)^2 \right]
\end{equation}
(Exercise~\ref{bayes.factor.mean.exercise}).  Despite the simplicity of this calculation, there is plenty to be uncomfortable
about.  First, testing a point null hypothesis is not really a Bayesian operation since $\mu_0$ is unknown and therefore should
be assigned a prior distribution as well (as noted in Section~\ref{Adjusting.Tests.Section}).  As soon as we want to assign this
prior the idea of a single point ceases to make obvious sense. There are ways to use the Dirac Delta function as a surrogate for
such a distribution, but these are not very intuitive from an inferential sense.  The next obvious alternative is to substitute a
small region around the point of interest for the single point, but this leads to some additional noted challenges.

\subsection{Bayes Factors for Difference of Means Test}\label{section:difference.of.means}
\index{subjectindex}{Bayes Factor!difference of means}\index{subjectindex}{Student's-$t$}
This section develops the Bayesian version of the standard Student's $t$-test for normal data that uses a Bayes Factor.  
For additional details and an example, see G\"{o}nen \etal (2005).\index{authorindex}{G\"{o}nen, M.}  Surprisingly little 
has been done to adapt this standard tool to Bayesian use.  Suppose we are interested in the two-sided test:
\begin{equation*}
	H_0\negthinspace : \mu_1 = \mu_2	\qquad\qquad	H_1\negthinspace : \mu_1 \ne \mu_2,
\end{equation*}
with common variance $\sigma^2$ in the two groups.  First we need to specify the prior distribution of the effect size (difference) to be 
tested.  We will say that under the hypothesis of a non-zero difference, the standardized difference, $|\mu_1 - \mu_2|/\sigma$ has prior 
mean $\delta$ and prior variance $\sigma_\delta^2$.  Note that this allows great flexibility for the test to be performed.  Next calculate 
the standard difference of means test statistic:
\begin{equation}\label{standard.t.test.statistic}
	t = \frac{ \bar{x}_1 - \bar{x}_2 }{ \left(\frac{ (n_1-1)s_1^2 + (n_2-1)s_2^2 }{ n_1 + n_2 - 2 }\right)^\half/\sqrt{n_\delta} } 
\end{equation}
where $n_\delta = (n_1^{-1} + n_2^{-1})^{-1}$, and the degrees of freedom are $\nu = n_1 + n_2 - 2$.  The
Bayes Factor for $H_0$ over $H_1$ (large values favoring the null) is:
\begin{equation}
 	BF_{(0,1)} = \frac{ T_{\nu}(t|0,1) }{ T_{\nu}(t|\delta \sqrt{n_\delta},1 + n_\delta \sigma_\delta^2) },
\end{equation}
where $T_{\nu}(t|A,B)$ denotes the value that results from plugging $t$ into a non-central $t$-distribution PDF with $\nu$ degrees
of freedom and parameters $A$ for location and $B^\half$ for scale (see Johnson, Kotz, and Balakrishnan 1994).
\index{authorindex}{Johnson, N. L.} \index{authorindex}{Kotz, S.} \index{authorindex}{Balakrishnan, N.} G\"{o}nen \etal
(2005)\index{authorindex}{G\"{o}nen, M.} point out that this is easily implemented in \R\ by first determining whether the
following terms can be identified:
\begin{equation*}
	pv  = \sqrt{ 1 + n_\delta\sigma_\delta^2 }
	\qquad\qquad
	ncp = \frac{ \delta \sqrt{n_\delta} }{ pv },
\end{equation*}
which is easy to implement.
%Now using the \R\ command for the non-central $t$-distribution and the definition of $t$ in \eqref{standard.t.test.statistic}, 
%\begin{R.Code}
%        B <- dt(t, n1+n2-2, 0) / (dt(t/pv, n1+n2-2, ncp)/pv),
%\end{R.Code}
%we see that this is easy to implement.  
The Bayesian version of the difference of means test differs noticeably from the non-Bayesian 
variant mainly in that we get to \emph{specify} the tested effect size prior, which is an important advantage.

\subsection{Bayes Factor for the Linear Regression Model}
An obvious and useful application for the Bayes Factor is the standard linear regression model where we want to compare two,
not necessarily nested, different right-hand-side specifications in $\y = \X\B + \EP$, where $\X$ is an $n \times k$, rank 
$k$ matrix of explanatory variables with a leading vector of ones, $\B$ is a $k \times 1$ unknown vector of coefficients,
$\y$ is an $n \times 1$ vector of outcomes, and $\EP$ is a $n \times 1$ vector of residuals with $\mathcal{N}(0,\sigma^2 I)$ 
for a constant $\sigma^2$ (homoscedasticity).  On page~\pageref{linear.likelihood.equation} the likelihood function for model
$j$ is:
\begin{equation}
    L_j(\B_j,\sigma_j^2|\X_j,\y) = (2\pi\sigma_j^2)^{-\frac{n}{2}} \exp\left[ -\frac{1}{2\sigma_j^2}(\y-\X_j\B_j)'(\y-\X_j\B_j) \right]
\end{equation}
where $j=0,1$ providing models $M_0$ and $M_1$.  Notice that $\y$ is not indexed here since both models intend to explain the
structure the same outcome variable.  Again, make the definitions $\bh = (\X'\X)^{-1}\X'\y$, and 
$\hat{\sigma}^2 = (\y-\X\bh)'(\y-\X\bh)/(n-k)$. 

Now specify possibly different conjugate priors for each of these models with $k_j$ columns of $\X$ according to:
\begin{align}
    p(\B_j|\sigma^2) &= (2\pi)^{-\frac{k_j}{2}}|\SI_j|^{-\frac{1}{2}}
                    \exp\left[ -\frac{1}{2} (\B_j - \mathbb{B}_j)'\SI_j^{-1}(\B_j - \mathbb{B}_j) \right], \nonumber \\
                   &\text{and:}     \nonumber \\
                    p(\sigma_j^2)    &\propto \sigma_j^{-(a_j-k_j)}\exp\left[ -\frac{b_j}{\sigma_j^2} \right]
\end{align}
as done on page~\pageref{linear.conjugate.priors} except for a multiplier $h_j$ on the variance term in the normal prior for 
$\B_j$: $\SI_j = h_j\sigma_j^2 \mathbf{I}$.  If we make the common choice of prior mean for $\B$ to be $\mathbb{B} = 0$ in
both models, then the marginal likelihood for model $j$ from this setup is:
\begin{equation}
    p_j(\y|\X_j,M_j) = \frac{ |\X_j'\X_j + h|^{-\half}|h_j|^{\half}b_j^{a_j}\Gamma(a_j+\frac{a_j}{2}) }
                            { \pi^{\frac{n}{2}} \Gamma(a_j) } 
                       \left(2b_j + (n-k_j)\hat{\sigma_j}^2\right).
\end{equation}
This means that the Bayes Factor for Model 1 over Model 0 is given by:
\begin{equation}\label{eq:BF.linear.model}
    BF_{(1,0)} = \frac{ p_1(\y|\X_1,M_1) }{ p_0(\y|\X_0,M_0) }
          = \frac{ \frac{ |\X_1'\X_1 + h|^{-\half}|h_1|^{\half}b_1^{a_1}\Gamma(a_1+\frac{a_1}{2}) }
                            { \pi^{\frac{n}{2}} \Gamma(a_1) } \left(2b_1 + (n-k_1)\hat{\sigma_1}^2\right) }
                 { \frac{ |\X_0'\X_0 + h|^{-\half}|h_0|^{\half}b_0^{a_0}\Gamma(a_0+\frac{a_0}{2}) }
                            { \pi^{\frac{n}{2}} \Gamma(a_0) } \left(2b_0 + (n-k_0)\hat{\sigma_0}^2\right) }.
\end{equation}
This is a long expression but a relatively simple form due to the elegance of the linear model.

\index{subjectindex}{survey data}\index{subjectindex}{Bayes Factor!GLM}
\begin{examplelist}
	\item	\label{election.survey.example}
        {\bf Bayes Factors for a Model of Election Surveys.}\index{subjectindex}{example!election surveys} For generalized linear
        models with dichotomous outcome variables, it is common to specify a logit or probit link function as described in
        Appendix~\ref{GLM.Chapter}.  That is, $g^{-1}(\X\B)$ is either the logit function, $\Lambda(\X\B)$, or the standard normal
        CDF, $\Phi(\X\B)$.  So for outcome variable $Y_i$, and prior distribution $p(\B)$ on the coefficients, we obtain the
        following posterior: \index{subjectindex}{logit model} \index{subjectindex}{probit model}
		\begin{equation}\label{dichot.choice.posterior}
			\pi(\B|\X,\Y) = \frac{ p(\B)\prod_{i=1}^{n}g^{-1}(\X\B)^{y_i}(1-g^{-1}(\X\B))^{1-y_i} }
	     		{ \int_{\B} p(\B)\prod_{i=1}^{n}g^{-1}(\X\B)^{y_i}(1-g^{-1}(\X\B))^{1-y_i} d\B }.
		\end{equation}
        When the prior is some numerical constant (i.e., not a function of the $\B$) then it passes out of the integral in the
        denominator and cancels.  This then becomes equivalent to the classical model described in every econometric book ever
        printed (only a slight exaggeration).  However, in general \eqref{dichot.choice.posterior} is not available in closed form
        for most prior specifications and MCMC techniques are generally required (see Chapter~\ref{Hierarchical.Chapter} for
        specific applications of this model).  Assume for the moment that we specified a simple multidimensional uniform prior
        for $p(\B)$ and that we wish to calculate a Bayes Factor for comparing one coefficient vector against another: $\Hz \B =
        \B_0$ vs. $\Ho \B = \B_1$.\index{subjectindex}{prior distribution!uniform!Bayes Factor} Determining the weight of
        evidence for Model~2 versus Model~1 here is performed simply by inserting $\B_0$ and $\B_1$ separately into
        \eqref{dichot.choice.posterior} and calculating the Bayes Factor according to $BF_{(1,0)} =
        \pi(\B_1|\X,\Y)/\pi(\B_0|\X,\Y)$.  This is actually just the likelihood ratio for Model~2 over Model~1.  This process
        is relatively general in that we can test differing specifications as well as restricted coefficient vectors versus
        unrestricted estimates.\index{subjectindex}{restricted estimates}

	\begin{table}[h]
    \parbox[c]{\linewidth}{
	    \begin{small}
	    \begin{center}
        \hspace{-55pt}
	    \tabletitle{\textsc{1964 Electoral Data}}\label{HJ.Table}
	    \begin{tabular}{rrrrrrrrr}
   	        &       &        &        &    &     &     &    &   \\[-5pt]    
   	    $\mathbf{N}$ &   $\mathbf{F}$ &    $\mathbf{L}$ &    $\W$ & \texttt{IND} & \texttt{DEM} & \texttt{WR} & \texttt{WD} & \texttt{SD} \\
   	        &       &        &        &    &     &     &    &   \\[-5pt]   
	    \hline
   	        &       &        &        &    &     &    &    &    \\[-5pt]
   	    109 & 0.102 & -2.175 &  9.984 & 0  &  0  &  0 &  0 &  0 \\
    	    35 & 0.115 & -2.041 &  3.562 & 1  &  0  &  0 &  0 &  0 \\
    	    33 & 0.214 & -1.301 &  5.551 & 0  &  1  &  0 &  0 &  0 \\
    	    75 & 0.258 & -1.056 & 14.358 & 0  &  0  &  1 &  0 &  0 \\
    	    50 & 0.544 &  0.176 & 12.403 & 1  &  0  &  1 &  0 &  0 \\
    	    52 & 0.677 &  0.740 & 11.731 & 0  &  1  &  1 &  0 &  0 \\
    	    70 & 0.606 &  0.431 & 16.713 & 0  &  0  &  0 &  1 &  0 \\
    	    56 & 0.890 &  2.091 &  5.482 & 1  &  0  &  0 &  1 &  0 \\
   	        189 & 0.975 &  3.664 &  4.607 & 0  &  1  &  0 &  1 &  0 \\
    	    31 & 0.727 &  0.979 &  6.153 & 0  &  0  &  0 &  0 &  1 \\
    	    56 & 0.893 &  2.122 &  5.351 & 1  &  0  &  0 &  0 &  1 \\
   	        344 & 0.990 &  4.595 &  3.406 & 0  &  1  &  0 &  0 &  1 \\
	    \end{tabular}
	    \end{center}
	    \end{small}
    }
	\end{table}
	\vspace{11pt}

        Hanushek\index{authorindex}{Hanushek, E. A.} and Jackson \index{authorindex}{Jackson, J. E.} (1977) develop a grouped
        logit model\index{subjectindex}{logit model!grouped} for Factor data based on a clever estimation approach suggested by
        Theil\index{authorindex}{Theil, H.} (1970) in which the link function is applied to the outcome variable, then a grouped
        data form of least squares is used to obtain coefficient estimates.  All generalized linear
        models\index{subjectindex}{GLM!interaction effects (automatic)} automatically imply interaction effects because of the
            link function.\footnote{To see that this is true, 
			calculate the marginal effect of a single coefficient by taking the derivative of a GLM 
			specification, which does not explicitly contain an interaction term, $E(Y_i) = g^{-1}\!
			(\beta_0 + \beta_1X_{i1} + \beta_2X_{i2})$, with regard to a variable of interest.  If 
			the form of the model implied no interactions, then we would obtain a marginal effect 
			free of other variables, but this is clearly not so: $\frac{\partial Y_i}{\partial 
			X_{i2}} = \frac{\partial}{\partial X_{i2}}g^{-1}\!\left(\beta_0 + \beta_1X_{i1} + 
			\beta_2X_{i2}\right) = (g^{-1})'\!\left(\beta_0 + \beta_1X_{i1} + 
			\beta_2X_{i2}\right)\beta_2.$ The calculation demonstrates that the presence of a link 
			function requires the use of the chain rule and therefore retains other terms on the 
			right-hand side in addition to $\beta_2$, and therefore we always get for a given variable 
			partial effects that are dependent on the levels of the other explanatory variables.}
    But when Theil's method is used, these automatic interaction effects are not provided, so if we believe that such effects
    exist, they must now be explicitly provided in the model specification.  The data come from a 1964 election survey in the
    United States regarding the presidential election (available in \texttt{BaM}).  The outcome variable is the log-ratio of the
    group proportion voting for Lyndon Johnson and the binary explanatory variables are: self-indicated indifference to the
    election (\texttt{IND}), a stated preference for Democratic party issues (\texttt{DEM}), and indications of party status as
    weak Republican (\texttt{WR}), weak Democrat (\texttt{WD}), or strong Democrat (\texttt{SD}).  The full data set is given in
    Table~\ref{HJ.Table}, which is a replication from Hanushek and Jackson. % NEED TO ADD TO BaM 
    \index{authorindex}{Hanushek, E. A.} \index{authorindex}{Jackson, J. E.} 

	The vector $\mathbf{N}$ is the number of cases in the group, the vector $\mathbf{F}$ is the observed cell proportion voting 
	for Johnson, $\mathbf{L}$ is log-ratio of this proportion given by $\mathbf{L}=\log [\mathbf{F}/(1-\mathbf{F})]$ (see Theil 
	[1970, p.107]) for a justification), and $\W$ collects the inverse of the diagonal of the matrix for the group-weighting from 
	$[N_iF_i(1-F_i)]$.  The uniform prior coefficient estimate is produced by 
    \begin{equation}
        b=(\X'\W\X)^{-1}\X\W\L, 
    \end{equation}
    along with the asymptotic variance-covariance matrix $(\X'\W\X)^{-1}$, and log-likelihood 
    \begin{equation}
	    \ell(b) = -(n/2)\log (2\pi)-(n/2)\log (s^2)-(1/(2s^2))\epsilon'\epsilon.  
    \end{equation}
    Table~\ref{Hanushek.Jackson.Table} gives the results for the simple model in Hanushek and Jackson and a second model that
    hypothesizes an interaction between indifference and weak Republican.

	\begin{table}[h]
    \parbox[c]{\linewidth}{
	    \begin{center}
        \hspace{-88pt}
	    \tabletitle{\textsc{1964 Election Survey, Grouped Data}}\label{Hanushek.Jackson.Table}
	    \vspace{4pt}
	    %\begin{small}
	    \renewcommand{\arraystretch}{1.2}
	    \begin{tabular}{lcccc}
	                &        &       &        &       \\[-7pt]
            	        & \multicolumn{2}{c}{Hanushek and Jackson} & \multicolumn{2}{c}{Intercept Model} \\ \cline{2-5}
	    	        & Coefficent  & Std. Error & Coefficent  & Std. Error \\
	    \hline
	                &        &       &        &       \\[-7pt]
	    \texttt{Intercept}   & -2.739 & 0.062 & -2.709 & 0.063 \\
	    \texttt{IND}         & ~1.145 & 0.061 & ~0.952 & 0.099 \\
	    \texttt{DEM}         & ~2.167 & 0.063 & ~2.187 & 0.063 \\
	    \texttt{WR}          & ~1.598 & 0.080 & ~1.477 & 0.095 \\
	    \texttt{WD}          & ~3.459 & 0.090 & ~3.465 & 0.091 \\
	    \texttt{SD}          & ~4.049 & 0.121 & ~4.083 & 0.122 \\
	    \texttt{(IND)(WR)}   &        &       & ~0.456 & 0.214 \\
	    \end{tabular} 
	    %\end{small} 
	    \end{center} 
    }
	\end{table}
	
	What these results show is that the interaction term is statistically reliable according to traditional
	levels and that the rest of the model is not substantively changed.  So which specification is better?
	The Bayes Factor for the interaction model relative to the Hanushek\index{authorindex}{Hanushek, E. A.} and 
	Jackson\index{authorindex}{Jackson, J. E.} model is $0.944$, indicating little support for adding the 
	interaction according to Jeffreys' typology\index{subjectindex}{Jeffreys' Bayes Factor typology} (given of 
	course the unsupported assumption of uniform priors).
\end{examplelist}
	

\subsection{Bayes Factors and Improper Priors}\index{subjectindex}{Bayes Factor!improper priors}\index{subjectindex}{prior distribution!improper}
From \eqref{bayes.Factor.1} it is easy to see that the form of the prior has a noticeable effect on the resulting Bayes Factor.  This 
sensitivity to the prior is a main criticism of Bayes Factors in general (see Kim\index{authorindex}{Kim, D.} 1991).  Interestingly, the form of 
the prior has a much greater effect on the Bayes Factor than other forms of Bayesian inference such as quantile descriptions of the posterior or 
posterior predictive distribution (Aitkin\index{authorindex}{Aitkin, M.} 1991; Gelman\index{authorindex}{Gelman, A.}, 
Meng\index{authorindex}{Meng, X-L.}, and Stern\index{authorindex}{Stern, H. S.} 1996).  
In standard Bayesian analysis, a substantial similarity between the prior and the 
posterior is evidence that the data had much less of an impact than the prior.  
This easily detected situation would be cause for alarm.  For instance, in the case
where a conjugate prior is specified, if the shape of the posterior distribution is 
very close to the prior, then we know that the beginning assumptions are relatively 
unmodified by conditioning on the data.\index{subjectindex}{Bayesian inference}

\index{subjectindex}{Bayes Factor!improper priors} \index{authorindex}{Mortera, J.} 
In the case where improper priors are used, the Bayes Factor cannot be specified except under very uninteresting 
	scenarios\footnote{Improper priors can be assigned to unknown nuisance parameters that are common to both models.  Therefore 
	they are not applicable to the parameters that motivate the model comparison in the first place.}
(Berger\index{authorindex}{Berger, J. O.} and Mortera 1999, p.542; Kass\index{authorindex}{Kass, R. E.} and Wasserman
\index{authorindex}{Wasserman, L.} 1995 [discussion], p.777).  The most common improper prior setup is to specify one or both of the 
densities proportional to a multiplicative constant:
\begin{equation}
        p_1(\theta_1) = c_1g_1(\theta_1)      \qquad  p_2(\theta_2) = c_2g_2(\theta_2),
        \label{improper.priors}
\end{equation}
where $g_1$ and/or $g_2$ are functions whose integrals over the respective sample spaces do not diverge.  A very common improper prior 
specification is a constant, $p(\theta) = k$, over the Lebesgue measure on $(-\infty,\infty)$ (see Chapter~\ref{Prior.Chapter}, 
Section~\ref{Improper.Priors.Section}).  One way to visualize this is as a rectangle that is $k=g(\theta)$ high and $c=\infty$ wide.  
Obviously $c$ is an infinite proportionality constant,\index{subjectindex}{infinite proportionality constant} but it typically does not 
prevent the calculation of a proper posterior distribution since:
\begin{align}
        \pi(\theta|\x) &= \frac{p(\theta) p(\x|\theta)}{\int_\theta p(\theta) p(\x|\theta)d\theta} \nonumber\\[9pt]
                       &= \frac{c g(\theta) p(\x|\theta)}{c\int_\theta g(\theta) p(\x|\theta)d\theta} \nonumber\\[9pt]
                       &= \frac{g(\theta) p(\x|\theta)}{\int_\theta g(\theta) p(\x|\theta)d\theta}.
\end{align}
To see that this is not true for Bayes Factors, take the form from \eqref{bayes.Factor.1} and substitute priors with specified 
proportionality constants according to \eqref{improper.priors} to obtain:\index{subjectindex}{Bayes Factor!improper priors}
\begin{equation}
        BF_{(1,2)} = \frac{c_1}{c_2}
            \frac{ \int_{\theta_1} g_1(\theta_1) p(\x|\theta_1)d\theta_1 }
                 { \int_{\theta_2} g_2(\theta_2) p(\x|\theta_2)d\theta_2 }.
        \label{improper.BF}
\end{equation}
So for any two proper priors (or improper priors that are finite and proportional to proper priors) the Bayes Factor can still be 
calculated.  However, if both of $c_1$, $c_2$ are unbounded, then the Bayes Factor is incalculable because the ratio of two different 
unknown unbounded quantities does not cancel out.

Returning to the problematic case of the two-sided (nested) Bayesian hypothesis test,
\index{subjectindex}{hypothesis testing!two-sided!Bayesian} this section treats the problem with Bayes Factors instead of posterior 
distribution quantiles.  This case represents the widest gulf between the frequentist and the Bayesian approaches, where tests to 
coincide can result in differences greater than an order of magnitude (Berger\index{authorindex}{Berger, J. O.} 
and Sellke\index{authorindex}{Sellke, T.} 1987, 
Casella\index{authorindex}{Casella, G.} and 
Berger\index{authorindex}{Berger, R. L.} 1987a, 
Lindley\index{authorindex}{Lindley, D. V.} 1957).  
While the easiest solution to the problems of two-sided testing is to 
dismiss this setup, as atheoretical and impractical,  avoiding such nested
problems is unreasonable in practice since linear regression, easily the 
most commonly used statistical procedure in the social and behavioral 
sciences, nests a null hypothesis of a zero coefficient value within 
the full sample space.\index{subjectindex}{model nesting}

\subsubsection{Local Bayes Factor}\index{subjectindex}{Bayes Factor!local}
Smith\index{authorindex}{Smith, A. F. M.} and Spiegelhalter\index{authorindex}{Spiegelhalter, D. J.} (1980) and 
Spiegelhalter\index{authorindex}{Spiegelhalter, D. J.} and Smith\index{authorindex}{Smith, A. F. M.} (1982) developed 
an ingenious way to solve the problem posed in \eqref{improper.BF} for nested linear models and known variances using 
an idea based loosely on Atkinson\index{authorindex}{Atkinson, A. C.} (1978), Geisser\index{authorindex}{Geisser, S.} 
and Eddy\index{authorindex}{Eddy, W. F.} (1979), and Lempers\index{authorindex}{Lempers, F. B.} (1971, Chapter 6).  
Suppose we ``imagine'' a set of data as a training sample and solve for $\frac{c_1}{c_2}$ in \eqref{improper.BF}:
\index{subjectindex}{training sample} \index{subjectindex}{Bayes Factor!improper priors}
\begin{equation}
BF_0 \frac{ \int_{\theta_2} g_2(\theta_2) p(\mathbf{x_0}|\theta_2)d\theta_2 }
                 { \int_{\theta_1} g_1(\theta_1) p(\mathbf{x_0}|\theta_1)d\theta_1 } = 
        Est.\left[\frac{c_1}{c_2}\right]
        \label{SandS.calc}
\end{equation}
(the order of the fraction with integrals changes as it moves to the left-hand-side of the equality).  The choice of the training
data, $\x_0$, is done with the objective of finding the smallest data set that gives proper posterior distributions in
\eqref{SandS.calc} and so that the simplest model is most favored: $BF_0(\x)$ is maximized for this specification over the other.
So that the second criterion is not perpetuated into the next stage of the test, Smith\index{authorindex}{Smith, A. F. M.} and
Spiegelhalter\index{authorindex}{Spiegelhalter, D. J.} set $BF_0(\x)=1$ with the theoretical justification that if the training
model is truly minimal, then the value of one is a good estimate.  Since the imaginary data set is conjured to support the model
in the numerator to the greatest extent possible, this value of one is actually a conservative value as it is now the lower bound
on $BF_0(\x)$.\index{subjectindex}{training sample!minimal}

The final Bayes Factor is thus a product of the training ``prior'' for $\frac{c_1}{c_2}$
and the Bayes Factor for the rest of the data:\index{subjectindex}{prior distribution!training}
\begin{equation}
        BF_{(1,2)} = Est.\left[\frac{c_1}{c_2}\right]
            \frac{ \int_{\theta_1} g_1(\theta_1) p(\x|\theta_1)d\theta_1 }
                 { \int_{\theta_2} g_2(\theta_2) p(\x|\theta_2)d\theta_2 }
        \label{SandS.BF}
\end{equation}
(note that the order of the fraction is returned because it is back on the right-hand-side of the equality).
\index{subjectindex}{normalizing factor}
Therefore by this method, we remove the non-identifiability\index{subjectindex}{non-identifiability} 
problem associated with the undefined ratio of normalizing constants and obtain a 
solution for the Bayes Factor.  Pettit\index{authorindex}{Pettit, L. I.} (1992) applies this 
method to nested linear models with outliers to judge the sensitivity of Bayes 
Factors to outliers when specifying improper priors (which necessarily put more 
weight on the model with more outliers).  Adman\index{authorindex}{Adman, V. E.} and 
Raftery\index{authorindex}{Raftery, A. E.} (1986) apply \emph{local Bayes Factors}
\index{subjectindex}{Bayes Factor!local}
(i.e., one that compares a specific model, $M_1$ with closely related models 
contained in a super-set model $M_2$ where closeness is defined as those 
alternatives that give a high weight for producing similar coefficient 
	estimates)\footnote{Smith\index{authorindex}{Smith, A. F. M.} and 
	Spiegelhalter\index{authorindex}{Spiegelhalter, D. J.} (1980) demonstrate a linkage
	between local Bayes Factor comparisons and Akaike information
	criterion (AIC)\index{subjectindex}{Akaike information criterion (AIC)} comparisons, 
	see p.\pageref{AIC.definition}.}
to a nonhomogeneous Poisson process\index{subjectindex}{Poisson process} using bounded improper priors
(Adman \index{authorindex}{Adman, V. E.} and Raftery \index{authorindex}{Raftery, A. E.} 1986, Raftery
\index{authorindex}{Raftery, A. E.} and Adman\index{authorindex}{Adman, V. E.} 1986), noting that this case is more difficult with
linear models since there are data conditions that can provide maximal support for the null resulting in an undefined ratio of
constants.\index{subjectindex}{Bayes Factor!improper priors}

While this idea of a minimal training\index{subjectindex}{training sample!minimal} sample providing a prior estimate 
of the incalculable quantity is very creative, two obvious problems exist.  The first is the fixing of $B_0(\x)=1$, 
a decision that lacks any theoretical justification other than the relatively vague idea that it is likely to be 
close in an optimal situation (for which there is no guarantee and no test).  The second is that determination of the 
training sample is difficult both in terms of selecting the sample size (Lempers\index{authorindex}{Lempers, F. B.} 
[1971] arbitrarily picked half of the full sample as the size of the training sample) and the sample components 
(there are $\binom{n}{m}$ ways of picking a training sample of $m$ out of a full sample of $n$).  For example, 
consider a state of nature that is maximally malevolent in that the training sample produces a premultiplier in 
\eqref{SandS.BF} that is as different from the rest of the Bayes Factor calculation as possible.  Since all of the 
integration in \eqref{SandS.BF} is done over the sample space of $\theta$, then the sample size of the training 
sample is equally weighted with the rest of the presumably much larger sample, and the ability to change the final 
Bayes Factor is thus substantial.

\subsubsection{Intrinsic Bayes Factor}
Berger\index{authorindex}{Berger, J. O.} and Pericchi\index{authorindex}{Pericchi, L. R.} (1996a, 1996b) propose a method of
picking the training sample that depends on the number of possible training samples being relatively small.  Their ``intrinsic''
Bayes Factor\index{subjectindex}{Bayes Factor!intrinsic} is an average of the Bayes Factors from every combinatorically possible
training sample.  In cases where the number of possible training sets is prohibitively large (i.e., almost every realistic
scenario in the social sciences), then a random sample of the possible training samples can be used.

To create the intrinsic Bayes Factor, Berger\index{authorindex}{Berger, J. O.} and Pericchi\index{authorindex}{Pericchi, L. R.}
first start with intrinsic prior densities $\nu(\theta_1)$ and $\nu(\theta_2)$.  An intrinsic prior is uninformative prior in the
sense that it provides the asymptotic equivalence of the Bayes Factor to the maximum likelihood ratio.  For details see
Berger\index{authorindex}{Berger, J. O.} and Pericchi\index{authorindex}{Pericchi, L. R.} (1996a) or
Berger\index{authorindex}{Berger, J. O.} and Mortera\index{authorindex}{Mortera, J.} (1999).  Next define a \emph{minimal training
sample}, $\x_m$, the smallest possible subset of the full sample, $\x_n$, so that at least one of the resulting posterior
densities using the intrinsic priors are proper.\index{subjectindex}{training sample!minimal} Calculate these minimum training set
posterior distributions: $\pi^I_i(\theta_i|\x) \propto \nu(\theta_i)  f_i(\x_m|\theta_i)$.  The intrinsic Bayes Factor is then
defined to be:
\begin{equation}
  B^I(\x) = \frac{ p^I_1(\theta_1|\x) }{ p^I_2(\theta_1|\x) }
                    AVE\left[\frac{ \pi^I_2(\theta_2|\x) }
                                  { \pi^I_1(\theta_1|\x) }\right],
\end{equation}
where $AVE[\;]$ is some average: arithmetic mean, geometric mean, harmonic 
mean, median, etc., over all possible minimal training samples.  This avoids 
the previously mentioned problem of inadvertently selecting a maximally skewed 
training sample.  Furthermore, since the average selected does not have to be 
a mean, some other robustizing measure of central location can be used so that the 
procedure is robust to outliers.  Unfortunately, selection of the averaging 
procedure can also make a substantive difference in the resulting Bayes Factor, 
and Berger\index{authorindex}{Berger, J. O.} and 
Pericchi\index{authorindex}{Pericchi, L. R.} do not 
provide any theoretical justification for one form over another.

In general, intrinsic Bayes Factors are more appropriate for two-sided tests 
than one-sided tests, since there is no explicit incorporation of direction 
(see Moreno\index{authorindex}{Moreno, E.} 1997).  
Berger\index{authorindex}{Berger, J. O.} and 
Pericchi\index{authorindex}{Pericchi, L. R.} identify intrinsic priors for large classes 
of nested models and specification, but found that they are not generally
appropriate for developing Bayes Factors in non-nested models.

The challenge in developing the intrinsic Bayes Factor is obtaining the 
appropriate intrinsic prior distributions for each of the tested models.  
While Berger\index{authorindex}{Berger, J. O.} and 
Pericchi\index{authorindex}{Pericchi, L. R.} 
tabulate some common forms, most situations will require separate 
derivation.  However, Berger\index{authorindex}{Berger, J. O.} \etal (1998) find a 
number of invariant specifications where the marginal density of the 
minimal training set is actually available in closed form analytically.

\subsubsection{Partial Bayes Factor}
O'Hagan\index{authorindex}{O'Hagan, A.} (1995) attempts to repair the difficulties in
specifying the local Bayes Factor in two ways, beginning with ``partial 
Bayes Factors.''\index{subjectindex}{Bayes Factor! partial} The procedure is quite simple.  
First divide the sample \index{subjectindex}{training sample}
into two components, $\x = (\mathbf{y},\mathbf{z})$, where $\mathbf{y}$ 
is the training sample of size $m$, and $\mathbf{z}$ is the sample 
proportion of size $n-m$ used for model comparison.  Use $\mathbf{y}$ to 
obtain training posteriors for the alternative models: $\pi_1(\theta_1|\mathbf{y})$
and $\pi_2(\theta_2|\mathbf{y})$ by assigning any desired prior for 
$\theta$ (including an improper prior) and using $\pi(\theta|\x) \propto p(\theta) p(\x|\theta)$.  
	\index{subjectindex}{prior distribution!improper}%
These training posteriors will necessarily be proper, and are used as 
priors for calculating the Bayes Factor for the rest of the sample:
\begin{equation}
  BF(\mathbf{z}|\mathbf{y})
  =\frac{ \int_{\theta_1} \pi_1(\theta_1|\mathbf{y})f_1(\mathbf{z}|\theta_1,\mathbf{y})d\theta_1 }
       { \int_{\theta_2} \pi_2(\theta_2|\mathbf{y})f_2(\mathbf{z}|\theta_2,\mathbf{y})d\theta_2 },
  \label{FBF}
\end{equation}
where:
\begin{equation*}
 	\int_{\theta_i} \pi_i(\theta_i|\mathbf{y})f_i(\mathbf{z}|\theta_i,\mathbf{y})d\theta_i =
		\int_{\theta_i} \pi_i(\theta_i)f_i(\x|\theta_i)d\theta_i \bigg/
		\int_{\theta_i} \pi_i(\theta_i)f_i(\mathbf{y}|\theta_i)d\theta_i.
\end{equation*}
Since $BF(\x) = BF(\mathbf{y},\mathbf{z})$, then $BF(\mathbf{z}|\mathbf{y}) = BF(\x)/BF(\mathbf{y})$, and it is clear that the 
partial Bayes Factor simply divides out the undefined ratio of normalizing constants.  Furthermore, 
O'Hagan\index{authorindex}{O'Hagan, A.} (1995, p.105) shows that the partial Bayes Factor is asymptotically consistent in that 
it will choose the correct model with probability one as $n/m \rightarrow \infty$.  \index{subjectindex}{normalizing factor}

\subsubsection{Fractional Bayes Factor}
The partial Bayes Factor eliminates the local Bayes Factor assumptions about 
$B_0$ and the problems associated with averaging over many possible training 
samples as with the intrinsic Bayes Factor, but it still requires determination 
of the proportion of the data selected as the training sample.  \index{subjectindex}{training sample}
O'Hagan\index{authorindex}{O'Hagan, A.} (1995) suggests a modification of the partial Bayes 
Factor, which makes the selection of the training sample, $\mathbf{y}$, less 
important.  Define $\eta = m/n$.  If $m$ and $n$ are reasonably large, then by
the properties of likelihood estimators $\ell(\mathbf{y}|\theta)\approx\ell(\x|\theta)^\eta$,
meaning that the likelihood based on the training sample approximates the full likelihood adjusted for
sample size.  For proper or improper priors on $\theta$, this leads to a Bayes Factor of the
following form:\index{subjectindex}{Bayes Factor!fractional}
\index{subjectindex}{Bayes Factor!improper priors}
\index{subjectindex}{training sample}
\begin{equation}
  BF_{\eta}(\x) =
  \frac{ \int_{\theta_1} \pi_1(\theta_1)f_1(\x|\theta_1)d\theta_1/\int_{\theta_1} \pi_1(\theta_1)f_1(\x|\theta_1)^{\eta}d\theta_1 }
       { \int_{\theta_2} \pi_2(\theta_2)f_2(\x|\theta_2)d\theta_2/\int_{\theta_2} \pi_2(\theta_2)f_2(\x|\theta_2)^{\eta}d\theta_2 }.
\end{equation}
Therefore if $\pi_1$ or $\pi_2$ are specified as improper priors, then the indeterminate constant
will cancel out in either the numerator or the denominator.  Also, the initial step of calculating
training posteriors has been removed by the $\eta$-ratio: absolute values of the prior density are
normalized out.

As with the partial Bayes Factor, determination of the proportion of the sample to commit to solving
the prior problem is influential on the resulting ratio.  It is obvious that $\eta$ should progress
toward zero as $n$ goes to infinity: in the limit the likelihood function subsumes any prior,
including improper priors.  O'Hagan\index{authorindex}{O'Hagan, A.} recommends the value $m_0$, which is the minimum value that
provides a consistent model choice, since this provides the greatest possible proportion of the data
for comparing the models.  In the presence of outliers, or potential outliers, he recommends
$\sqrt{n}$ or $\log(n)$ as robustizing values.  In any event, the subjectivity of priors is
replaced to some extent by the subjectivity of sample dichotomization.

The fractional Bayes Factor\index{subjectindex}{Bayes Factor!fractional} loses quite a bit of the 
character of Bayesian inference by mechanically removing the impact of the improper 
prior.  The resulting quantity no longer has a Bayes Factor interpretation since it 
is not the ratio of alternate posteriors over priors, although it remains useful (Conigliani, Castro, and O'Hagan 2000). 
Rubin\index{authorindex}{Rubin, D. B.} (1984, p.1152) sets out the following definition of \emph{Bayesianly justifiable} 
calculation:\index{subjectindex}{Bayesianly justifiable}
\index{subjectindex}{Bayes Factor!improper priors}
\begin{quote}
  \ldots it treats known values as observed values of random variables, treats unknown values as
  unobserved random variables, and calculates the conditional distribution of unknowns given knowns
  and model specifications using Bayes' theorem.
\end{quote}
As new data are observed, the researcher will have to adjust the value of $\eta$ or recognize
that the test is changing criteria.  Therefore new fractional Bayes Factors will not be constants of
previous fractional Bayes Factors and the update from the new data.  While this is only mildly 
inconvenient, it does violate the conditional updating of estimates of unknowns based on knowns in 
the Rubin\index{authorindex}{Rubin, D. B.} statement of Bayesianly justifiable.

\subsubsection{Redux}
It is appropriate to end this section with a brief discussion of what these methods are doing relative to alternatives.  Given the
general problem that Bayes Factors can be sensitive to the selection of priors and the specific problem that Bayes Factors are
incalculable for improper priors, these four techniques segment the sample in differing ways to cancel out the effects of
indeterminate constants as seen in \eqref{improper.BF}.  While these approaches all use sample quantities to determine prior
density specifications, they are not empirical Bayesian methods (Carlin\index{authorindex}{Carlin, B. P.} and
Louis\index{authorindex}{Louis, T. A.} 2009, Chapter~5) in the classic sense: use of the data to empirically estimate the highest
level of hyperpriors in a hierarchical model.  Furthermore, none of these models produces the goal of ``objective Bayesianism''
\index{subjectindex}{objective Bayes} since subjective decisions are made in every case.  Nonetheless, despite the
discussed flaws, each of these methods provides a means of presenting Bayes Factor evidence for one model over another using
improper priors and either a nested or non-nested test.\index{subjectindex}{Bayes Factor!improper priors}

\subsection{Two-Sided Hypothesis Tests and Bayes Factors}\label{Adjusting.Tests.Section}
\index{subjectindex}{hypothesis testing!Bayesian!adjusting} 
Berger\index{authorindex}{Berger, J. O.} and 
Delampady\index{authorindex}{Delampady, M.} (1987) propose integrating over two sections of 
the prior space, $p(\theta)$, to create separate prior densities for the unknown 
coefficient under the alternate assumptions that $H_0$ and $H_1$ are true.  This 
is an attempt to show that the common classical practice of testing a point null 
hypothesis\index{subjectindex}{hypothesis testing!point null} can be approximated by a precise 
null \emph{interval} hypothesis.  The test begins with segmenting an unconditional 
prior probability across the sample space of $\theta$, and defining two conditional 
priors:
\begin{align}
  g_0(\theta|H_0) =& \frac{1}{\pi_0}p(\theta)I_\Omega(\theta) \nonumber \\
  g_1(\theta|H_1) =& \frac{1}{1-\pi_0}p(\theta)I_{\Omega\prime}(\theta) \nonumber \\
      \text{where:}& \quad  \Omega = \{\theta\range |\theta-\theta_0|<\epsilon\},
     \quad \text{and} \quad \pi_0 = \int_\Omega p(\theta)d\theta.
\end{align}
In this intuitive setup $\pi_0$ is the prior probability corresponding to $H_0$ and 
$g_0$ is the conditional distribution of $\theta$ assuming that this $H_0$ is true.  
Conversely, $1-\pi_0$ is the prior density corresponding to $H_1$ and $g_1$ is the 
conditional distribution of $\theta$ assuming that $H_1$ is true.  The primary 
difficulty is determining a reasonable interval around the point null: 
$\{\theta_0 - \epsilon : \theta_0 + \epsilon\}$ so that the test: $\Hz |\theta - \theta_0| 
\le \epsilon, \Ho |\theta - \theta_0| > \epsilon$ substitutes for the true point 
null test: $\Hz \theta = \theta_0, \Ho \theta \ne \theta_0$.  

The core problem is that larger values of $\epsilon$ move further away from the desired nature of the two-sided problem and that
smaller values of $\epsilon$ move toward ``Lindley's\index{subjectindex}{paradox!Lindley's/Jeffreys'} Paradox,'' alternately
called ``Jeffreys' Paradox'' (Jeffreys\index{authorindex}{Jeffreys, H.} 1961, Lindley\index{authorindex}{Lindley, D. V.} 1957,
Shafer \index{authorindex}{Shafer, G.} 1982); this is the fact that in testing a point null hypothesis for a fixed prior, and
posterior cutoff points calibrated to match some constant frequentist $\alpha$ value, as the sample size goes to infinity,
$p(H_0)\rightarrow 1$.  This happens no matter how small the value for $\alpha$ happens to be because parameter values of
negligible likelihood are given nonzero prior weights (Aitkin\index{authorindex}{Aitkin, M.} 1991, p.115).  Furthermore, there is a
problem when the range imposed by $\epsilon$ is not theoretically driven: ``Such limits would be a sheer guess and merely
introduce an arbitrariness'' (Jeffreys\index{authorindex}{Jeffreys, H.} 1961, 367).  Casella\index{authorindex}{Casella, G.} and
Berger\index{authorindex}{Berger, R. L.} (1987b) object to Berger\index{authorindex}{Berger, J. O.} and
Delampady's\index{authorindex}{Delampady, M.} practice of assigning $\pi_0 = \frac{1}{2}$ as a no-information prior since no
reasonable researcher would begin some enterprise with an expectation of failure at 50\%.

\index{authorindex}{Berger, J. O.} \index{authorindex}{Brown, L. D.} \index{authorindex}{Wolpert, R. L.}
\index{authorindex}{Boukai, B.} \index{authorindex}{Wang, Y.}
Berger, Brown, and Wolpert (1994) followed by Berger, Boukai, and Wang (1997) designed a  
conditioning statistic so that frequentist probabilities coincide with analogous Bayesian
posterior density regions, where the cost of this coincidence is the introduction of a middle
``no-decision''\index{subjectindex}{hypothesis testing!no decision region} region into the hypothesis test.

Starting with a purely Bayesian perspective, if a prior probability, $\pi_0$, is assigned to $H_0$ with complementary prior, 
$1-\pi_0$, for $H_1$, then the posterior probability in favor of $H_0$ is: 
\begin{equation}
    p(H_0|\x) = \left[1 + \frac{1-\pi_0}{\pi_0} \frac{1}{BF_{(0,1)}}\right]^{-1}, 
\end{equation}
where $BF_{(0,1)}$ is the Bayes Factor for $H_0$ over $H_1$.  If equal probability is assigned to the two hypotheses, then 
\begin{equation}
    p(H_0|\x) = BF_{(0,1)}/(1+BF_{(0,1)}), \qquad\text{and}\qquad p(H_1|\x) =1/(1+BF_{(0,1)}).  
\end{equation}    
This leads to the selection of $H_0$ if $BF_{(0,1)} \le 1$, and $H_1$ if $BF_{(0,1)} >1$.

The setup just described is deliberately rigged to resemble a frequentist decision-making process.  These authors specify 
$F_0$ and $F_1$ as the assumed smooth and invertible cumulative distribution function of the Bayes Factor, $BF_{(0,1)}$, 
under the assumption of Model~0 and Model~1 respectively.  Now define:
\begin{align}
    r=1, \quad \alpha=F_0^{-1}(1-F_1(1)), \quad &\text{if} \quad F_0^{-1}(1-F_1(1)) \ge 1 \nonumber\\
    r=F_1^{-1}(1-F_0(1)), \quad \alpha=1,\quad &\text{if} \quad F_0^{-1}(1-F_1(1)) <  1.
\end{align}
This leads to the following test with three decision regions:
\begin{align}
  \text{Reject $H_0$ if}&\quad        BF_{(0,1)} \le r \nonumber\\
  \text{No decision if}&\quad     r < BF_{(0,1)} \le \alpha \nonumber\\
  \text{Accept $H_0$ if}&\quad        BF_{(0,1)} \ge \alpha \label{BBW.Test}
\end{align}
for values of the Bayes Factor.  The frequentist conditioning statistic $S=\text{min}[B,F_0^{-1}(1-F_1(B))]$ leads to 
identical conditional error probabilities as the Bayes Factor test outlined in \eqref{BBW.Test}.  Despite the authors' 
claims that the no-decision region is observed to be small in their empirical trials, most Bayesians are likely to be 
uncomfortable with the idea that some fraction of the sample space of the test statistic remains ambiguous (should a 
loss function\index{subjectindex}{loss function} be assigned to the no-decision region as well?).  Non-Bayesians, 
however, are more accustomed to the idea of ``weak evidence'' for a given hypothesis.  Finally, short of showing a 
clever intersection of Bayesian and frequentist testing, this approach may be too restrictive to both sides to be 
widely useful.

\subsection{Challenging Aspects of Bayes Factors}
It should also be noted that there is nothing in the setup of the Bayes Factor or the subsequent judgment about the strength of
evidence for one hypothesis over another that accommodates a directional (one-sided) test.  Therefore Bayes Factors are restricted
to the two-sided test considered here (Berger\index{authorindex}{Berger, J. O.} and Mortera\index{authorindex}{Mortera, J.} 1999),
although some authors have found exceptions for very particular situations such as strict parameter restrictions
(Dudley\index{authorindex}{Dudley, R. M.} and Haughton\index{authorindex}{Haughton, D.} 1997), or orthogonalized parameters
(Kass\index{authorindex}{Kass, R. E.} and Vaidyanathan\index{authorindex}{Vaidyanathan, S. K.} 1992).
For discussions of one-sided Bayes Factors see Casella and Berger (1987a), Moreno (2005), Marden (2000), and Wetzels \etal (2009).
\index{authorindex}{Marden, J. I.} \index{authorindex}{Casella, G.} \index{authorindex}{Berger, R. L.} \index{authorindex}{Moreno, E.}
\index{authorindex}{Wetzels, R.} \index{authorindex}{Raaijmakers, J. G. W.} \index{authorindex}{Jakab, E.} 
\index{authorindex}{Wagenmakers, E-J.}

Occasionally authors make simplifying assumptions that are hard to support in actual data-analytic settings.  For instance, some
researchers use \emph{Laplace's method}, described in Section~\ref{laplace.approximation.section}, which makes the assumption that
the posterior is approximately normally distributed to get Bayes Factors in otherwise difficult cases.  See, for instance, Kass
(1993), Kass, Tierney, and Kadane (1989), Kass and Raftery (1995), Tierney and Kadane (1986), and Wong and Li (1992).  This
assumption is far less worrisome with a large sample and in small dimensions, yet this is not always the case in social science
research.  
	\index{authorindex}{Kass, R. E.}%
	\index{authorindex}{Tierney, L.}%
	\index{authorindex}{Kadane, J. B.}%
	\index{authorindex}{Raftery, A. E.}%
	\index{authorindex}{Wong, W. H.}%
	\index{authorindex}{Li, B.}%

Some modifications of the Bayes Factor appear not to work in practice.  The posterior Bayes Factor (Aitkin 1991) uses the ratio of 
the posterior means as a substitute for the standard form in order to be less sensitive to the form of the prior.  Unfortunately the 
posterior Bayes Factor introduces some minor undesirable properties such as noninvariance to hypothesis aggregation 
(Lindley\index{authorindex}{Lindley, D. V.} 1991, Comments), and that it can prefer models independently of the strength of the data 
(Goldstein\index{authorindex}{Goldstein, M.} 1991).

Sometimes the Bayes Factor is just too unstable to compute numerically due to large ratio differences and computer
rounding/truncating at the register level.  Machine accuracy matters, of course, when comparing very large to very small numbers.
Using natural logarithms is often an effective way to deal with numerical problems, and the log of the Bayes Factor has a nice
interpretation:
\begin{equation}
    \log(BF_{(1,2)}) = \log\left( \frac{\pi(M_1|\x)/p(M_1)}{\pi(M_2|\x)/p(M_2)} \right)
                     = \log\left( \frac{\pi(M_1|\x)}{\pi(M_2|\x)} \right)
                     - \log\left( \frac{p(M_1)}{p(M_2)} \right).
\end{equation}
This means that the log of the Bayes Factor is the log ratio of the posteriors minus the log ratio of the priors, which contrasts
posterior and prior information between the two models.

Gelman and Rubin \index{authorindex}{Gelman, A.} \index{authorindex}{Rubin, D. B.} 
(1995) criticize the use of Bayes Factors for the same reason that others denigrate the null 
hypothesis significance test: it is assumed in the comparison that one of the proposed models is the correct specification.  This is 
really more of a problem in the way that social science research is presented in published work where often only a very limited number 
of specifications are tested, whereas many more were posited in earlier stages of the research.  Also, Han and Carlin (2001) 
outline computational problems that arise from using Bayes Factors for some hierarchical models 
(see Chapter~\ref{Hierarchical.Chapter}) and some of the Markov chain Monte Carlo challenges (see Chapter~\ref{MCMC.Chapter}).

Occasionally the integrals in \eqref{bayes.Factor.1} are forbidding, but Chib (1995) provides a way of finding marginal
likelihoods from Gibbs sampling output, and Chib and Jeliazkov (2001) show how this can be done with Metropolis-Hastings output.
\index{authorindex}{Jeliazkov, I.}\index{authorindex}{Chib, S.} We will describe these techniques later in
Chapter~\ref{MCMC.Utilitarian.Chapter} in Section~\ref{Section:chibs.method} (starting on page~\pageref{Section:chibs.method}).
\index{authorindex}{Chib, S.} \index{authorindex}{Jeliazkov, I.}  Also Morey \etal (2010) use the Savage-Dickey density ratio with
MCMC output to conveniently calculate the Bayes Factor. \index{authorindex}{Morey, R. D.} These tools are extremely useful, but 
restricted to MCMC output.  Additionally, the \R\ package \mcmcpack\ by Martin, Quinn and Park provides a useful function, 
\texttt{bayesF}, for calculating the Bayes Factor for commonly used regression models, and the \R\ package \texttt{BayesFactor}
by Morey and Rouder provides the Bayes Factor individually for a list of common models. Other package authors have included 
functions as well to address specific problems.  \index{subjectindex}{BayesFactor@\texttt{BayesFactor}}
\index{authorindex}{Quinn, K. M.} \index{authorindex}{Martin, A.} 

\section{The Bayesian Information Criterion (BIC)}
Kass\index{authorindex}{Kass, R. E.} (1993) and Kass and Raftery\index{authorindex}{Raftery, A. E.} (1995) suggest using the
Bayesian information criterion (BIC)\index{subjectindex}{Bayesian information criterion (BIC)} \index{subjectindex}{Schwarz
criterion} as a substitute for the full calculation of the Bayes Factor when such calculations are difficult since the BIC can be
calculated without specifying priors.  It is therefore more appealing to non-Bayesians and seemingly less subjective.  Alternately
called the Schwarz criterion, after the author of the initial article (1978), it is given by:
\index{subjectindex}{Bayesian information criterion (BIC)} 
\begin{equation}
        \text{BIC} = -2\ell(\hat{\T}|\x) +p\log(n)
\end{equation}
where $\ell(\hat{\T}|\x)$ is the maximized log likelihood value, $p$ is the number of explanatory variables in the model (including 
the constant), and $n$ is the sample size.  This is very similar to the earlier ``consistent'' AIC of Bozdogan (1987): 
$CAIC = -2\ell(\hat{\T}|\x) + p(1 + \log(n))$. To compare two models with the BIC, create a Schwarz criterion difference statistic 
according to:\index{subjectindex}{Schwarz criterion!difference statistic}\index{authorindex}{Bozdogan, H.}
\begin{equation}
  S = \text{BIC}_{\text{model 1}} - \text{BIC}_{\text{model 2}} = \ell(\hat{\T}_1|\x) -
                                      \ell(\hat{\T}_2|\x) -
                                      \frac{1}{2}(p_1 - p_2)\log(n)
\end{equation}
where the subscripts indicate the model source.  This is extremely easy to calculate and has the following asymptotic property:
\begin{equation}
  \frac{S - \log(BF_{(1,2)})}{\log(BF_{(1,2)})} \xrightarrow[n \to \infty]{} 0.
  \label{BIC.convergence}
\end{equation}
Unfortunately $S$ is only a rough approximation to $\log(BF_{(1,2)})$ since the relative error of this approximation is $\circ(1)$
(although this can be improved for very specific models, e.g., Kass\index{authorindex}{Kass, R. E.} and
Wasserman\index{authorindex}{Wasserman, L.} (1995) for an example using normal priors), and typically large samples are required.
In fact, the log form in \eqref{BIC.convergence} is essential since $\frac{\exp[S]}{BF_{(1,2)}} \nrightarrow 1$ as $n$ goes to
infinity (Kass\index{authorindex}{Kass, R. E.} and Wasserman \index{authorindex}{Wasserman, L.} 1995, p.928).  Nonetheless,
Kass\index{authorindex}{Kass, R. E.} and Raftery \index{authorindex}{Raftery, A. E.} (1995) argue that it is often possible to
use the approximation: $S \approx -2\log(BF_{(1,2)})$.  Robert\index{authorindex}{Robert, C. P.} (2001, p.353) is critical of the
use of the BIC\index{subjectindex}{Bayesian information criterion (BIC)} to estimate Bayes Factors because it removes the effect
of any specified priors, it is difficult or impossible to calculate in complex models with non-iid data, and maximum likelihood
estimations are required for every model compared and therefore might not be as much of a shortcut as intended.
\index{subjectindex}{iid}

The BIC should be used with caution under certain circumstances.  First, the BIC can be inconsistent as the number of parameters 
gets very large (Stone 1977a 1977b).\index{authorindex}{Stone, M.} Models that are this large (thousands of parameters) are rare in 
the social sciences, but they do occur in other areas such as statistical genetics.  Berger \etal (2003) follow-up on Stone's
brief comment with a more detailed exposition of how  the BIC can be a poor approximation to the logarithm of Bayes Factor
using the Stone's ANOVA example and suggest the alternative forms: Generalized Bayes Information Criterion (GBIC) and a Laplace 
approximation to the logarithm of the  Bayes Factor.  Chakrabarti and Ghosh (2006) prove some of the properties of these alternative
forms.  

A commonly used measure of goodness-of-fit outside of Bayesian work is the Akaike information criterion 
(Akaike\index{authorindex}{Akaike, H.}, 1973, 1974, 1976).  The idea is to select a model that minimizes the negative likelihood 
penalized by the number of parameters: \index{subjectindex}{Akaike information criterion (AIC)}
\begin{equation}
        \text{AIC} = -2\ell(\hat{\T}|\x) +2p,
	\label{AIC.definition}
\end{equation}
where $\ell(\hat{\T}|\x)$ is the maximized model log likelihood value and $p$ is the number of explanatory variables in the model
(including the constant).  The AIC is very useful in comparing and selecting non-nested model specifications, but many authors have
noted that the AIC has a strong bias toward models that overfit with extra parameters since the penalty component is obviously
linear with increases in the number of explanatory variables, and the log likelihood often increases more rapidly
\index{subjectindex}{bias!model specification} (Carlin and Louis 2009, p.53; Neft\c{c}i 1982, p.539; Sawa 1978, p.1280).
\index{authorindex}{Carlin, B. P.} \index{authorindex}{Louis, T. A.} \index{authorindex}{Neft\c{c}i, S. N.}
\index{authorindex}{Sawa, T.}%

Using the value of the log likelihood alone without some overfitting penalization as a measure of model fit is a poor strategy since the 
likelihood never decreases by adding more explanatory variables regardless of their inferential quality.  See also the lesser known 
alternatives: MIC (Murata, Yoshizawa, and Amari 1994) and TIC (Takeuchi 1976), NIC (Stone 1974), and the explicitly Bayesian EAIC (Brooks 
2002).  Burnham and Anderson (2002) argue in their book that when the data size is small relative to the number of specified parameters, 
one should use a modified AIC according to:
\begin{equation}
    AIC_c = AIC + 2p(p+1)/(n-p-1),
\end{equation}
which add a further penalty.
\index{authorindex}{Murata, N.} \index{authorindex}{Yoshizawa, S.} \index{authorindex}{Amari, S.}   \index{authorindex}{Takeuchi, K.}
\index{authorindex}{Stone, M.} \index{authorindex}{Brooks, S. P.} \index{authorindex}{Burnham, K. P.} \index{authorindex}{Anderson, D. R.}
\index{authorindex}{Spiegelhalter, D. J.} \index{authorindex}{Best, N. G.} \index{authorindex}{Carlin, B. P.} 
\index{authorindex}{van der Linde, A.} 

Both the AIC and the BIC have a problem with a widened definition of parameters. Suppose we have panel data with $n$ individuals
each measured over $t$ time-points. If individual-specific effects are drawn from a single random effects term (i.e., given a
distributional assignment), then group-level variables from this specification play different roles. Such models are very common
Bayesian specifications and are the subject of Chapter~\ref{Hierarchical.Chapter}. In this simple case should this latent variable
be counted as one parameter or $n$ parameters?  If it counts as $n$ parameters, then the effect of shrinkage is ignored as well,
substantially inflating the value of the AIC and BIC.  On the other hand, if it counts as one parameter it appears that we are
intentionally reducing the penalty component to advantage the model relative to one without such a random effect. We now turn to a
tool specifically designed to handle this problem.

\section{The Deviance Information Criterion (DIC)}\index{subjectindex}{Deviance Information Criterion (DIC)|(}\label{DIC.Section}
A very useful tool for model assessment and model comparison is the deviance information criterion created by
Spiegelhalter, Best, Carlin, and van der Linde (2002), although earlier, more narrow versions exist.  The DIC has already
become a popular model comparison choice since it is integrated into the \winbugs\ package for MCMC estimation (see
Chapter~\ref{MCMC.Chapter}).  There are two objectives here: describing ``model complexity'' and model fit as in the AIC or BIC.
For instance, in the BIC the term $-2\ell(\hat{\T}|\x)$ is thought of as a describer of fit, whereas $p\log(n)$ shows complexity
in the form of parameter vector size in the model.  Suppose we have a model under consideration, defined by $p(\y|\T)$ for data
$\y$ and parameter vector $\T$.  The first quantity to define is the ``Bayesian deviance'' specified by Spiegelhalter \etal
as:\index{authorindex}{Spiegelhalter, D. J.}
\index{subjectindex}{Deviance Information Criterion (DIC)!standardizing factor}  
\begin{equation}
	D(\T) = -2\log[p(\y|\T)] + 2\log[f(\y)],
	\label{Deviance.definition}
\end{equation}
where $f(\y)$ is some function of just the data, the ``standardizing factor.'' Spiegelhalter\index{authorindex}{Spiegelhalter, D. J.} 
\etal (2002) de-emphasize $f(\y)$ for model comparison and even suggest using $f(\y)=1$ (giving zero contribution above) since
this term must be identical for both model calculations and therefore cancels out.  Note the similarity to the AIC and BIC, except
that there is no $p$ term in the second part.  We can use \eqref{Deviance.definition} in explicitly posterior terms by inserting a
condition on the data and taking an expectation over $\T$:
\begin{equation}
	\overline{D(\T)} = E_\T[-2\log[p(\y|\T)|\y]] + 2\log[f(\y)],
	\label{Posterior.mean.Deviance.definition}
\end{equation}
(the authors switch notation between $\overline{D(\T)}$ and $\bar{D}$).  This posterior mean difference is now a measure of
Bayesian model fit.  Now define $\tilde{\T}$ as a posterior estimate of $\T$, which can be the posterior mean or some other easily
produced value (although Spiegelhalter \etal note that obvious alternatives such as the median may produce problems).  Observe
that we can also insert $\tilde{\T}$ into \eqref{Deviance.definition}.  The \emph{effective dimension} of the model is now defined
by: \index{subjectindex}{Deviance Information Criterion (DIC)!effective dimension}
\begin{equation}
	p_D = \overline{D(\T)} - D(\tilde{\T}).
	\label{effective.dimension.definition}
\end{equation}
This is the ``mean deviance minus the deviance of the means.''  Another way to think about effective dimensions is by counting the roles 
that parameters take on in Bayesian models.  In this way, $p_D$ is the sum of the parameters each of which is weighted according to: 
$\omega_p = 1$ for parameters unconstrained by prior information, $\omega=0$ for parameters completely specified (fixed) by prior 
information, and $\omega \in [0\range 1]$ for parameters with specific dependencies on the data or priors.

Also, due to the subtraction, $p_D$ is independent of our choice of $f(\y)$.  This is illustrated by expanding 
\eqref{effective.dimension.definition} and applying Bayes' Law:
\begin{align}
    p_D &= \overline{D(\T)} - D(\tilde{\T}) \nonumber\9
	    &= \left\{ E_{\T|\y}[-2\log[p(\y|\T)|\y] + 2\log[f(\y)] \right\} 
	     - \left\{ -2\log[p(\y|\tilde{\T})] + 2\log[f(\y)] \right\}			\nonumber\9
	    &= E_{\T|\y}[-2\log(p(\y|\T))]  + 2\log(p(\y|\tilde{\T})) 			\nonumber\9
	    &= E_{\T|\y}\left[ -2\log\left( \frac{p(\T|\y)p(\y)}{p(\T)} \right) \right]  
	     + 2\log\left( \frac{p(\tilde{\T}|\y)p(\y)}{p(\tilde{\T})} \right) 		\nonumber\9
	    &= E_{\T|\y}[ -2\log( p(\T|\y)/p(\T) ) ]  
	     +             2\log( p(\tilde{\T}|\y)/p(\tilde{\T}) ).  
\end{align}
We can think of the ratio $p(\T|\y)/p(\T)$ here as the gain in information provided by conditioning on the data in the model, and 
correspondingly $p(\tilde{\T}|\y)/p(\tilde{\T})$ as the gain in information after plugging in the chosen estimate.

In a simulation context (subsequent chapters), $p_D$ can be computed as the mean of simulated values of $D(\T)$ minus $D(\T)$ plugging the 
mean of the simulated values of $\T$, hence the interpretation as the difference between the posterior mean of the deviance and the 
deviance at the poster mean (or some other chosen statistic).  So while $\overline{D(\T)}$ is a Bayesian measure of model fit, $p_D$ is 
designed to be a ``complexity measure'' for the effective number of parameters in the model.  As such it is the Bayesian analogy to the 
second term in the AIC.

The Deviance Information Criterion is created by adding an additional model fit term to the effective dimension:
\index{subjectindex}{Deviance Information Criterion (DIC)!definition}
\begin{equation}
	DIC = \overline{D(\T)} + p_D = 2\overline{D(\T)} - D(\tilde{\T})
\end{equation}
and is thus the Bayesian measure of model fit above with an extra complexity penalization.  The logic here goes back to a common criticism 
of the AIC that using a plug-in value rather than integrating out unknown parameters leads to insufficient incentive for parsimonious models.  
The DIC also uses a plug-in value ($\tilde{\T}$), but incorporates an additional penalty to compensate.  Some authors still prefer an adapted 
version of the AIC, such as Brooks (2002)\index{authorindex}{Brooks, S. P.} who suggests the \emph{expected Akaike information criterion}: 
$EAIC = \overline{D(\T)} + 2p$, and $D(\bar{\T}) + 2p$.

\begin{examplelist}
    \item   {\bf Hierarchical Models of Rural Migration in Thailand.}	\index{subjectindex}{data!migration in Thailand}
            Garip and Western (2011, 2005 results shown here) \index{authorindex}{Western, B.} \index{authorindex}{Garip, F.} use
            the example of village-level migration to contrast hierarchical model specifications for a dichotomous
            outcome.\footnote{Note: this example is based on an earlier version of the paper which had more starkly contrasting
            $p_D$ and DIC values.} They look at survey data on young adults (18-25) in 22 Northeastern Thai villages where the
            outcome variable is 1 if the  respondent spent at least two months away from the village in 1990 and 0 otherwise.  The
            individual-level explanatory variables are sex, age, years of education, number of prior trips, and the village-level
            explanatory variables are prior trips from the villages and the Gini index of prior trips.

        \begin{table}[h]
        \parbox[c]{\linewidth}{
        \hspace{18pt}
        \tabletitle{\textsc{Available Model Specifications, Thai Migration}}\label{Thai.Specification.Table}
        \vspace{4pt}
        \renewcommand{\arraystretch}{1.2}
	    \begin{center}
            \hspace{-33pt}
		    \begin{tabular}{lll}
		    & & \7
		    Model		     & Specification						    
				        & Prior Parameters	\\
		    \hline
		    & & \7
		    \emph{Pooled} 	     & $\logit(p_{ij}) = \alpha + x_{ij}'\beta + z_j'\gamma$	    
				        & $\alpha\sim \mathcal{N}(0,10^6)$ \\
				        &							    
				        & $\beta[1:4] \sim \mathcal{N}(0,10^6)$ \\
                               	        &                                                           
				        & $\gamma[1:2] \sim \mathcal{N}(0,10^6)$ \9
		    \emph{Fixed Effect}  & $\logit(p_{ij}) = \alpha_j + x_{ij}'\beta$		    
				        & $\alpha[1:22] \sim \mathcal{N}(0,10^6)$ \\
				        &							    
				        & $\beta[1:4] \sim \mathcal{N}(0,10^6)$ \9
		    \emph{Random Effect} & $\logit(p_{ij}) = \alpha_j + x_{ij}'\beta + z_j'\gamma$   
				        & $\mu = 0$\\
				        & $\alpha[1:22] \sim \mathcal{N}(\mu,\tau^2)$ 	            
			 	        & $\tau \sim \mathcal{IG}(10^{-3},10^{-3})$ \\
				        &							    
			 	        & $\beta[1:4] \sim \mathcal{N}(0,10^6)$ \\
                                        &                                                           
			 	        & $\gamma[1:2] \sim \mathcal{N}(0,10^6)$ \9
		    \emph{Random}        & $\logit(p_{ij}) = \alpha_j + x_{ij}'\beta_j + z_j'\gamma$ 
			 	        & $\mu_\alpha \sim \mathcal{N}(0,10^6)$ \\
		    \emph{Intercept and} & $\alpha[1:22] \sim \mathcal{N}(\mu_\alpha,\tau_\alpha^2)$ 
			 	        & $\tau_\alpha^2\sim\mathcal{IG}(10^{-3},10^{-3})$ \\
		    \emph{Random Slope}  & $\beta[1:22][1:4]\sim \mathcal{N}(\mu_\beta,\tau_\beta^2)$
			 	        & $\mu_\beta \sim \mathcal{N}(0,10^6)$ \\
				        &							    
			 	        & $\tau_\beta^2 \sim\mathcal{IG}(10^{-3},10^{-3})$ \\ 
                                        &                                                           
			 	        & $\gamma[1:2] \sim \mathcal{N}(0,10^6)$ \9
		    \end{tabular}
		    \end{center}
            }
		    \end{table}

            Methodological interest here centers on the utility of specifying hierarchies in the model.  These are useful tools
            for recognizing different levels that the data affect some outcome.   For classical non-Bayesian references, see Bryk
              and Raudenbush (2001) \index{authorindex}{Bryk, A. S.} \index{authorindex}{Raudenbush, S. W.} or Goldstein (1985).
              \index{authorindex}{Goldstein, H.} We will study Bayesian hierarchical models in considerable detail in
              Chapter~\ref{Hierarchical.Chapter}, and for the moment an excellent source for background reading is Good (1980a).
              \index{authorindex}{Good, I. J.} There are two data matrices here with rows: $x_{ij}$ for individual $i$ in village
              $j$, and $z_j$ for village $j$.  The four contrasting models for the probability of migration, $p_{ij}$,  given by
              Western and Garip are given in Table~\ref{Thai.Specification.Table} 
            where $\alpha$ is a common intercept, $\alpha_j$ is a village-specific intercept, and $[\beta,\gamma]$ is the unknown
            parameter vector to be described with a posterior distribution.  There are many contrasting features in these models
            having to do with the relative effects in the data and the role we would like these effects to play in the model.
            Suppose we simply wanted to test the relative quality of the models with the DIC, ignoring for now any substantive
            issues with different hierarchies.
              \index{authorindex}{Western, B.} \index{authorindex}{Garip, F.}  

		Despite the notable structural differences in these models, both the individual-level and village-level
		coefficients are remarkably similar and do not present any substantively different stories whatsoever
		(except of course that the fixed effects model does not have village-level coefficients).  It is a modest
		difference, but the posterior distributions for the random intercept and random slope/random intercept
		are more diffuse than for the pooled model, reflecting the random effects modeling heterogeneity.

		\begin{table}[h]
        \parbox[c]{\linewidth}{
            \hspace{18pt}
		    \tabletitle{\textsc{Model Comparison, Thai Migration}}\label{Thai.Table}
		    \vspace{4pt}
		    \renewcommand{\arraystretch}{1.2}
		    \begin{tabular}{lcccc}
					    &		& \emph{Fixed}	& \emph{Random}	& \emph{Rdm. Slope} \\[-5pt]
		    Model:			& \emph{Pooled}	& \emph{Effect}	& \emph{Effect}	& \emph{\& Intercept}	\\
		    \hline
		          		    &             	&             	&             	&              \\[-7pt]
		    \texttt{Intercept} 		    & -0.80 (0.12)	& -0.66 (0.10)	& -0.79 (0.13)	& -0.76 (0.14) \9
		    \texttt{Male} 			    & ~0.32 (0.14)	& ~0.38 (0.14)	& ~0.33 (0.14)	& ~0.30 (0.15) \9
		    \texttt{Age} 			    & -0.13 (0.08)	& -0.15 (0.08)	& -0.13 (0.08)	& -0.13 (0.09) \9
		    \texttt{Education}          & ~0.39 (0.07)	& ~0.38 (0.08)	& ~0.39 (0.07)	& ~0.39 (0.08) \9
		    \texttt{Prior trips}($i$)	& ~1.08 (0.09)	& ~1.13 (0.10)	& ~1.08 (0.09)	& ~1.22 (0.16) \9
		    \texttt{Prior trips}($j$) 	& -0.61 (0.37)	&    		    & -0.59 (0.43)	& -0.69 (0.46) \9
		    \texttt{Gini trips}($j$) 	& -0.62 (0.21)	&    		    & -0.60 (0.24)	& -0.61 (0.25) \9
		    \hline
		    \multicolumn{5}{c}{Posterior means (posterior standard deviations in parentheses)}\\
		    \hline
		 		            &             	&             	&             	&              \7
		    $p_D$ 			& ~~~~7.02 	& ~~~26.24 	& ~~~10.93 	& ~~~30.58 \\
		    DIC 			& 1259.41 	& 1273.29 	& 1259.62 	& 1247.55 \\
		    \end{tabular} 
        }
		\end{table}

		\index{subjectindex}{Deviance Information Criterion (DIC)!example}
		We see from the $p_D = 7.02$ value (Table~\ref{Thai.Table}) that the pooled model is the most parsimonious, 
		with the extra $0.02$
		above the 7 specified parameters coming from prior information.  Interestingly, even though it contains
		$7+22$ parameters, the random intercept model is only slightly less parsimonious with $p_D=10.93$.
		Not surprisingly, the random slope/random intercept model is the least parsimonious with 110 random
		effects and 2 village-level parameters specified, which is only moderately less parsimonious than the
		random effect model with 26 parameters, justifying the hierarchical structure imposed.  Furthermore,
		with the lowest DIC value, we find additional support for the more complex structure.	
\end{examplelist}

\subsection{Some Qualifications}
Some concerns emerge with the use of the DIC (the Spiegelhalter \etal (2002) paper is accompanied by 23 pages of discussion).
First, its lack of invariance to reparameterizations of the parameters where the subsequent differences can be large, Therefore,
we should be cautious in interpreting large DIC changes that come from reparameterizations only.  Secondly, there is evidence that
DIC comparisons are most straightforward when the form of the likelihoods (focus) are the same and only the selected explanatory
variables differ.  Otherwise it is essential to make the standardizing factor the same.  Likelihood functions from hierarchical
models with different level structures provide difficult interpretational problems (see the example above).  In fact, the DIC is
strongly justified only for likelihood functions based on exponential family forms (Appendix~\eqref{GLM.Chapter}).  Helpfully,
Celeux \etal (2006)\index{authorindex}{Celeux, G.} survey strengths and weakness for the DIC beyond applications to exponential
families.  When the posterior distributions are non-symmetric or multimodal, the use of the posterior mean may be inappropriate.
It is possible for $p_D$ to be negative, so DIC can also be negative. Clearly this provided interpretational problems.  The DIC is
not a function of the marginal likelihood, therefore it is not related to Bayes Factor comparisons.  The DIC does not work when
the likelihood depends on discrete parameters, so it does not work with mixture likelihoods (\winbugs\ will ``gray-out'' the DIC
button automatically).  Finally, specification of the DIC implies use of the data twice: once to produce the posterior and once
again with $p(\y|\T)$ in the expectation.  The two objections are: philosophical problems with violating canonical tenets of
Bayesian inference, and practical problems with the increased tendency to overfit the data.  

\section{Comparing Posterior Distributions with the\\ Kullback-Leibler Distance}\label{section.with.KL}
There are many situations where it is convenient to compare distributions, posteriors in particular.  The Bayes Factor
(Chapter~\ref{Testing.Chapter}), in particular, provides a mechanical way to inferentially compare two model results.  A different and
more general method for evaluating the difference between two distributions is the \emph{Kullback-Leibler distance} (sometimes called the
\emph{entropy distance}), which is given by: \index{subjectindex}{Kullback-Leibler distance}
\begin{equation}\label{kullback.leibler.equation}
    I(f,g) = \int \log\left[\frac{f(x)}{g(x)}\right]f(x)dx,
\end{equation}
for two candidate distributions $f(x)$ and $g(x)$ (Robert\index{authorindex}{Robert, C. P.} and 
Casella\index{authorindex}{Casella, G.} 1999, p.222, White\index{authorindex}{White, H.} 1996, p.9).
This is the expected log-ratio of the two densities with the expectation taken relative to one selected
distribution ($f(x)$ here).  Despite the simple form of \eqref{kullback.leibler.equation}, it can 
occasionally be difficult to calculate analytically.  If $f(x)$ and $g(x)$ are exponential family distributions,
the resulting cancellations can considerably simplify the integral
(McCulloch\index{authorindex}{McCulloch, R. E.} and
Rossi\index{authorindex}{Rossi, P. E.} 1992).
For instance, suppose we are interested in comparing the Kullback-Leibler distance between two proposed
univariate Poisson distributions indexed by intensity parameters
$\lambda_1$ and $\lambda_2$.  The form of $I(f,g)$ is given by:
\begin{align} 
    I(f,g) &= \sum_{\mathcal{X}} \log \left[ \frac{ e^{-\lambda_1}\lambda_1^\x/\x! }{ e^{-\lambda_2}\lambda_2^\x/\x! } \right]
          \frac{ e^{-\lambda_1}\lambda_1^\x }{ \x! }                                         \nonumber \9
           &= \sum_{\mathcal{X}} \left[ (\lambda_2 - \lambda_1) + \x\log\left(\frac{\lambda_1}{\lambda_2}\right)\right]
          \frac{ e^{-\lambda_1}\lambda_1^\x }{ \x! }                                         \nonumber \9
           &= \lambda_2 - \lambda_1 + \lambda_1 \log\left(\frac{\lambda_1}{\lambda_2}\right).\label{KB.poisson}
\end{align}
However, the Kullback-Leibler calculations are rarely this direct.  Furthermore, the Kullback-Leibler distance
is best used only comparatively among a set of alternative specifications since it is not a true Cartesian
distance, the scale is still arbitrary, and it is not symmetric: $ I(f,g) \ne I(g,f)$.  Nonetheless, this is
an excellent way to measure the closeness of a set of posteriors.

General details about using the Kullback-Leibler distance can be found in Kullback\index{authorindex}{Kullback, S.} (1968) and
Brown\index{authorindex}{Brown, L. D.} (1986), and hypothesis testing applications are given by
Goel\index{authorindex}{Goel, P. K.} and\\
DeGroot\index{authorindex}{DeGroot, M. H.} (1979),
Janssen\index{authorindex}{Janssen, A.}
(1986), Ebrahimi, Habibullah, and Soofi (1992), and Robinson (1991).  The relationship to the
        \index{authorindex}{Ebrahimi, N.}%
        \index{authorindex}{Habibullah, M.}%
        \index{authorindex}{Soofi, E. S.}%
        \index{authorindex}{Robinson, P. M.}%
AIC\index{subjectindex}{Akaike information criterion (AIC)} model fitting criteria (Chapter~\ref{Testing.Chapter})
is explored by Hurvich and Tsai
(1991) and Hurvich, Shumway, and Tsai (1990).  Efron (1978) uses the ``curve of constant Kullback-Leibler
        \index{authorindex}{Hurvich, C. M.}%
        \index{authorindex}{Tsai, C-L.}%
        \index{authorindex}{Shumway, R.}%
        \index{authorindex}{Efron, B.}%
information'' in relating the natural parameter space of coefficient with its model-induced
expected parameter space, and several authors, such as Hernandez and Johnson (1980), Johnson
        \index{authorindex}{Hernandez, F.}%
        \index{authorindex}{Johnson, R. A.}%
        \index{authorindex}{Sakia, R. M.}%
(1984), and Sakia (1992), use Kullback-Leibler information to estimate the parameter of the popular
Box-Cox transformation.  There is also a wealth of studies using Kullback-Leibler information
to make direct model comparisons, such as Leamer (1979), Sawa (1978), Vuong (1989), and White
        \index{authorindex}{Leamer, E. E.}%
        \index{authorindex}{Sawa, T.}%
        \index{authorindex}{Vuong, Q. H.}%
        \index{authorindex}{White, H.}%
(1996, Chapter 9).  We will return to this technique in Section~\ref{kullback.leibler.example} with a numerical example.

    \begin{center}
    \begin{table}[h]
    \parbox[c]{\linewidth}{
    \tabletitle{\textsc{Martikainen \etal (2005) Mortality Parameters}}\label{relative.mortality.table1}
    \begin{tabular}{l|rrrr}
                & \texttt{Married}   & \texttt{Never Married} & \texttt{Divorced}  & \texttt{Widowed}   \\
        \hline
        \texttt{Men}     & 1.00      & 1.84          & 2.08      & 1.51      \\
        \texttt{Women}   & 1.00      & 1.59          & 1.62      & 1.28      \\
    \end{tabular}
    }
    \end{table}
    \end{center}

\begin{examplelist}
    \item   \label{example:relative.mortality} {\bf Example: Models of Relative Mortality}
            \index{authorindex}{Martikainen, P.}
            \index{authorindex}{Martelin, T.}
            \index{authorindex}{Nihtila, E.@Nihtil\"{a}, E.}
            \index{authorindex}{Majamaa, K.}
            \index{authorindex}{Koskinen, S.}
    Martikainen \etal (2005) seek to explain why the non-married population has a higher mortality rate compared to the
    married population, an effect seen in many countries.  Their supposition is that the health-related behavior of 
    non-married individuals contributes to their higher rates.  Part of this study uses data from 1996 to 2000 in Finland
    for individuals aged 30-64 years.  Their models produce mortality rates relative to the married populations where
    the values are interpreted as Poisson intensity parameters.  Using their ``full model'' we see the relative intensity 
    parameter estimates in Table~\ref{relative.mortality.table1}.

    These values can be evaluated as single point estimates, as the original authors do, but actually they are
    describing distributional differences.  Given the asymmetry of the Poisson distribution, simple numerical
    comparison of the estimated intensity parameters may not give a complete summary.  So using \eqref{KB.poisson}
    we can summarize distributional differences, as given in Table~\ref{relative.mortality.table1}.
    \begin{comment}
     men <- c(1.00,1.84,2.08,1.51)    
     women <- c(1.00,1.59,1.62,1.28)
     kb.diff <- function(lambda1,lambda2) return(lambda2-lambda1+lambda1*log(lambda1/lambda2))
     kb.diff(men[1],men[2]); kb.diff(men[1],men[3]); kb.diff(men[1],men[4])
                             kb.diff(men[2],men[3]); kb.diff(men[2],men[4])
                                                     kb.diff(men[3],men[4])
     kb.diff(women[1],women[2]); kb.diff(women[1],women[3]); kb.diff(women[1],women[4])
                                 kb.diff(women[2],women[3]); kb.diff(women[2],women[4])
                                                             kb.diff(women[3],women[4])
    \end{comment}

    \begin{table}[h]
    \parbox[c]{\linewidth}{\hspace{-0.4in}
    \tabletitle{\textsc{Martikainen \etal (2005) Mortality Differences}}\label{relative.mortality.table2}
    \begin{tabular}{l|rrr}
        \texttt{Men}      & \texttt{Never Married} & \texttt{Divorced}  & \texttt{Widowed} \\
        \hline
        \texttt{Married}         & 0.23023       & 0.34763   & 0.09789   \\
        \texttt{Never Married}   &               & 0.01441   & 0.03369   \\
        \texttt{Divorced}        &               &           & 0.09614   \\
        \hline
        \texttt{Women}    & \texttt{Never Married} & \texttt{Divorced}  & \texttt{Widowed} \\
        \hline
        \texttt{Married}         & 0.12627       & 0.13757   & 0.03314   \\
        \texttt{Never Married}   &               & 0.00028   & 0.03483   \\
        \texttt{Divorced}        &               &           & 0.04162   \\
    \end{tabular}
    }
    \end{table}

        The results in Table~\ref{relative.mortality.table2} reinforce the main findings that men fare worse outside of marriage
        than women, which supports the authors' hypothesis about behavioral factors: accidents from risky ventures, violence, and
        alcohol.  The Kullback-Leibler distributional differences highlight more directly comparisons between the non-marriage
        categories.  For example the distribution difference between \texttt{Never Married} and \texttt{Divorced} is much greater
        for \texttt{Men} than for \texttt{Women} (roughly two orders of magnitude).  Since these are all relative measures, we can
        assert that there is virtually no distributional distance between \texttt{Divorced} and \texttt{Never Married} for
        \texttt{Women} (notice that $0.00028$ is much smaller than $0.13757-0.12627 - 0.0113$).  
\end{examplelist}
                
\section{Laplace Approximation of Bayesian Posterior\\ Densities}\label{laplace.approximation.section}
Laplace's\index{authorindex}{Laplace, P. S.} method\index{subjectindex}{Laplace approximation!posterior densities} (1774) is a well-known 
method for approximating the shape of marginal posterior densities that is very useful in the Bayesian context when direct calculations are 
difficult.  The now standard reference to approximating Bayesian posterior densities with Laplace's method is Tierney
\index{authorindex}{Tierney, L.} and Kadane\index{authorindex}{Kadane, J. B.} (1986), and theoretical details on the accuracy of the 
approximation can be found in Wong\index{authorindex}{Wong, W. H.} and Li\index{authorindex}{Li, B.} (1992) and Kass
\index{authorindex}{Kass, R. E.} and Vaidyanathan\index{authorindex}{Vaidyanathan, S. K.} (1992).  Kass\index{authorindex}{Kass, R. E.} 
(1993) shows how the Laplace approximation\index{subjectindex}{Laplace approximation} can be handy for calculating Bayes Factors.  In 
general, the Laplace approximation is a very useful tool to have at one's disposal when a normal approximation posterior is reasonable, and 
can be especially useful in higher dimensions when other procedures fail.

The basic idea is to carry out a Taylor series expansion\index{subjectindex}{Taylor series expansion} around the maximum likelihood estimate 
value, ignore the negligible terms, and normalize.  For simplicity we will derive the approximation in only one dimension, but the 
generalization is obvious.  Start with a posterior density of interest calculated by the likelihood times the specified prior: 
$\pi(\theta|\x) \propto L(\x|\theta)p(\theta)$, and assume that this distributional form is nonnegative, integrable, and single-peaked about 
a mode, $\hat{\theta}$, which is a reasonable set of assumptions provided that sample sizes are not 
	small.\footnote{Kass\index{authorindex}{Kass, R. E.} and Raftery\index{authorindex}{Raftery, A. E.}
    	(1995) recommend that for $k$ explanatory variables one should ideally have a sample 
	size of at least $20k$, and that less than $5k$ is ``worrisome''  (naturally such 
	guidelines depend on the behavior of the likelihood function in general and 
	substantial deviations from normality will obviously require even greater sample 
	sizes).  Carlin\index{authorindex}{Carlin, B. P.} and 
	Louis\index{authorindex}{Louis, T. A.} (2009, p.110) 
	note that the Laplace approximation is therefore rarely helpful for higher 
	dimensional problems such as those with more than ten explanatory variables.}

The posterior expectation of some smooth function of $\theta$, $g(\theta|\x)$ (such as a mean, variance, or other desired summary quantity) 
is given by:
\begin{equation}\label{posterior.expectation.for.laplace}
    E[g(\theta|\x)] = \int_{\theta} g(\theta|\x)L(\x|\theta)p(\theta) d\theta = \int_{\theta} g(\theta|\x)\pi(\theta|\x) d\theta.
\end{equation}
Using Bayes' Law we know that:
\begin{equation}\label{bayes.law.for.laplace}
    \pi(\theta|\x) = \frac{L(\x|\theta)p(\theta)}{\int_{\theta} L(\x|\theta)p(\theta)d\theta}.
\end{equation}
So \eqref{posterior.expectation.for.laplace} becomes:
\begin{equation}\label{extended.posterior.expectation.for.laplace}
        E[g(\theta|\x)] = \int_{\theta} g(\theta|\x)
                            \frac{L(\x|\theta)p(\theta)}{\int_{\theta} L(\x|\theta)p(\theta)d\theta} d\theta
                        = \frac{\int_{\theta} g(\theta|\x)L(\x|\theta)p(\theta)d\theta}
                            {\int_{\theta} L(\x|\theta)p(\theta)d\theta}.
\end{equation}
Now define the quantities:
\begin{align}\label{h.defs.for.laplace}
    -nh_1(\theta) &= \log g(\theta|\x) + \log L(\x|\theta) + \log p(\theta)
        \nonumber \\
    -nh_2(\theta) &= \log L(\x|\theta) + \log p(\theta).
\end{align}
Substituting these values into \eqref{extended.posterior.expectation.for.laplace} produces:
\begin{equation}\label{substituted.posterior.expectation.for.laplace}
        E[g(\theta|\x)] = \frac{\int \exp[-nh_1(\theta)]d\theta}
                                       {\int \exp[-nh_2(\theta)]d\theta}.
\end{equation}
If we have a mathematically convenient approximation for either $h_1(\theta)$ or $h_2(\theta)$, then $E[g(\theta|\x)]$ becomes very easy 
to calculate.  Consider a Taylor series expansion \index{subjectindex}{Taylor series expansion} around some arbitrary point $\theta_0$ for 
$h(\theta)$ where this generic identification applies to either $h_1(\theta)$ or $h_2(\theta)$:
\begin{align}
    h(\theta) = h(\theta_0) +        (\theta - \theta_0)h'(\theta_0)
                      &+ \frac{1}{2!}(\theta - \theta_0)^2h''(\theta_0)  \nonumber \\
                      &+ \frac{1}{3!}(\theta - \theta_0)^3h'''(\theta_0)
                      + R_n(\theta),
\end{align}
where $R_n(\theta)$ is the remainder consisting of progressively smaller terms in the expansion.  In fact, since 
$\lim_{\theta \to \theta_0}R_n(\theta)/(\theta-\theta_0)^3 = 0$ (Robert and Casella 2004), then we can safely ignore higher order terms and 
just use: $h_1(\theta)$ or $h_2(\theta)$:
\begin{equation}\label{approx.h.for.laplace}
        h(\theta) \approx h(\theta_0) +             (\theta - \theta_0)  h'(\theta_0)
                                      + \frac{1}{2!}(\theta - \theta_0)^2h''(\theta_0)
                                      + \frac{1}{3!}(\theta - \theta_0)^3h'''(\theta_0).
\end{equation}
Recall that we are most interested in the modal point of the posterior density, so pick $\theta_0$ to be the point where the first derivative 
vanishes: $\hat{\theta}$.\index{subjectindex}{posterior distribution!mode}  Substituting $\hat{\theta}$ into \eqref{approx.h.for.laplace} 
eliminates the linear term: $(\theta - \hat{\theta})h'(\hat{\theta})$.  If we ignore the rapidly vanishing cubic term, 
$\frac{1}{3!}(\theta - \hat{\theta})^3f'''(\hat{\theta})$, then this is said to be a first-order approximation.  In the cases where more 
accuracy is required, this term can be expanded in a second Taylor series around $\hat{\theta}$ to produce second- and third-order accuracy.  
Often the sample size is sufficiently large so that first-order accuracy is sufficient (see Robert\index{authorindex}{Robert, C. P.} and 
Casella\index{authorindex}{Casella, G.} [2004, p.109] for details on this second-order expansion). 

Take the now reduced form in \eqref{approx.h.for.laplace} and make the substitution $\hat{\sigma}^2 = (h''(\theta_0))^{-1}$ to derive:
\begin{align}
    \int_{\theta} \exp[&-nh(\theta)]d\theta
           \nonumber \\
        &= \int_{\theta} \exp \left[ -nh(\hat{\theta})
                                      -\frac{n}{2\hat{\sigma}^2}(\theta-\hat{\theta})^2 \right] d\theta,
           \nonumber \\
        &= \exp[-nh(\hat{\theta})]
               \int_{\theta} \exp\left[ -\frac{ n }{ 2\hat{\sigma}^2 }(\theta-\hat{\theta})^2 \right] d\theta,
           \nonumber \\
        &= \exp[-nh(\hat{\theta})](\sqrt{2\pi\hat{\sigma}^2} n^{-\frac{1}{2}})
               \int_{\theta} \frac{ n^{\frac{1}{2}} }{\sqrt{2\pi\hat{\sigma}^2}}
           \exp\left[ -\frac{n}{2\hat{\sigma}^2}(\theta-\hat{\theta})^2 \right] d\theta,
           \nonumber \\
        &= \exp[-nh(\hat{\theta})](\sqrt{2\pi\hat{\sigma}^2} n^{-\frac{1}{2}}),
\end{align}
using the standard trick of pushing terms into the integral so that it equals one.  This is precisely the justification for a normal 
approximation\index{subjectindex}{posterior distribution!normal approximation} in \eqref{extended.posterior.expectation.for.laplace}.  
The resulting approximation for $E[g(\theta|\x)]$ is:
\begin{align}\label{tierney.kadane.form}
    \hat{E}[g(\theta|\x)]
            &= \frac{ \int_{\theta} \exp[-nh_1(\theta)]d\theta }
                { \int_{\theta} \exp[-nh_2(\theta)]d\theta }
            \nonumber \\[9pt]
        &= \frac{ \exp[-nh_1(\hat{\theta}_1)](\sqrt{2\pi\sigma_1^2} n^{-\frac{1}{2}}) }
                { \exp[-nh_2(\hat{\theta}_2)](\sqrt{2\pi\sigma_2^2} n^{-\frac{1}{2}}) }
            \nonumber \\[9pt]
        &= \frac{\sigma_1}{\sigma_2} \exp[-n(h_1(\hat{\theta}_1) - h_2(\hat{\theta}_2)].
\end{align}
The last line of \eqref{tierney.kadane.form} is exactly the form found by Tierney and Kadane to be 
accurate to within:
\begin{equation}
    E[g(\theta|\x)] = \hat{E}[g(\theta|\x)](1 + \circ(n^{-2})),
\end{equation}
where $\circ(n^{-2})$ indicates ``on the order of'' or ``at the rate of'' $1/n^2$ as $n$ increases.  Therefore Laplace's
method\index{subjectindex}{Laplace approximation} replaces integration as a method of obtaining an estimate of $E[g(\theta|\x)]$
with differentiation.  Typically differentiation is not only easier to perform, but the algorithms are typically more numerically
stable (Gill, Murray, and Wright 1981).\index{authorindex}{Gill, P.} \index{authorindex}{Murray, W.} \index{authorindex}{Wright,
M. H.}

The variance of $g(\theta|\x)$ is produced from standard theory: 
$\Var[g(\theta|\x)] = \hat{E}[g(\theta|\x)^2] - (\hat{E}[g(\theta|\x)])^2$, which also has relative error on the order of $\circ(n^{-2})$ 
(Tierney\index{authorindex}{Tierney, L.} and Kadane\index{authorindex}{Kadane, J. B.} 1986, p.83).  The asymptotic properties are given in 
detail by several authors:  de Bruijn\index{authorindex}{de Bruijn, N. G.} (1981, Chapter 7), Kass, Tierney, and Kadane (1989), Tierney, 
Kass, and Kadane (1989b), and Schervish (1995).  For an applicable, but more generalized discussion, of the 
asymptotic behavior of Gaussian kernel estimators, see Le Cam\index{authorindex}{Le Cam, L.} (1986, Section 12.4) or
Barndorff-Nielsen\index{authorindex}{Barndorff-Nielsen, O. E.} and Cox\index{authorindex}{Cox, D. R.} (1989, Chapter 1).
\index{authorindex}{Kass, R. E.} \index{authorindex}{Tierney, L.}
\index{authorindex}{Kadane, J. B.} \index{authorindex}{Schervish, M. J.} 

The multivariate formulation of \eqref{tierney.kadane.form}\index{subjectindex}{Laplace approximation!multivariate} follows directly by 
substituting $\T$ for $\theta$, $|\SI|^{\frac{1}{2}}$ for $\sigma$, where $\SI$ is the inverse negative of the second derivative matrix
(Hessian)\index{subjectindex}{Hessian} of the likelihood function evaluated at the modal point.  Occasionally the multivariate approximation 
imposes some additional computational difficulties, such as the conditioning of the Hessian (Albert\index{authorindex}{Albert, J. H.} 1988; 
Hsu\index{authorindex}{Hsu, J. S. J.} 1995).  In the case where the derivatives required to produce \eqref{tierney.kadane.form} are not 
readily calculable, MCMC simulation techniques (introduced in Chapter~\ref{MCMC.Chapter}) have been developed to produce desired interim 
quantities (Raftery\index{authorindex}{Raftery, A. E.} 1996).  In some cases a slightly more complicated ``saddle point'' approximation 
can be substituted (Goutis and Casella 1999; Tierney, Kass, and Kadane 1989b).  Tierney, Kass, and Kadane (1989a) 
\index{authorindex}{Goutis, C.} \index{authorindex}{Casella, G.} \index{authorindex}{Tierney, L.} \index{authorindex}{Kass, R. E.}
\index{authorindex}{Kadane, J. B.} present a more flexible form of the basic Laplace procedure\index{subjectindex}{Laplace approximation} 
variant that reduces the need for typically more difficult higher-order derivatives.

Several implementation details are worth observing.  First, note that the two modal points, $\hat{\theta}_1$ and $\hat{\theta}_2$ differ.  
The second point is simply the standard maximum likelihood value.  Tierney \index{authorindex}{Tierney, L.} and Kadane 
\index{authorindex}{Kadane, J. B.} (1986) as well as Press\index{authorindex}{Press, S. J.} (1989, Section 3.3.1) point out that the first 
value typically lives in the same neighborhood and therefore can easily be found with a simple numerical search algorithm such as 
Newton-Raphson, using $\hat{\theta}_2$ as a starting point.  Also, under fairly general circumstances \eqref{tierney.kadane.form} is the 
\emph{empirical Bayes estimator} for $g(\theta|\x)$ (Lehmann\index{authorindex}{Lehmann, E. L.} and Casella\index{authorindex}{Casella, G.} 
1998, p.271).  It is also possible to use simulation techniques as a substitute for difficult derivatives in complex models.  This approach 
is then called \emph{Laplace-Metropolis} estimation\index{subjectindex}{Laplace-Metropolis} (Lewis\index{authorindex}{Lewis, S. M.} and 
Raftery\index{authorindex}{Raftery, A. E.} 1997, Raftery\index{authorindex}{Raftery, A. E.} 1996).

One convenient feature of Laplace's approximation is its flexibility with regard to new prior forms and newly observed data.  Define 
$p_{new}(\theta)$ as an alternative prior distribution that the researcher is interested in substituting into the calculation of the 
statistic of interest, $E[g(\theta|\x)]$.\index{subjectindex}{Laplace updating} Rather than recalculate the Laplace approximation, we can 
use the following shortcut (Carlin\index{authorindex}{Carlin, B. P.} and Louis\index{authorindex}{Louis, T. A.} 2009, pp.111-112).  First 
calculate a prior odds ratio of the new prior to the one already used in the calculation:
\begin{equation}
    b(\theta) = \frac{p_{new}(\theta)}{p(\theta)},
\end{equation}
so the new expected value of $g$ is:
\begin{align}
        \hat{E}_{new}[g(\theta|\x)] &=
         \frac{\int g(\theta|\x)L(\x|\theta)p(\theta)b(\theta)d\theta}
             {\int L(\x|\theta)p(\theta)b(\theta)d\theta}.
          \nonumber \\
          &= \frac{b(\theta_1)}{b(\theta_2)}  \hat{E}[g(\theta|\x)].
\end{align}
This means that we don't have to fully recalculate the expected value; instead we use the already maximized $\theta$ values, 
$\hat{\theta}_1$ and $\hat{\theta}_2$, from the previous calculations.  So we can very quickly try a large number of alternate 
prior specifications and immediately see the implications.

\section{Exercises}
\begin{exercises}
	\item	Perform a frequentist hypothesis test of $\Ho \theta=0$ versus $\Hz \theta=500$, where 
		    $X \sim \mathcal{N}(\theta,1)$ (one-tail at the $\alpha=0.01$ level).  A single datapoint is observed,
		    $x = 3$.  What decision do you make?  Why is this not a defensible procedure here?  Does a Bayesian
		    alternative make more sense?\index{subjectindex}{hypothesis testing!frequentist}
        
            % NEW
    \item   A random variable $X$ is distributed $\mathcal{N}(\mu,1)$ with unknown $\mu$.  A standard (non-Bayesian) hypothesis
            test is set up as: $\Ho \mu = 0$ versus $\Hz \mu = 50$ with $\alpha = 0.05$ (one-sided such that the critical
            value is 1.645 for rejection).  A sample of size 10 is observed, $(2.77,0.91,1.88,2.28,1.86,1.33,2.99,2.07,1.58,2.99)$, 
            with mean 2.07 and standard deviation 0.70.  Do you decide to reject the null hypothesis?  Why might this not be 
            reasonable?  \index{subjectindex}{hypothesis testing!frequentist}

    \item   Akaike\index{authorindex}{Akaike, H.} (1973) states that models with negative AIC
            \index{subjectindex}{Akaike information criterion (AIC)} are better than the saturated model and by extension the model 
            with the largest negative value is the best choice.  Show that if this is true, then the BIC
            \index{subjectindex}{Bayesian information criterion (BIC)} is a better asymptotic choice for comparison with the saturated 
            model.

            % NEW
    \item   For a given binomial experiment we observe $x$ successes out of $n$ trials.  Using a conjugate beta prior distribution 
            with parameters $\alpha$ and $\beta$, show that the marginal likelihood is:
            \index{subjectindex}{conjugacy!beta-binomial}
            \begin{equation*}
                \binom{n}{x}\frac{ \Gamma(\alpha+\beta)\Gamma(x+\alpha)\Gamma(n+\beta-x) }
                                 { \Gamma(\alpha)\Gamma(\beta)\Gamma(n+\alpha+\beta) }.
            \end{equation*}

	\item	Derive the last line of \eqref{bayes.factor.relationship} from:
            \begin{equation*}
		        p(H_0|\data) = 1 - p(H_1|\data) = 1 - 
		        \frac{1}{BF_{(1,0)}} \left[\frac{p(\data)}{p(H_0)}p(H_0|\data)\right]\frac{p(H_1)}{p(\data)}.
            \end{equation*}
		    % \begin{align*}
		    % 1 &= \frac{1}{p(H_0|\data)} - \frac{1}{BF_{(1,0)}} \frac{p(\data)}{p(H_0)} \frac{p(H_1)}{p(\data)} \9 
		    % 1 &= \frac{1}{p(H_0|\data)} - \frac{1}{BF_{(1,0)}} \frac{p(H_1)}{p(H_0)} \9 
		    % 1 + \frac{1}{BF_{(1,0)}} \frac{p(H_1)}{p(H_0)} &= \frac{1}{p(H_0|\data)} \9  
		    % p(H_0|\data) &= \left[ 1 + \frac{1}{BF_{(1,0)}} \frac{p(H_1)}{p(H_0)} \right]^{-1}.
		    % \end{align*}

            % NEW
    \item  \label{bayes.factor.mean.exercise} Derive the Bayes Factor for a two-sided test of a mean in
           \eqref{bayes.factor.mean} on page~\pageref{bayes.factor.mean}.

    \item   (Berger\index{authorindex}{Berger, J. O.} 1985).  A child takes an IQ test with the result that a score over 
            100 will be designated as above average, and a score of under 100 will be designated as below average.  The 
            population distribution is distributed $\mathcal{N}(100,225)$ and the child's posterior distribution is 
            $\mathcal{N}(110.39,69.23)$.  Test competing designations on a single test, $p(\theta\le 100|x)$ vs. 
            $p(\theta>100|x)$, with a Bayes Factor using equally plausible prior notions (the population distribution),
            and normal assumptions about the posterior. \index{subjectindex}{example!IQ score data}

            % NEW
    \item   Recalculate the model and the Bayes Factor for including an interaction term from the Hanushek
            \index{authorindex}{Hanushek, E. A.} and Jackson\index{authorindex}{Jackson, J. E.} election surveys example 
            (page~\pageref{election.survey.example}) using informed normal priors.\index{subjectindex}{logit model!grouped}

   	\item   Returning to the beta-binomial model from Chapter~\ref{Model.Chapter}, set up a Bayes Factor for: $p<0.5$ 
            versus $p\ge 0.5$, using a uniform prior, and the data:\\ $[0,1,0,1,1,1,1,0,0,0,0,0,0,0,1]$.  Perform
            the integration step using rejection sampling (Chapter~\ref{MC.Chapter}).
		    \index{subjectindex}{distribution!beta}\index{subjectindex}{conjugacy!beta-binomial}

            % NEW
    \item   Consider Example~\ref{dichot.exercise} from Appendix~A (page~\pageref{dichot.exercise}).  Specify normal priors 
            for the three-dimensional vector $\mathbf{\beta}$ with mean and variance equal to the observed sample quantities 
            in Table~\ref{FDR.table} on page~\pageref{FDR.table}.  Bedrick, Christensen, and Johnson (1997) 
			\index{authorindex}{Bedrick, E. J.}%
			\index{authorindex}{Christensen, R.}%
			\index{authorindex}{Johnson, W.}%
		    suggest using Bayes Factors to test competing link functions, as opposed to the more conventional idea of testing 
            competing  variable specifications.  Calculate the Bayes Factor for each of the three link functions against each 
            other.  Which would you use?  Why?

    \item   Demonstrate using rejection sampling\index{subjectindex}{rejection sampling} the following equality from Bayes'
            \index{authorindex}{Bayes, T.} original (1763) paper, using different values of $n$ and $p$:
            \index{subjectindex}{rejection sampling}
            \begin{equation}
                    \int_0^1 \binom{n}{p}x^p(1-x)^{n-p}dx = \frac{1}{n}.   \nonumber
            \end{equation}

            % NEW
    \item   Using the Palm Beach County voting model in Section~\ref{pbc.example} compare the full model with a null model 
            (intercept only) writing your own DIC function for linear specification in \R.
            \index{subjectindex}{Deviance Information Criterion (DIC)}\index{subjectindex}{example!2000 presidential election}

   	\item   Calculate the posterior distribution for $\mathbf{\beta}$ in a logit regression model
            \index{subjectindex}{logit model}
            \begin{equation}
                    r(\X'\bb) = p(\y=1|\X'\bb) = 1/[1+\exp(-\X'\bb)]    \nonumber
            \end{equation}
            with a \begin{normalfont}$\mathcal{BE}(A,B)$\end{normalfont} prior.  Perform a formal test of a 
            \begin{normalfont}$\mathcal{BE}(4,4)$\end{normalfont} prior versus a 
            \begin{normalfont}$\mathcal{BE}(4,1)$\end{normalfont} prior.  Calculate the Kullback-Leibler distance 
            between the two resulting posterior distributions.  \index{subjectindex}{Kullback-Leibler distance}

            % NEW
    \item   Taking the state-level obesity data from Exercise~\ref{exercise:obesity} on page~\pageref{exercise:obesity},
            segment the cases in Southern and non-Southern states to form two groups.  Do a Bayesian version of the standard
            Student's $t$-test for normal data that uses a Bayes Factor as described in Section~\ref{section:difference.of.means},
            starting on page~\pageref{section:difference.of.means}.  
            \index{subjectindex}{data!obesity}\index{subjectindex}{Student's-$t$}

	\item   (Aitkin 1991).\index{authorindex}{Aitkin, M.}   Model~1 specifies $y \sim \mathcal{N}(\mu_1,\sigma^2)$ with
		    $\mu_1$ specified and $\sigma^2$ known, Model~2 specifies $y \sim \mathcal{N}(\mu_2,\sigma^2)$ with $\mu_2$ 
            unknown and the same $\sigma^2$.  Assign the improper uniform prior to $\mu_2$: $p(\mu_2)=C/2$ over the 
            support $[-C\range C]$, where $C$ is large enough value to make this a reasonably uninformative prior.  For 
            a predefined standard significance test level critical value $z$, the Bayes Factor for Model~1 versus
		    Model~2 is given by:
		    \begin{equation*}
			    B = \frac{ 2Cn^{1/2}\phi(z) }
				    { \sigma \left[ \Phi\left( n^{1/2}\frac{\bar{y}+C}{\sigma} \right)
				               - \Phi\left( n^{1/2}\frac{\bar{y}-C}{\sigma} \right)
				    \right] }.
		    \end{equation*}
            Show that $B$ can be made as large as one likes as $C \rightarrow \infty$ or $n \rightarrow \infty$ for any fixed
            value of $z$.  This is an example of Lindley's paradox for a point null hypothesis test
            (Section~\ref{Adjusting.Tests.Section} starting on page~\pageref{Adjusting.Tests.Section}).
            \index{subjectindex}{paradox!Lindley's/Jeffreys'}

            % NEW
    \item   Chib (1995) introduces MCMC based tools for obtaining marginal likelihoods and therefore Bayes Factors.
            \index{authorindex}{Chib, S.}  Using the notation of that paper, $f(\y|\T_k,M_k)$ is the density function of
            the observed data under model $M_k$ with model-specific parameter vector $\T_k$, which has the prior
            distribution $\pi(\T_k|M_k)$ (notice that this different terminology for a prior than that used in this
            text).  If a model is estimated with a Gibbs sampler then $G$ iterated values, $\T^{(1)},\ldots,\T^{(G)}$,
            are produced.  Chib \index{authorindex}{Chib, S.} notes that the marginal likelihood under model $M_k$:
            \begin{equation*}
                m(\y|M_k) = \int f(\y|\T_k,M_k)\pi(\T_k|M_k)d\T_k
            \end{equation*}
            can be estimated with the harmonic mean of the generated likelihood values:
            \begin{equation*}
                \hat{m}_{NR} = \left\{ \frac{1}{G} \sum_{g=1}^G \left( \frac{1}{(\y|\T_k^{(g)},M_k)} \right) \right\}^{-1}.
            \end{equation*}
            For more details see Section~\ref{Section:chibs.method} starting on page~\pageref{Section:chibs.method}.
            Write an algorithm in pseudo-code to implement this process.  Why is this estimate ``simulation-consistent''
            but not numerically stable?

    \item   Given a Poisson likelihood function, instead of specifying the conjugate gamma distribution, stipulate 
            $p(\mu) = 1/\mu^2$.  Derive an expression for the posterior distribution of $\mu$ by first finding the value of 
            $\mu$, which maximizes the log density of the posterior, and then expanding a Taylor series around this point 
            (i.e., Laplace approximation).  Compare the resulting distribution with the conjugate result.
            \index{subjectindex}{Laplace approximation}

            % NEW
    \item   Returning to the example data from wars in ancient China (Section~\ref{ancient.china.war.example}, starting on
            page~\pageref{ancient.china.war.example}), calculate the Bayes Factors from possible mixes of the covariates using
            the \texttt{BayesFactor} package in \R.  Start with the following code:
            \index{subjectindex}{example!war in ancient China} \index{subjectindex}{BayesFactor@\texttt{BayesFactor}}
            \begin{R.Code}
lapply(c("BaM","BayesFactor"),library,character.only=TRUE) 
data(wars) 
regressionBF(SCOPE ~ ., data=wars)
            \end{R.Code} 
            From this long list pick a baseline model where the covariate selection is substantively reasonable and determine a 
            small set of alternative models that fit the better according to the Bayes Factor criteria.  

    \item   Using the Palm Beach County voting model in Section~\ref{pbc.example} (starting on page~\pageref{pbc.example}) with 
            uninformative priors, compare the full model with the nested model, leaving out the technology variable using a local
            Bayes Factor.  Experiment with different training samples and compare the subsequent results.  Do you find that the
            Bayes Factor is sensitive to your selection of training sample?\index{subjectindex}{Palm Beach County, FL}
            \index{subjectindex}{training sample}  The dataset is available as \texttt{pbc.vote} in \texttt{BaM}.
            \index{subjectindex}{example!2000 presidential election}

            % NEW
    \item   \label{exercise.chib.greenberg} Using the capital punishments data from Exercise~\ref{executions.example} on 
            page~\pageref{executions.example} (in the \texttt{BaM} package as dataset \texttt{executions}), write a Gibbs sampler
            in \R\ for a probit model (Appendix~A) with a latent variable representation (Chib and Greenberg 1998) and
            uninformative priors.  \index{authorindex}{Chib, S.} \index{authorindex}{Greenberg, E.}
            \index{subjectindex}{example!death penalty} Use the following steps with conditional distributions at the $j$th 
            iteration:
            \begin{enumerate}
                \item   Sample the latent variable according to truncated normal specification:
                        \begin{equation*}
                            z^{[j]}_i \sim \begin{cases}
                                \mathcal{TN}_{(-\infty,0)}\left(\x_i'\B^{[m-1]}, 1\right) \qquad \text{if} \; y_i = 0 \9
                                \mathcal{TN}_{[0,\infty)} \left(\x_i'\B^{[m-1]}, 1\right) \qquad \text{if} \; y_i = 1 
                            \end{cases}
                        \end{equation*}
                        for $i=1,\ldots,n$.  This creates the vector $\z^{[j]}$.
                \item   Sample the coefficient vector according to the normal specification:
                        \begin{equation*}
                            \B^{[j]} \sim \mathcal{N}\left( (\X'\X)^{-1}\X'\z^{[j]}, (\X'\X)^{-1} \right)
                        \end{equation*}
            \end{enumerate}
            Run this Gibbs sampler for two different specifications (mixes of covariates).  Modify the code to include 
            a calculation of the DIC within the sampler and use these values to compare the two model fits.
\end{exercises}



\thispagestyle{empty}
\chapter{Basics of Markov Chain Monte Carlo}\label{MCMC.Chapter}
\setcounter{examplecounter}{1}

\section{Who Is Markov and What Is He Doing with Chains?} \index{subjectindex}{Markov chain Monte Carlo (MCMC)|(}
The use of Markov chain Monte Carlo (MCMC) methods to evaluate integral quantities has exploded over the last two decades.  Beginning 
with the seminal review paper by Gelfand\index{authorindex}{Gelfand, A. E.} and Smith \index{authorindex}{Smith, A. F. M.} (1990), the 
rate of publication of MCMC works has grown exponentially. While this is relatively a recent development, the genesis dates back to two 
important works: the 1953 essay by Metropolis, Rosenbluth, Rosenbluth, Teller, and Teller, as well as Geman and Geman's (1984) 
introduction of the Gibbs sampler as a method for obtaining difficult posterior quantities 
in the process of image restoration.\index{subjectindex}{Gibbs sampler} The lack of early recognition of the importance of the 
1953 contribution is a testament to the barriers that may exist between statistical physics and other fields, and the hindsight that 
sufficiently powerful computational resources were not widely available until sometime afterwords.\index{subjectindex}{statistical physics} 
This history is nicely reviewed by Robert and Casella (2011), as well as by Richey (2010).
Another fundamental tool in this family is \emph{simulated annealing} (Kirkpatrick,\index{authorindex}{Kirkpatrick, S.}
Gelatt,\index{authorindex}{Gelatt, C. D.} and Vecchi \index{authorindex}{Vecchi, M. P.} [1983], 
\v{C}ern\'{y}\index{authorindex}{Cerny@\v{C}ern\'{y}, V.} [1985]), which is described at the begining of 
Chapter~\ref{MCMC.Extensions.Chapter}.
\index{authorindex}{Robert, C. P.} \index{authorindex}{Casella, G.} \index{authorindex}{Richey, M.}
\index{authorindex}{Geman, S.}\index{authorindex}{Geman, D.} 
\index{authorindex}{Metropolis, N.} 
\vspace{-6pt}

This chapter introduces the basic ideas behind MCMC methods with the goal of providing accessible introductions, whereas 
Chapter~\ref{MCMC.Theory.Chapter} covers more technical issues and Chapter~\ref{MCMC.Utilitarian.Chapter} discusses some practical 
guidance on running MCMC simulations.  In Chapter~\ref{MCMC.Extensions.Chapter} we look at extensions and enhancements to the standard 
MCMC algorithms.  The primary distinction made in this chapter is between standard Monte Carlo simulation methods, as covered in 
Chapter~\ref{MC.Chapter}, and the \emph{Markov chain} type of Monte Carlo methods characterized by a dependence structure between 
consecutive simulated values.  Standard Monte Carlo methods produce a set of \emph{independent} simulated values according to some 
desired probability distribution.  MCMC methods produce chains in which each of the simulated values is mildly dependent on the 
preceding value.  The basic principle is that once this chain has run long enough it will find its way to the desired 
posterior distribution of interest and we can summarize this distribution by letting the chain wander around, thus producing summary 
statistics from recorded values.  The ``magic'' that occurs is that a process based on mechanically producing serially correlated 
values from joint or conditional distributions eventually gives values that can be treated as independent draws from marginals.

\subsection{What Is a Markov Chain?}\index{subjectindex}{Markov chain!definition}
Chapter~\ref{MC.Chapter} paid extensive attention to the second ``MC'' in ``MCMC'' and we have yet to provide a precise definition for 
the first ``MC.''  The initial definition required is that of a more primitive concept that underlies Markov chains.  A 
\emph{stochastic process} \index{subjectindex}{stochastic process} is a consecutive set of random quantities defined on some known state 
space, $\Theta$, indexed so that the order is known: $\{\theta^{[t]}\range t \in T\}$.  Here the state space (which we can also refer to 
as the parameter space when directly referring to the support of a parameter vector of interest) is just the allowable range of values 
for the random vector of interest.  This will be more precisely defined in Chapter~\ref{MCMC.Theory.Chapter}.  Frequently, but not 
necessarily, $T$ is the set of positive integers implying consecutive, even-spaced time intervals: $\{\theta^{[t=0]}, \theta^{[t=1]}, 
\theta^{[t=2]}, \ldots\}$.  With MCMC we are concerned only with this restricted type of stochastic process.  

The state space, $\Theta$, is either discrete or continuous depending on how the variable of 
interest is measured, but the implications for our purposes apply more to notation than to fundamental theory.  Standard references on 
stochastic processes include Doob \index{authorindex}{Doob, J. L.} (1990), Hoel\index{authorindex}{Hoel, P. G.}, Port,
\index{authorindex}{Port, S. C.} and Stone\index{authorindex}{Stone, C. J.} (1987), Karlin \index{authorindex}{Karlin, S.} and Taylor
\index{authorindex}{Taylor, H. M.} (1981, 1990), and Ross\index{authorindex}{Ross, S.} (1996).

A \emph{Markov chain}\index{subjectindex}{Markov chain!definition} is a stochastic process with the property that at time $t$ in the 
series, the probability of making a transition to any new state is dependent only on the current state of the process, $\theta^{[t]}$, and 
is therefore \emph{conditionally} independent of the previous values: $\theta^{[0]}, \theta^{[1]},\ldots, \theta^{[t-1]}$.  
\index{subjectindex}{Markov chain!conditional independence}  This is stated more formally:
\begin{equation}
    p(\theta^{[t]} \in A|\theta^{[0]}, \theta^{[1]},\ldots, \theta^{[t-2]}, \theta^{[t-1]})
        = p(\theta^{[t]} \in A|\theta^{[t-1]}),
\end{equation}
where $A$ is any identified set (an event or range of events) on the complete state space.  So a Markov chain wanders around the state 
space ``remembering'' only where it has been in the last period.\index{subjectindex}{Markovian property}  This property turns out to be 
enormously useful in generating samples from desired limiting distributions of the chain because when the chain eventually finds the 
region of the state space with the highest density, it will produce a sample from this distribution that is only mildly 
nonindependent.  \index{subjectindex}{Markov chain!nonindependence}  These are the sample values that we will then use to describe the 
posterior distribution of interest.

A fundamental concern is the transition process that defines the probabilities of moving to other points in the state space, given the 
current location of the chain.  The most convenient way to think about this structure is to define the \emph{transition kernel}, $K$, 
\index{subjectindex}{Markov chain!transition kernel} as a general mechanism for describing the probability of moving to some other 
specified state based on the current chain status (Robert\index{authorindex}{Robert, C. P.} and Casella\index{authorindex}{Casella, G.} 
2004, p.208).  The advantage of this notation is that it subsumes both the continuous state space case as well as the discrete state 
space case.  It is required that $K(\theta,A)$ be a defined probability measure for all $\theta$ points in the state space to the set 
$A \in \Theta$.  Thus the function $K(\theta,A)$ maps potential transition events\index{subjectindex}{Markov chain!transition kernel} to 
their probability of occurrence.

When the state space is discrete, then $K$ is a matrix mapping, $k \times k$ for $k$ discrete elements in $A$, where each cell defines 
the probability of a state transition from the first term in the parentheses to all possible states:
\index{subjectindex}{Markov chain!transition kernel!discrete}
\begin{equation}\label{transition.matrix}
    P_A = \left[ \begin{array}{lll}
                p(\theta_1,\theta_1)    & \ldots    & p(\theta_1,\theta_k)   \\
                :                       &           & :                      \\
                p(\theta_k,\theta_1)    & \ldots    & p(\theta_k,\theta_k)   \\
    \end{array} \right],
\end{equation}
where the row indicates where the chain is at this period and the column indicates where the chain is going in the next period.  The 
rows of $P_A$ sum to one and define a conditional PMF since they are all specified for the same starting value and cover each possible 
destination in the state space: for row $i$: $\sum_{j=1}^{k}p(\theta_i,\theta_j) = 1$.  Each matrix element is a well-behaved 
probability, $p(\theta_i,\theta_j)\ge 0,\;\forall i,j\in A$.  When the state space is continuous, then $K$ is a conditional PDF: 
$f(\theta|\theta_i)$, meaning a properly defined probability statement for all $\theta \in A$, given some current state 
$\theta_i$.\index{subjectindex}{Markov chain!transition kernel!continuous}

An important feature of the transition kernel is that transition probabilities between two selected states for arbitrary numbers of 
steps $m$ can be calculated multiplicatively.  For instance, with a discrete state space the probability of transitioning from the 
state $\theta_i=x$ at time $0$ to the state $\theta_j=y$ in exactly $m$ steps is given by the multiplicative series:
\begin{equation}\label{discrete.transition.probability}
    p^m(\theta_j^{[m]}=y|\theta_i^{[0]}=x) =
      	\underbrace{ \sum_{\theta_1}\sum_{\theta_2}\cdots\sum_{\theta_{m-1}} }_{\text{all possible paths}}
      	\underbrace{ p(\theta_i,\theta_1)p(\theta_1,\theta_2)\cdots
                                            p(\theta_{m-1},\theta_j) }_{ \text{transition products} }.
\end{equation}
So $p^m(\theta_j^{[m]}=y|\theta_i^{[0]}=x)$ is also a stochastic transition matrix,\index{subjectindex}{stochastic transition matrix} 
and this property holds for all discrete chains exactly as given, and for continuous Markov chains with only a slight modification
involving integrals rather than summations.  The basic idea of (\ref{discrete.transition.probability}) is that the complete probability 
of transitioning from $x$ to $y$ is a product of all the required intermediate steps where we sum over all possible paths that reach 
$y$ from $x$.

\subsection{A Markov Chain Illustration}\label{simple.MC.example}\index{subjectindex}{Markov chain!two-dimensional example}
Start with a two-dimensional discrete state space, which can be thought of as discrete vote choice between two political parties, a 
commercial purchase decision between two brands, or some other choice.  Suppose that voters/consumers who normally select $\theta_1$ 
have an 80\% chance of continuing to do so, and voters/consumers who normally select $\theta_2$ have only a 40\% chance of continuing 
to do so.  Since there are only two choices, this leads to the transition matrix $\Pmat$:
\begin{align}
	&\;\;\qquad\quad\;
	\overbrace{ \begin{array}{rr} \theta_1 & \theta_2 \\ \end{array} }^{\text{next period}}	\nonumber \\
	\text{\footnotesize current period}&
	\begin{cases}
		\begin{array}{r}  \theta_1 \\ \theta_2 \\ \end{array}
		\left[ \begin{array}{rr} 0.8 & 0.2 \\ 0.6 & 0.4 \\ \end{array} \right].		\nonumber
	\end{cases}
\end{align}

All Markov chains begin with a starting point assigned by the researcher.\index{subjectindex}{Markov chain!starting points}  This 
two-dimensional initial state defines the proportion of individuals selecting $\theta_1$ and $\theta_2$ before beginning the chain.  
For the purposes of this example, assign the starting point:
\begin{equation}
	S_0 = \left[ \begin{array}{rr} 0.5 & 0.5 \\ \end{array} \right];				\nonumber
\end{equation}
that is, before running the Markov chain, 50\% of the population select each alternative.  To get to the first state, we 
simply multiply the initial state by the transition matrix:
\begin{equation}
	S_1 = \left[ \begin{array}{rr} 0.5 & 0.5 \\ \end{array} \right]
        \left[ \begin{array}{rr} 0.8 & 0.2 \\ 0.6 & 0.4 \\ \end{array} \right]
	= \left[ \begin{array}{rr} 0.7 & 0.3 \\ \end{array} \right] = S_1.
	\nonumber
\end{equation}
So after the first iteration we have the new proportions: 70\% select $\theta_1$ and 30\% select $\theta_2$.  This process continues 
multiplicatively as long as we like:
\begin{align}
	\text{Second state:} \quad
		S_2 = \left[ \begin{array}{rr} 0.7 & 0.3 \\ \end{array} \right]
        	\left[ \begin{array}{rr} 0.8 & 0.2 \\ 0.6 & 0.4 \\ \end{array} \right]
		&= \left[ \begin{array}{rr} 0.74 & 0.26 \\ \end{array} \right]
		\nonumber\\[4pt]
	\text{Third state:} \quad
		S_3 = \left[ \begin{array}{rr} 0.74 & 0.26 \\ \end{array} \right]
        	\left[ \begin{array}{rr} 0.8 & 0.2 \\ 0.6 & 0.4 \\ \end{array} \right]
		&= \left[ \begin{array}{rr} 0.748 & 0.252 \\ \end{array} \right]
		\nonumber\\[4pt]
	\text{Fourth state:} \quad
		S_4 = \left[ \begin{array}{rr} 0.748 & 0.252 \\ \end{array} \right]
        	\left[ \begin{array}{rr} 0.8 & 0.2 \\ 0.6 & 0.4 \\ \end{array} \right]
		&= \left[ \begin{array}{rr} 0.7496 & 0.2504 \\ \end{array} \right].
		\nonumber
\end{align}
As one might guess, the choice proportions are converging to $[0.75,0.25]$.  This is because the transition matrix is pushing toward 
a steady state or more appropriately ``stationary'' distribution of the proportions.  So when we reach this distribution all future 
states, $\mathbf{S}$, are constant: $\mathbf{S}^P = \mathbf{S}$.  
\index{subjectindex}{Markov chain!stationary distribution}
\index{subjectindex}{Markov chain!invariant distribution (see stationary)}
\index{subjectindex}{Markov chain!equilibrium distribution (see stationary)}

Imagine that this stationary distribution was the articulation of some PMF or PDF that we could not analytically describe but would 
like to.  If we could run this Markov chain sufficiently long we would eventually get the stationary distribution \emph{for any point 
in the state space}.  In fact, for this simple example we could solve directly for the steady state $\mathbf{S}= [s_1,s_2]$ by stipulating:
\begin{equation}
	\left[ \begin{array}{rr} s_1 & s_2 \\ \end{array} \right]
        \left[ \begin{array}{rr} 0.8 & 0.2 \\ 0.6 & 0.4 \\ \end{array} \right]
	= \left[ \begin{array}{rr} s_1 & s_2 \\ \end{array} \right],
	\nonumber
\end{equation}
and solving the resulting two equations for the two unknowns (using necessarily $s_1 + s_2 = 1$).  While this example is wildly 
oversimplified, it serves to show some basic characteristics of Markov chains.  The operation of running a Markov chain until it 
reaches its stationary distribution is a critical part of the process employed in MCMC estimation for Bayesian models.

\begin{examplelist}
	\item {\bf A Markov Chain for Card-Shuffling.}\label{Example.Cards}
	\index{subjectindex}{Markov chain!card-shuffling example}	\index{subjectindex}{example!playing cards}
	A simplistic algorithm for shuffling a deck of cards is to take the top card and insert it uniformly back into the deck, and 
	repeat this process many times.  Thus the top card has the probability $1/52$ of being placed at any position in the stack, 
	including returning to the top position.  This is clearly a stochastic process operating on a discrete state space since 
	there is a sequential set of identifiable states from a finite number ($52!$) of possible states.  Is this a Markov chain 
	though?  The probability of some state being the next observed state is conditional on two things: \emph{(1)} the current 
	arrangement of the cards in the deck, and \emph{(2)} the placement of the top card at this step.  Therefore conditional on 
	the current state, previous states are irrelevant and the probabilistic process depends only on the transition process and 
	this current state.  So we can claim that this shuffling process is a Markov chain.
	
	More interestingly, what is the limiting (stationary) distribution of this Markov chain?  Such examples from games of chance 
	can be quite interesting on their own, but they also show the mechanics of basic Markov chains.  See Bayer and Diaconis (1992) 
	or Diaconis (1988) \index{authorindex}{Bayer, D.} \index{authorindex}{Diaconis, P.} for more details.  Rather than 
	analytically look at the limiting distribution, let's simulate it with \R.  For simplicity (without loss of generality), we will 
	use only $n=3$ cards.  This means that the sample space ($3!$ elements large) is the set: 
	\begin{equation*}
		\{[1,2,3],[1,3,2],[2,1,3],[2,3,1],[3,1,2],[3,2,1]\} 
	\end{equation*}
	and the transition kernel is:
        \begin{equation}
            K  = \left[ \begin{array}{cccccccccccc}
            \third &\;\qquad& 0 &\;\qquad& \third &\;\qquad& \third &\;\qquad& 0 &\;\qquad& 0 \\ 
                 0 &\;& \third &\;&      0 &\;&      0 &\;& \third &\;& \third \\ 
            \third &\;& \third &\;& \third &\;&      0 &\;&      0 &\;&      0 \\ 
                 0 &\;&      0 &\;&      0 &\;& \third &\;& \third &\;& \third \\ 
            \third &\;& \third &\;&      0 &\;&      0 &\;& \third &\;&      0 \\ 
                 0 &\;&      0 &\;& \third &\;& \third &\;&      0 &\;& \third \\
            \end{array} \right] \nonumber
        \end{equation}
	
	\noindent where the placement of zeros on any given row indicate that element of state space is an impossible destination given the 
	starting point indicated by the row.  So on the top row, we have the starting state $[1,2,3]$, making $[1,3,2]$, $[3,1,2]$, 
	and $[3,2,1]$ impossible from the operation of moving the top card only.  We can set up a simulation with the following:

	\begin{R.Code}
	P <- matrix(c(1/3,0,1/3,0,1/3,0,0,1/3,1/3,0,1/3,0,1/3,
	              0,1/3,0,0,1/3,1/3,0,0,1/3,0,1/3,0,1/3,0,
	              1/3,1/3,0,0,1/3,0,1/3,0,1/3),nrow=6)

	MC.multiply <- function(P.in,N)  \{
	    P1 <- c(1,0,0,0,0,0)%*%P.in
	    for (i in 1:(N-1))  \{
	        P1 <- P1%*%P.in
	        print(P1)
	    \}
	    P1
	\}

	MC.multiply(P,15)
	\end{R.Code}

	These 15 iterations produce:\\[18pt]
    \parbox[c]{\linewidth}{
    \begin{small}
	\begin{tabular}{rrrrrr}
		0.2222222 & 0.1111111 & 0.2222222 & 0.2222222 & 0.1111111 & 0.1111111 \\
		0.1851852 & 0.1481481 & 0.1851852 & 0.1851852 & 0.1481481 & 0.1481481 \\
		0.1728395 & 0.1604938 & 0.1728395 & 0.1728395 & 0.1604938 & 0.1604938 \\
		0.1687243 & 0.1646091 & 0.1687243 & 0.1687243 & 0.1646091 & 0.1646091 \\
		0.1673525 & 0.1659808 & 0.1673525 & 0.1673525 & 0.1659808 & 0.1659808 \\
		0.1668953 & 0.1664380 & 0.1668953 & 0.1668953 & 0.1664380 & 0.1664380 \\
		0.1667429 & 0.1665905 & 0.1667429 & 0.1667429 & 0.1665905 & 0.1665905 \\
		0.1666921 & 0.1666413 & 0.1666921 & 0.1666921 & 0.1666413 & 0.1666413 \\
		0.1666751 & 0.1666582 & 0.1666751 & 0.1666751 & 0.1666582 & 0.1666582 \\
		0.1666695 & 0.1666638 & 0.1666695 & 0.1666695 & 0.1666638 & 0.1666638 \\
		0.1666676 & 0.1666657 & 0.1666676 & 0.1666676 & 0.1666657 & 0.1666657 \\
		0.1666670 & 0.1666664 & 0.1666670 & 0.1666670 & 0.1666664 & 0.1666664 \\
		0.1666668 & 0.1666666 & 0.1666668 & 0.1666668 & 0.1666666 & 0.1666666 \\
		0.1666667 & 0.1666666 & 0.1666667 & 0.1666667 & 0.1666666 & 0.1666666 \\
		0.1666667 & 0.1666666 & 0.1666667 & 0.1666667 & 0.1666666 & 0.1666666 \\
	\end{tabular}
    \vspace{18pt}
    \end{small}
    }

	This is not a proof, but there is evidence here that the limiting distribution is uniform across the six states.  In fact 
	running \texttt{MC.multiply} for thousands or tens of thousands of iterations produces no changes in probabilities.  We 
	could therefore conclude that this algorithm produces a shuffled deck, albeit not in the most efficient fashion.
\end{examplelist}

\subsection{The Chapman-Kolmogorov Equations} \index{subjectindex}{Chapman-Kolmogorov Equations}
The form of (\ref{discrete.transition.probability}) also leads to a more general notion of how chain probabilities are strung 
together.  The Chapman-Kolmogorov equations specify how successive events are bound together probabilistically.  These are given 
here for both discrete and continuous state spaces where we abbreviate the left-hand side expression of 
\eqref{discrete.transition.probability}:
\begin{align}\label{chapman-kolmogorov}
    p^{m_1+m_2}(x,y) = \sum_{\text{all}\: z}p^{m_1}(x,z)p^{m_2}(z,y)
        &\qquad \text{discrete case,}  \nonumber\\
    p^{m_1+m_2}(x,y) = \int_{\text{range}\;z}p^{m_1}(x,z)p^{m_2}(z,y)dz
        &\qquad \text{continuous case}.
\end{align}
The Chapman-Kolmogorov equations are particularly elegant for the discrete case because (\ref{chapman-kolmogorov}) can be 
represented as a series of transition matrix multiplications:
\begin{equation}
    p^{m_1+m_2} = p^{m_1}p^{m_2} = p^{m_1}p^{m_2-1}p = p^{m_1}p^{m_2-2}p^2 =\ldots.
\end{equation}
Thus iterative probabilities can be decomposed into segmented products in any way that we like, depending on the interim steps.

\subsection{Marginal Distributions} \index{subjectindex}{Markov chain!marginal distribution}
The final basic notational characteristic of Markov chains that we will provide here is the \emph{marginal} distribution at some 
step $m$th from the transition kernel.  For the discrete case the marginal distribution of the chain at the $m$ step is obtained 
by inserting the current value of the chain, $\theta_i^{[m]}$, into the row of the transition kernel for the $m^{th}$ step, $p^{m}$:
\begin{equation}
    \pi^{m}(\theta) = [p^m(\theta_1), p^m(\theta_2),\ldots,p^m(\theta_k)].
\end{equation}
So the marginal distribution at the first step of the Markov chain is given by:
\begin{equation}
    \pi^{1}(\theta) = \pi^{0}(\theta)p^{1},
\end{equation}
where $\pi^{0}$ is the initial starting value assigned to the chain and $p^{1} = p$ is the simple transition matrix given in 
\eqref{transition.matrix}.  A neat consequence of the defining characteristic of the transition matrix is the relationship between 
the marginal distribution at some (possibly distant) step and the starting value:
\begin{equation}\label{marginal.distribution.definition}
    \pi^{n} = p\pi^{n-1} = p(p\pi^{n-2}) = p^2(p\pi^{n-3}) = \ldots = p^{n}\pi^{0}.      
\end{equation}
Since it is clear here that successive products of probabilities quickly result in lower probability values, the property above 
shows how Markov chains eventually ``forget'' their starting points.\index{subjectindex}{Markov chain!memory}  The marginal 
distribution for the continuous case is only slightly more involved since we cannot just list as a vector the quantity:
\begin{equation}
    \pi^{m}(\theta_j) = \int_{\theta} p(\theta,\theta_j)\pi^{m-1}(\theta)d\theta,
\end{equation}
which is the marginal distribution of the chain, given that the chain is currently on point $\theta_j$ at step $m$.

\section{General Properties of Markov Chains}
There are several properties of Markov chains that are important to us, particularly when 
discussing long run Markov chain stability.  These properties have intimidating names that are inherited from 
mathematical Markov chain theory, but in reality are fairly straightforward ideas.  
Generally, if we can describe the mathematical status of a particular chain, then we can 
often determine if it is capable of producing a useful sample from the distribution of interest.  The 
properties are only briefly summarized here and those interested in a more technical and 
detailed treatment should read: Gamerman\index{authorindex}{Gamerman, D.} and Lopes\index{authorindex}{Lopes, H. F.} (2006), 
Iosifescu\index{authorindex}{Iosifescu, M.} (1980), Norris\index{authorindex}{Norris, J. R.} (1997), 
Nummelin\index{authorindex}{Nummelin, E.} (1984), or Tierney\index{authorindex}{Tierney, L.} (1996).

\subsection{Homogeneity}
A \emph{homogeneous}\index{subjectindex}{Markov chain!properties!homogeneous} Markov chain at step $n$ has a transition probability 
that does not depend on the value of $n$.  So the decision to move at this step is independent of this being the current point in 
time.  Interestingly, but not importantly, at the starting point the chain cannot be homogeneous since the marginal distribution for 
the first step is clearly not independent of the initial values, which are hand-picked.  One reason that the Gibbs sampler and the 
Metropolis-Hastings algorithm, both given in detail in this chapter, dominate MCMC implementations is that the chains they define 
possess this critical property: there is no explicit value of $n$ that governs the transition kernel.

\subsection{Irreducibility}\index{subjectindex}{Markov chain!properties!irreducible}
There are also properties directly associated with states.  A state is \emph{absorbing} \index{subjectindex}{absorbing state} if 
once the chain enters this state it cannot leave: $p(A,A^c) = 0$.  The obverse of absorbing is \emph{transient}.  A state is 
transient\index{subjectindex}{transient state} if, given that a chain currently occupies state $A$, the probability of not 
returning to $A$ is non-zero. A more relevant case of absorbing is the situation where a state, $A$, is \emph{closed} with regard 
to some other state, $B$: $p(A,B) =0$.\index{subjectindex}{Markov chain!closed states}

A Markov chain is \emph{irreducible}\index{subjectindex}{Markov chain!properties!irreducible} if every point or collection of 
points (a subspace, required in the continuous case), $A$, can be reached from every other point or collection of points.  A 
convenient way to remember the principle behind irreducibility is the notion that you could reduce the set if you wanted to 
(this is obviously always possible except for the null set), but that \emph{you do not want to} because then there will be points 
that cannot be reached from other points.  This means that $p(\theta_i,\theta_j) \ne 0,\; \forall i,j \in A$.  Notice that 
irreducibility is a characteristic of both the chain and the subspace.  So irreducibility implies the existence of a path between 
any two points in the subspace.  The key relationship of interest is between irreducibility and recurrence.

\subsection{Recurrence}\index{subjectindex}{recurrence} \index{subjectindex}{Markov chain!properties!recurrent}   
\emph{If a subspace is closed, finite, and irreducible, then all states within this subspace are recurrent}.  Recurrence is a 
desirable property of Markov chains.  An irreducible Markov chain is called \emph{recurrent} with regard to a given state, $A$, 
which is a single point or a defined collection of points (required for the bounded-continuous case), if the probability that the 
chain occupies $A$ infinitely often over unbounded time is nonzero. This can also be restated by saying that the expected number 
of returns to $A$ in the limit is infinity.  The Markov chain is \emph{positive recurrent}
\index{subjectindex}{Markov chain!properties!positive recurrent} \index{subjectindex}{recurrence!positive} if the mean time to 
return to $A$ is bounded, otherwise it is called \emph{null recurrent}.  This characteristic of Markov chains was introduced by 
Doeblin\index{authorindex}{Doeblin, W.} (1940) (obviously without knowing of its eventual importance in practical Bayesian 
applications).  

If we only had to deal with discrete or finite-bounded state spaces then this form of recurrence would be enough, but with 
unbounded-continuous state spaces it is necessary to have a stricter definition of recurrence that guarantees that the probability 
of visiting $A$ infinitely often in the limit is now one: \emph{Harris recurrence}\index{subjectindex}{recurrence!Harris} 
\index{subjectindex}{Markov chain!properties!Harris recurrent} (Harris\index{author}{Harris, T. E.} 1956).  First we say that a 
set $A$ is Harris recurrent if the probability of the chain visiting $A$ infinitely often in the limit is one.  An irreducible 
Markov chain is Harris recurrent if, for a finite probability measure $\mathfrak{P}$, the chain at time $n$ has for all such finite 
subsets $A$ of the measure space a non-zero probability of reaching $A$ (Athreya\index{authorindex}{Athreya, K. B.}
and Ney\index{authorindex}{Ney, P.} 1978).  Formally: if there exists a $\sigma$-finite probability measure $\mathfrak{P}$ on the
measure space $\mathbf{S}$ so that an irreducible Markov chain, $X_n$, at time $n$ has the
property: $p(X_n \in A)=1,\; \forall A \in S$ where $\mathfrak{P}(A) > 0$.

The distinction between recurrence and Harris recurrence is important in demonstrating the convergence of specific Markov chain 
algorithms for continuous state spaces.  In fact, an aperiodic, irreducible chain with an invariant distribution on an unbounded 
continuous state space that is \emph{not} Harris recurrent has a positive probability of getting stuck forever in an area bounded 
away from convergence, given a starting point there.  

Additional details on recurrence are found in the key works in this area: Meyn\index{authorindex}{Meyn, S. P.} and Tweedie
\index{authorindex}{Tweedie, R. L.} (1994), Tierney\index{authorindex}{Tierney, L.} (1994), and Athreya,
\index{authorindex}{Athreya, K. B.} Doss,\index{authorindex}{Doss, H.} and Sethuraman\index{authorindex}{Sethuraman, J.} (1996).  
For our purposes it is sufficient to simply consider the point: ``Harris recurrence essentially says that there is no 
measure-theoretic pathology'' (Chan 1994).  The greatest concern here is that ill-chosen starting points can cause eventual problems.
\index{authorindex}{Chan, K. S.} and Geyer\index{author}{Geyer, C. J.} 

Given a set of recurrent states (nonempty, and bounded or countable) or Harris recurrent states, then the union of these states creates 
a new state that is closed and irreducible (Meyn\index{authorindex}{Meyn, S. P.} and Tweedie\index{authorindex}{Tweedie, R. L.} 1993).
\index{subjectindex}{unions of recurrent states}  This means that the linkage between recurrence and irreducibility is important in 
defining a subspace that captures a Markov chain and at the same time assures that this Markov chain will explore all of the subspace.  
Whenever a chain wanders into a closed, irreducible set of Harris recurrent states, it then stays there and visits every single state 
(eventually) with probability one (replaces almost sure convergence with convergence at every point).
\index{subjectindex}{Markov chain!recurrence and irreducibility}

\subsection{Stationarity}
Define $\pi(\theta)$ as the stationary distribution of the Markov chain for $\theta$ on the state space $A$.
\index{subjectindex}{Markov chain!stationary distribution} We denote $p(\theta_i,\theta_j)$ to indicate the probability that the 
chain will move from $\theta_i$ to $\theta_j$ at some arbitrary step $t$ from the transition kernel, and $\pi^{t}(\theta)$ as the 
marginal distribution.  This stationary distribution is then defined as satisfying:
\begin{align}\label{stationarity.requirement}
    \sum_{\theta_i}\pi^{t}(\theta_i)p(\theta_i,\theta_j) = \pi^{t+1}(\theta_j)
        &\qquad \text{Discrete case}  \nonumber\\
    \int\pi^{t}(\theta_i)p(\theta_i,\theta_j)d\theta_i = \pi^{t+1}(\theta_j)
        &\qquad \text{Continuous case}.
\end{align}
Therefore multiplication by the transition kernel and evaluating for the current point (the summation step for discrete sample 
spaces and the integration step for continuous sample spaces) produces the same marginal distribution: $\pi = \pi p$ in shorthand.  
This demonstrates that the marginal distribution remains fixed when the chain reaches the stationary distribution and we might as 
well drop the superscript designation for iteration number and just use $\pi(\theta)$.  
\index{subjectindex}{stationary distribution and marginals}

Once the chain reaches its stationary distribution (also called its \emph{invariant distribution}, \emph{equilibrium distribution}, 
or \emph{limiting distribution} if discussed in the asymptotic sense), it stays in this distribution and moves about, or ``mixes,''
\index{subjectindex}{Markov chain!mixing} throughout the subspace according to marginal distribution, $\pi(\theta)$, forever.  This 
is exactly what we want and expect from MCMC.  If we can set up the Markov chain so that it reaches a stationary distribution that 
is the desired posterior distribution from our Bayesian model, then all we need to do is let it wander about this subspace for a 
while, producing empirical samples to be summarized.  The good news is that the two primary forms of MCMC kernels we will use have 
the property that they are guaranteed to eventually reach a stationary distribution that is the desired posterior distribution.

\subsection{Ergodicity}\index{subjectindex}{Markov chain!ergodicity|(}
It is also possible to define the \emph{period} of a Markov chain.  This is simply the length of time to repeat an identical cycle 
of chain values.\index{subjectindex}{Markov chain!chain period}  It is desirable to have an aperiodic chain, i.e., where the only 
length of time for which the chain repeats some cycle of values is the trivial case with cycle length equal to one.  Why?  It 
seems as though we would not necessarily care if there were some period to the chain values, particularly if the period were quite 
long, or perhaps in the discrete state if it included every value in the state space.  The answer is that the recurrence property 
alone is not enough to assure that the chain reaches a state where the marginal distribution remains fixed and identical to the 
posterior of interest.

If a chain is irreducible, positive Harris recurrent, and aperiodic, then we call it \emph{ergodic}.  Ergodic Markov chains have 
the property:\index{subjectindex}{Markov chain!properties!ergodicity}
\begin{equation}
    \underset{n\rightarrow \infty}{\text{lim}}p^n(\theta_i,\theta_j) = \pi(\theta_j),
\end{equation}
for all $\theta_i$, and $\theta_j$ in the subspace (Nummelin\index{authorindex}{Nummelin, E.} 1984).  Therefore, in the limit, the 
marginal distribution at one step is identical to the marginal distribution at all other steps.  Better yet, because of the 
recurrence requirement, this limiting distribution is now closed and irreducible, meaning that the chain will never leave it and 
will eventually visit every point in the subspace.  Once a specified chain is determined to have reached its ergodic state, sample 
values behave as if they were produced by the posterior of interest from the model.

The \emph{ergodic theorem} is analogous to the strong law of large numbers but for Markov chains.\index{subjectindex}{ergodic theorem}  
It states that any specified function of the posterior distribution can be estimated with samples from a Markov chain in its ergodic 
state because averages of sample values give strongly consistent parameter estimates.  More formally, suppose 
$\theta_{i+1},\ldots,\theta_{i+n}$ are $n$ (not necessarily consecutive) values from a Markov chain that has reached its ergodic 
distribution, a statistic of interest, $h(\theta)$, can be calculated empirically:
\begin{equation}
    \hat{h}(\theta_i) = \frac{1}{n}\sum_{j=i+1}^{i+n}h(\theta_j) \approx h(\theta),
\end{equation}
and for finite quantities this converges almost surely: $p[\hat{h}(\theta_i) \rightarrow h(\theta),\;\text{as}\; n\rightarrow\infty]=1$ 
(Roberts\index{authorindex}{Roberts, G. O.} and Smith\index{authorindex}{Smith, A. F. M.} 1994, p.210; Tierney
\index{authorindex}{Tierney, L.} 1994, p.1717).  The remarkable result from ergodicity is that even though Markov chain values, by their 
very definition, have serial dependence, the mean of the chain values provides a strongly consistent estimate of the true parameter.  
\index{subjectindex}{Markov chain!nonindependence} For a given empirical estimator $\hat{h}(\theta_i)$ with bounded limiting variance, 
we get the central limit theorem results:\index{subjectindex}{central limit theorem!ergodic} 
\index{subjectindex}{ergodic!central limit theorem result}
\begin{equation}
    \sqrt{n}\frac{\hat{h}(\theta_i)-h(\theta)}{\sqrt{\Var(\hat{h}(\theta_i))}}
    \underset{n \rightarrow \infty}{\longrightarrow} \mathcal{N}(0,1).
\end{equation}
This is an important principle for MCMC estimation because it says that we can take the simulation values from the stationary 
distribution and safely ignore the serial nature of their production.  For more detailed theoretical justifications, see Meyn
\index{authorindex}{Meyn, S. P.} and Tweedie\index{authorindex}{Tweedie, R. L.}, (1993, Chapter 15), or Tierney
\index{authorindex}{Tierney, L.} (1994).  
\index{subjectindex}{Markov chain!ergodicity|)}

\section{The Gibbs Sampler}\label{gibbs.sampling.section}
The Gibbs sampler,\index{subjectindex}{Gibbs sampler|(} originating with Geman\index{authorindex}{Geman, S.} 
\index{authorindex}{Geman, D.} and Geman (1984), is the most widely used MCMC technique.  This is a testament to its simplicity and 
reliability as a method for producing useful chain values.  The Gibbs sampler requires specific knowledge about the conditional 
nature of the relationship between the variables of interest. The basic idea, which is not difficult to conceptualize, is that if it 
is possible to express each of the coefficients to be estimated as conditioned on the others, then by cycling through these 
conditional statements, we can eventually reach the true joint distribution of interest.

\subsection{Description of the Algorithm}
The Gibbs sampler is a transition kernel\index{subjectindex}{Gibbs sampler!transition kernel} created by a series of full conditional 
distributions that is a Markovian updating scheme based on conditional probability statements.
\index{subjectindex}{Gibbs sampler!characteristics} If the limiting distribution of interest is $\pi(\T)$ where $\T$ is a $k$ length 
vector of coefficients whose posterior distribution we want to describe, then the objective is to produce a Markov chain that cycles 
through these conditional statements moving toward and then around this distribution.  The set of full conditional distributions for 
$\T$ are denoted $\TT$ and defined by $\pi(\TT) = \pi(\theta_i|\T_{-i})$ for $i=1,\ldots,k$, where the notation $\T_{-i}$ indicates 
a specific parametric form from $\TT$ without the $\theta_i$ coefficient.

There must be an analytically definable full conditional statement for each coefficient in the $\T$ vector and these probability 
statements need to be completely articulated so that it is possible to draw samples from the described distribution.  This 
requirement facilitates the iterative nature of the Gibbs sampling algorithm, which cycles through these full conditionals drawing 
parameter values based on the most recent version of all of the previous parameters in the list.  The order does not matter, but it 
is essential that the most recent draws from the other samples be used.  This looks like the following procedure (note the use of 
the most recent iteration values at each step): \index{subjectindex}{Gibbs sampler!full conditional distributions}
\index{subjectindex}{Gibbs sampler!cycles}
\begin{enumerate}
    \item   Choose starting values: $\T^{[0]} = [\theta_1^{[0]},\theta_2^{[0]},\ldots,\theta_k^{[0]}]$
    \item   At the $j^{th}$ starting at $j=1$ complete the single cycle by drawing values
            from the $k$ distributions given by:
            \begin{align*}
             &\theta_1^{[j]}     && \sim              && \pi(\theta_1      && |                     && \theta_2^{[j-1]},   
			         && \theta_3^{[j-1]}, && \theta_4^{[j-1]}, && \ldots,               && \theta_{k-1}^{[j-1]}, 
				 && \theta_k^{[j-1]}) && \9
             &\theta_2^{[j]}     && \sim              && \pi(\theta_2      && |                     && \theta_1^{[j]},     
				 && \theta_3^{[j-1]}, && \theta_4^{[j-1]}, &&\ldots,                && \theta_{k-1}^{[j-1]}, 
			         && \theta_k^{[j-1]}) && \9
             &\theta_3^{[j]}     && \sim              && \pi(\theta_3      && |                     && \theta_1^{[j]},     
				 && \theta_2^{[j]},   && \theta_4^{[j-1]}, && \ldots,               && \theta_{k-1}^{[j-1]}, 
				 && \theta_k^{[j-1]}) && \9
                  &                   && \;\;\vdots        &&                   &&			    &&                       
				 &&                   &&                   &&                       &&                       
				 &&   		      && \9
             &\theta_{k-1}^{[j]} && \sim\;            && \pi(\theta_{k-1}  && |                     && \theta_1^{[j]},   
			         && \theta_2^{[j]},   && \theta_3^{[j]},   && \ldots,               && \theta_{k-2}^{[j]},     
				 &&\theta_k^{[j-1]})  && \9
             &\theta_{k}^{[j]}   && \sim              && \pi(\theta_{k}   && |                     && \theta_1^{[j]},   
			         && \theta_2^{[j]},   && \theta_3^{[j]},  && \ldots,               && \theta_{k-2}^{[j]},   
				 &&\theta_{k-1}^{[j]})  && 
            \end{align*}
        \item Increment $j$ and repeat until convergence.
\end{enumerate}
Once convergence is reached, all simulation values are from the target posterior distribution and a sufficient number should then 
be drawn so that all areas of the posterior are explored.   Notice the important feature that during each iteration of the cycling 
through the $\T$ vector, conditioning occurs on $\T$ values that have already been sampled for that cycle; otherwise the $\T$ 
values are taken from the last cycle.  So in the last step for a given $j$ cycle, the sampled value for the $k^{th}$ parameter 
gets to condition on \emph{all} $j$-step values.  

The statements above clearly demonstrate that it is required to have the full set of conditional distributions to run the Gibbs 
sampling algorithm.  As we will see in Chapter~\ref{Hierarchical.Chapter}, these necessary statements often fall naturally out of 
the hierarchical conditional relationships.  In some cases they come from classic theory.  For example, a simple bivariate normal 
specification for $\theta_1$ and $\theta_2$ given by:
	$\mathcal{N}\left( \left[\begin{smallmatrix} 0 \\ 0 \end{smallmatrix} \right],
	\left[ \begin{smallmatrix} 1 & \rho \\ \rho & 1 \end{smallmatrix} \right] \right)$,
gives the set of full conditional distributions:
\begin{align}
        \theta_1^{[j]}|\theta_2^{[j-1]} \sim \mathcal{N}(\rho\theta_2^{[j-1]},1-\rho^2)		\nonumber\9
        \theta_2^{[j]}|\theta_1^{[j]}   \sim \mathcal{N}(\rho\theta_1^{[j]},1-\rho^2).		\nonumber
\end{align}	\index{subjectindex}{distribution!bivariate normal!conditional}

If the Gibbs sampler has run sufficiently long, forthcoming full cycles of the algorithm produce a complete sample of the 
coefficients in the $\T$ vector.  All future iterations from this point on produce samples from the desired limiting 
distribution and can therefore be described empirically.  The most impressive aspect of the Gibbs sampler is that these 
conditional distributions contain sufficient information to eventually produce a sample from the full joint distribution of 
interest.  \index{subjectindex}{Gibbs sampler!joint distribution}

\subsection{Handling Missing Dichotomous Data with the Gibbs Sampler}\index{subjectindex}{Gibbs sampler!missing data}
Suppose that a coin is flipped 20 times and the result is recorded such that a head is fixed as 1 and a tails is fixed as 0.  
Of these $X_1,X_2,\ldots,X_{19}$ are observed and the last, $X_{20}$, is lost or unobserved.  The standard question for this 
experiment is whether or not the coin is fair, or more generally, what is the probability that coin produces a head, $\theta$.  

A useful simplification is to treat the sum of the observed $0/1$ outcomes as a single random variable as done in
Example~\ref{betabinomial.example} (page~\pageref{betabinomial.example}), $Y = \sum_{i=1}^{19} \X_i$, and ignore the missing
value.  Thus $Y$ is now distributed according to the binomial probability mass function:
\begin{equation*}
    p(Y|\theta) = \binom{n}{Y}\theta^Y(1-\theta)^{n-Y}
\end{equation*}
where $n=19$ here.  Of course this neglects the effect of the missing value ($X_{20}$) and may cause bias. However, we can use 
Gibbs sampling to estimate its value simultaneously along with the unknown $\theta$.  Clearly, the PMF of the missing value is 
still a Bernoulli form conditional on $\theta$.  If we were able to treat $Y$ and $X_{20}$ as known (observed) then the 
distribution of $\theta$ would be:
\begin{equation*}
    p(\theta|Y,X_{20}) = \binom{20}{Y+X_{20}}\theta^{Y+X_{20}}(1-\theta)^{20-Y-X_{20}}
\end{equation*}
which gives the kernel of a beta probability density function.  This means that we now have two full conditional distributions, 
given temporary values for each unknown, $\theta^*$ and $X_{20}^*$:
\begin{align*}
    \theta|X_{20}^*,Y &\sim \text{\normalfont beta}(Y+X_{20}^*+1,20-Y-X_{20}^*+1) \\
    X_{20}|Y,\theta^* &\sim \text{\normalfont Bernoulli}(\theta^*).
\end{align*}
This is an ideal setup for Gibbs sampling since the joint distribution can be treated as a full conditional distribution for 
either $\theta$ or $X_{20}$ if the other is given a temporary value.  So if our fundamental interest lies in estimating 
$\theta$ without bias, this process explicitly incorporates the missing value rather than ignoring it.

\begin{figure}[h!]
\parbox[l]{\linewidth}{ \hspace{-0.28in}
	\epsfig{file=./Images/mcmc_intro.figure01.ps,height=6.0in,width=3in,clip=,angle=270}  }
    \vspace{-18pt}
	\caption{\textsc{Gibbs Sampling Demonstration}} \label{Gibbs.Demo.Figure}
\end{figure}

We begin with the sample: $\{1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1\}$,
\index{subjectindex}{burn-in period of a chain} and run the chain for 50,000 iterations.  Let us consider the first 20,000 
iterations as ``burn-in'' while the Markov chain finds its limiting distribution and dispose of them as pre-convergence 
values.  The posterior mean of the rest of the simulated $\theta$ values is $0.714$ (with variance $0.009$) suggesting that 
coin is far from ``fair.''  This is also different than the observed data mean of $0.737$, which would have been a biased estimator.  
The posterior mean for the missing datapoint is $0.723$ making it seem more likely to be a one than a zero, although 
the posterior variance of approximately $0.2$ suggests some caution in the claim.  However, regardless of our confidence in 
the posterior claim for the missing data, the estimate of $\theta$ remains unbiased.

We can, perhaps more usefully, describe the output of the Gibbs sampler graphically, as done in the two-panel 
Figure~\ref{Gibbs.Demo.Figure}.  Here we have a traceplot that shows the time-series of the Markov chain path and a histogram 
of the last 30,000 draws.  One advantage to describing posterior distributions with MCMC is that summary statistics are trivial 
to generate since we can calculate various quantities directly on the empirical draws.  The 95\% highest posterior density 
region was produced from taking a histogram with a large number of bins (300 rather than the 100 shown in the graph) and 
sorting the density values observing where the thresholds of interest fall.  Naturally this is just an approximation since a 
histogram is not a perfectly smooth density, but the parameters are sufficiently adjustable for 30,000 draws that it can be a 
very accurate approximation.  Importance sampling can also be used to produce HPD regions as described on 
page~\pageref{importance.sampling.hpd}.

\index{subjectindex}{example!terrorist events in Britain}\index{subjectindex}{changepoint problem}
\begin{examplelist} \label{tweed.example}
	\item	{\bf Changepoint Analysis for Terrorism in Great Britain: 1970-2004.}
            This example looks at a series of terrorist events over a 35 year period in the U.K. in which there were injuries and/or
            fatalities.  This period, 1970 to 2004, is dominated by events related to the ``the troubles'' in Northern Ireland and
            the Provisional IRA's objective of influencing British policy by bombing in England.  The beginning date is selected
            to roughly coincide with three important events: the split of the IRA into the Official IRA and the Provisional IRA
            (1969), the start of internment for IRA members (1971), and the ``Bloody Sunday'' march in which British paratroopers
            killed 14, and injured 13, demonstrators.  These data are subsetted from TWEED (``Terrorism in Western Europe: Events
            Data'') compiled by Jan Oskar Engene and made available at \texttt{http://folk.uib.no/sspje/tweed.htm} as well as in
            \texttt{BaM}.  The data are characterized by relatively high counts in the early era and relatively low counts in the
            late era.  Thus the question is when was there a pronounced change in terrorism rates?  The data are shown in 
            Table~\ref{Tweed.Table}.
        
        \vspace{-1pt}
		\begin{table}[h]
        \parbox[c]{\linewidth}{ \hspace{0.21in}
        	\tabletitle{\textsc{Count of Terrorism Incidents in Britain, 1970-2004 by Rows}}\label{Tweed.Table}
        	\vspace{11pt}
        	\renewcommand{\arraystretch}{1.2}
		        \begin{tabular}{llllllllllll}
			        1&21&23&9&15&17&18&5&4&8&4&1 \\
                    5&2&3&0&1&2&3&3&6&5&3&2 \\
                    3&0&1&1&1&1&0&0&0&0&0 & \\
		        \end{tabular}
        }
		\end{table}
        \vspace{-11pt}
		
		Statistically, the objective is to use this sequence to estimate the \emph{changepoint} and also to obtain 
		posterior estimates of the intensity parameters of the two separate Poisson processes
		\index{subjectindex}{Poisson process} (Poisson because these are counts).  This process is addressed more 
		generally for an unknown number of changepoints by Phillips\index{authorindex}{Phillips, D. B.} and 
		Smith\index{authorindex}{Smith, A. F. M.} (1996).
		
		Specifically, $x_1, x_2,\ldots, x_n$ are a series of count data where there exists the possibility of a 
		changepoint at some period, $k$, along the series.  Therefore there are two Poisson data-generating processes:
		\begin{align}
    		x_i|\lambda &\sim \mathcal{P}(\lambda) \qquad i=1,\ldots,k       \nonumber\\
    		x_i|\phi    &\sim \mathcal{P}(\phi) \qquad i=k+1,\ldots,n,       \nonumber
		\end{align}
		where the determination of which to apply depends on the location of the changepoint $k$.  So now there are 
		three parameters to estimate: $\lambda$, $\phi$, and $k$.  This problem is distinguished by the added 
		complexity that one of the parameters, $k$, operates in a different capacity on the others: determining a 
		change in the serial data generation process, rather than as a conventional parametric input.
		
		The three independent priors applied to this model are:\index{subjectindex}{distribution!gamma}
		\begin{align}
    		\lambda &\sim \mathcal{G}(\alpha,\beta)          \nonumber\\
    		\phi    &\sim \mathcal{G}(\gamma,\delta)            \nonumber\\
    		k       &\sim \text{discrete uniform on} [1, 2,\ldots, n],  \nonumber
		\end{align}
		where for purposes of this example, the prior parameters are assigned according to: $\alpha=4$, $\beta=1$, $\gamma=1$, 
		$\delta=2$.  Since the mean of a gamma distribution is the ratio of its parameters (in the scale version of the
        gamma distribution), this assignment of parameters roughly is close to, but slightly larger than, the mean of 
		the first 50\% of the data ($\alpha\beta$), and the second 50\% of the data ($\gamma\delta$).  This leads to the 
		following joint posterior and its proportional simplification:
		\begin{align}
    			\pi(&\lambda,\phi,k|\y) \propto 
				L(\lambda,\phi,k|\y)\pi(\lambda|\alpha,\beta)\pi(\phi|\gamma,\delta)\pi(k)
                           	\nonumber\9
                            &=\left( \prod_{i=1}^{k} \frac{e^{-\lambda}\lambda^{y_i}}{y_i!} \right)
                             	\left( \prod_{i=k+1}^{n} \frac{e^{-\phi}\phi^{y_i}}{y_i!} \right)
                             	\left( \frac{\beta^\alpha}{\Gamma(\alpha)}\lambda^{\alpha-1}e^{-\beta\lambda} \right) 
				\nonumber \\
                            &\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad \times
				\left( \frac{\delta^\gamma}{\Gamma(\gamma)}\phi^{\gamma-1}e^{-\delta\phi} \right)
                             	\frac{1}{n}\nonumber\9
                            &\propto \lambda^{\alpha-1+\sum_{i=1}^{k}y_i} \phi^{\gamma-1+\sum_{i=k+1}^{n}y_i}
                           	\exp[-(k+\beta)\lambda -(n-k+\delta)\phi ].
                            	\nonumber
		\end{align}
		The last line easily provides the full conditional distribution for $\lambda$ and $\phi$:
		\begin{align}
    			\lambda|\phi,k &\sim \mathcal{G}(\alpha+\sum_{i=1}^{k}y_i,\beta+k)      	\nonumber\\
    			\phi|\lambda,k &\sim \mathcal{G}(\gamma+\sum_{i=k+1}^{n}y_i,\delta+n-k),         \nonumber
		\end{align}
		but the full conditional distribution for $k$ requires more work.  Start with the likelihood function
		under the assumption that $\lambda$ and $\phi$ are fixed and rearrange:
		\begin{align}
			p(\y|k,\lambda,\phi) 
                              &= \left( \prod_{i=1}^{k} \frac{e^{-\lambda}\lambda^{y_i}}{y_i!} \right)
                             	 \left( \prod_{i=k+1}^{n} \frac{e^{-\phi}\phi^{y_i}}{y_i!} \right)	\nonumber\9
			      &= \left(\prod_{i=1}^{n}\frac{1}{y_i!}\right) e^{k(\phi-\lambda)} e^{-n\phi}
				 \lambda^{\sum_{i=1}^{k}y_i} \left(\prod_{i=k+1}^{n}\phi^{y_i}\right)
				 \left(\prod_{i=1}^{k}\frac{\phi^{y_i}}{\phi^{y_i}}\right)		\nonumber\9
			      &= \left( \prod_{i=1}^{n}\frac{e^{-\phi}\phi^{y_i}}{y_i!} \right) 
				 \left( e^{k(\phi-\lambda)} \left(\frac{\lambda}{\phi}\right)^{\sum_{i=1}^{k}y_i} \right) \nonumber\9
			      &= f(\y,\phi)L(\y|k,\lambda,\phi).
		\end{align}
		What this does is provide two functions, the first of which is free of $k$.  Since our objective is simply a full 
		conditional statement for $k$, we will use only the modified likelihood function that contains $k$, 
		$L(\y|k,\lambda,\phi)$, after a cancellation below.  Suppressing the conditioning on $\lambda$ and $\phi$ for
		notational clarity only, apply Bayes' Law with a generic prior on $k$, $p(k)$:
		\begin{align}
			p(k|\y) &= \frac{ f(\y,\phi) L(\y|k)p(k) }{ \sum_{\ell=1}^{n} f(\y,\phi) L(\y|k_\ell)p(k_\ell) } \nonumber\9
			        &= \frac{ L(\y|k)p(k) }{ \sum_{\ell=1}^{n} L(\y|k_\ell)p(k_\ell) }.
		\end{align}	
		Here we took advantage of the discrete feature of $k$ and summed over all possible values with
		the index $\ell$.  For simple priors like our discrete uniform, this simplifies even more such that with
		proportionality now $p(k|\y) = L(\y|k)/\sum_{\ell=1}^{n} L(\y|k_\ell)$.  So each iteration of the Gibbs sampler
		will calculate an $n$-length probability vector for $k$ and draw a value accordingly.  This is implemented in the
		following \R\ code:
		\begin{R.Code}
cp.gibbs <- function(theta.matrix,y,a,b,g,d)  \{
    n <- length(y); y.bar <- mean(y)
    k.prob <- rep(0,length=n)
    for (i in 2:nrow(theta.matrix))  \{
        lambda <- rgamma(1,a+sum(y[1:theta.matrix[(i-1),3]]), 
            rate=b+theta.matrix[(i-1),3])
        phi    <- rgamma(1,g+sum(y[theta.matrix[(i-1),3]:n]), 
            rate=d+length(y)-theta.matrix[(i-1),3])
        for (j in 1:n)  k.prob[j] <- exp( j*(phi-lambda) + 
				                          log(lambda/phi)*sum(y[1:j]-y.bar/j) )
        k.prob <- (lambda/phi)^y.bar*k.prob
        k      <- sample(1:n,size=1,prob=k.prob)
        theta.matrix[i,] <- c(lambda,phi,k)
    \}
    theta.matrix
\}
		\end{R.Code}
		There are a few subtleties in the code.  In the second for-loop the data \texttt{y[]} are mean-centered
		and this is undone in the next line to increase numerical stability (large count values can send entries
		of \texttt{k.prob} to infinity).  Notice also that the function does not normalize the \texttt{k.prob}
		vector.  This is because the \texttt{sample} function will do it automatically in cases where the vector
		values do not sum to one.  

        \begin{table}[h!]
        \parbox[c]{\linewidth}{ \hspace{0.15in}
                \blstable
                \tabletitle{\textsc{Gibbs Sampler Draws for $k$ Parameter}}\label{kappa.draws}
				\begin{tabular}{lrrrrrr}
					Value &    7 &   8 &  9 & 10 &  11 & 21 \\
					\hline
					Count & 4352 & 330 & 10 & 283 & 24 &  1 
				\end{tabular}\bls
        }
        \end{table}

		The function is run for 10,000 iterations retaining only the last 5,000 chain values, which are summarized 
		in Table~\ref{coal.table}.  This is a conservative strategy with this simple model and there is no
		evidence of non-convergence for the Markov chain.  The quantile results for $k$ seem confusing at first
		until one remembers that $k$ is a discrete variable.  The Markov chain therefore jumps from integer value
		to integer value.  In fact, the chain had strong ``preferences'' as indicated by the values shown in
        Table~\ref{kappa.draws}.  The Markov chain was started with $k=21$, which explains that isolated value.

		\blstable
		\begin{table}[h!]
        \parbox[c]{\linewidth}{
		    \tabletitle{\textsc{Terrorism in Great Britain Model, Posterior Summary}}\label{coal.table} 
		    \vspace{0.07in}
		    \begin{tabular}{lrrrrrr}
 		    Quantile       & $\quad$ & $\lambda$ & $\quad$ & $\phi$    & $\quad$ & $k$               \\
		    \hline
 		    Minimum        & $\quad$ & 8.61      & $\quad$ & 1.39      & $\quad$ & 7.00              \\
 		    First quartile & $\quad$ & 12.40     & $\quad$ & 2.60      & $\quad$ & 7.00              \\
 		    Median         & $\quad$ & 13.30     & $\quad$ & 2.85      & $\quad$ & 7.00              \\
 		    Third quartile & $\quad$ & 14.40     & $\quad$ & 3.09      & $\quad$ & 7.00              \\
 		    Maximum        & $\quad$ & 18.90     & $\quad$ & 4.10      & $\quad$ & 11.00             \\
 		    \hline
 		    Mean           & $\quad$ & 13.30     & $\quad$ & 2.83      & $\quad$ & 7.29              \\
		    \end{tabular}
        }
		\end{table}\bls

		Figure~\ref{Gibbs.Fig} shows the first 100 iterations of the sampler in a pair of two-dimensional graphs where 
		$k$ is depicted on the y-axis in both cases plotted against $\lambda$ and $\phi$, respectively.  Notice that the 
		Gibbs sampler converges quickly to the region of the reported posterior.

		Each movement is a straight line here since the intermediate steps of the Gibbs sampler hold all other parameter 
		values constant while sampling for a single parameter conditioned on these other values.  The $k$ dimension is
		jittered slightly to reveal the concentration at $k=7$.
		\index{subjectindex}{Gibbs sampler!mapping of steps}  So looking at the first step in the left-hand side panel of 
		Figure~\ref{Gibbs.Fig}, we begin by conditioning on $k=21$ and move from the starting value $\lambda=1.0$ to a 
		sampled value of $6.447$.  Then the second half of the first iteration holds this value for $\lambda$ constant and 
		draws a value for $k$ of $10$.  After these two steps, the first full iteration of the sampler is done.  This 
		process then continues for $4999$ more iterations.  Similarly, in the second panel the sampler goes from the same 
		starting point to $\phi=2.084$.  Note that the algorithm proceeds rapidly to a more preferred region.

		\begin{figure}[h!]
		\parbox[r]{\linewidth}{ \hspace{-0.09in}
			\epsfig{file=Images/mcmc_intro.figure02.ps,width=5.8in,height=4.95in,clip=,angle=0} }
            \vspace{-15pt}
			\caption{\textsc{Gibbs Sampling Path for Terrorism Events}}  \label{Gibbs.Fig} 
		\end{figure}

	\item	{\bf Changepoint Analysis for Military Fatalities in Afghanistan.}
		\index{subjectindex}{example!military fatalities in Afghanistan}
		A more topical example comes from looking at military fatalities among coalition forces in Afghanistan for 
		\emph{Operation Enduring Freedom}, the ongoing effort to depose and defeat the Taliban government forces that
		had sheltered Al Queda.  The monthly count data covers the period from October 2001 to January 2007 and are culled 
		from U.S. military and NATO sources, and are provided in Table~\ref{afghan.death.table} for 52 monthly periods, listed
        by rows.	

	        \begin{center}
                \blstable
                \begin{table}[h!]
		\parbox[l]{\linewidth}{ \hspace{12pt}
                \tabletitle{\textsc{NATO Fatalities in Afghanistan, 10/01 to 1/07}}\label{afghan.death.table} \vspace{0.07in}
		\begin{tabular}{rrrrrrrrrrrrrrrrrrrrr}
   		     3 &  5 &  4 & 10 & 12 & 14 &  9 &  1 &  3 &  0 &  3 &  1 &  6 &  1 &  8 &  4 &  7 & 12 &  2 & 2 & 7 \\
		     2 &  4 &  2 &  6 &  8 &  1 & 11 &  2 &  3 &  3 &  9 &  5 &  2 &  3 &  4 &  8 &  7 &  1 &  3 & 2 & 6 \\
		    19 &  4 & 29 &  2 & 33 & 12 & 10 &  7 &  4 &  0 &    &    &    &    &    &    &    &    &    &   &   \\
		\end{tabular}
		}
		\end{table}
		\end{center}
\newpage
        There is strong evidence that the resurgent Taliban made a strategic decision at some point about the possibility of
        winning and correspondingly escalated their activities.  Naturally, we cannot know for certain the exact timing of such a
        decision, but it may be possible to estimate the timing by looking for a changepoint in coalition fatalities.  Looking at
        the data for only a short period of time provides some idea that there is a measurable difference in the early periods and
        the later periods.  In fact, the mean of the first 40 periods is about 4.95 and the last ten periods 10.66.  Using these
        means we construct gamma priors for $\lambda$ and $\phi$ with $\alpha=1$, $\beta=2$, $\gamma=4$, and $\delta=4$.  The same
        algorithm from the Terrorism in Great Britain example is used, producing marginal posteriors summarized in
        Table~\ref{afghan.results.table}.

		\blstable
		\begin{table}[h!]
        \parbox[c]{\linewidth}{ 
		    \tabletitle{\textsc{Afghan Fatalities Posterior}}\label{afghan.results.table} \vspace{0.07in}
            \begin{tabular}{lcccccc}
 		    Quantile       & $\quad$ & $\lambda$ & $\quad$ & $\phi$ & $\quad$ & $k$                  \\
		    \hline
 		    Minimum        & $\quad$ & 3.675     & $\quad$ & 6.258    & $\quad$ & 31.00             \\
 		    First quartile & $\quad$ & 4.480     & $\quad$ & 8.533    & $\quad$ & 31.00             \\
 		    Median         & $\quad$ & 4.703     & $\quad$ & 9.035    & $\quad$ & 42.00             \\
 		    Third quartile & $\quad$ & 4.913     & $\quad$ & 9.590    & $\quad$ & 42.00             \\
 		    Maximum        & $\quad$ & 5.755     & $\quad$ & 12.532   & $\quad$ & 44.00             \\
 		    \hline
 		    Mean           & $\quad$ & 4.703     & $\quad$ & 9.057    & $\quad$ & 41.49             \\
		    \end{tabular}
        }
        \end{table}\bls

		This turns out to be quite revealing.  The posterior mean for the later period intensity parameter is about
		twice the size of that for the early period.  This supports the popular conception that the war has become
		more ``hot'' in more recent years.  Also, it appears that there is strong evidence that the change occurred
		around the 42nd month, which is March of 2005.  Figure~\ref{Afghan.Fig} shows that the posterior for
		the changepoint is highly concentrated, indicating that the data are strongly asserting a point around
		40 (recall that the prior for $k$ was uniform over the full range of months).

		\begin{figure}[h]
		\parbox[l]{\linewidth}{ \hspace{0.20in} 
			\epsfig{file=./Images/mcmc_intro.figure05.ps,height=5.41in,width=2.55in,clip=,angle=270} }
		\caption{\textsc{Afghan Fatalities Posterior Histogram, HPD Region}} \label{Afghan.Fig}
		\vspace{-11pt}
		\end{figure}

		These changepoint models have many varieties and extensions.  The classic, early article by Berry and Hartigan 
		\index{authorindex}{Berry, D.} \index{authorindex}{Hartigan, J. A.} (1993) is an excellent starting point.  
		Other common parametric forms for the outcomes are normal (Menzefricke 1981,\index{authorindex}{Menzefricke, U.} 
		Skates, Pauler, and Jacobs \index{authorindex}{Skates, S. J.} \index{authorindex}{Pauler, D. K.} 
		\index{authorindex}{Jacobs, I. J.} 2001) and binomial/negative binomial (Zhao and Chu \index{authorindex}{Zhao, X.} 
		\index{authorindex}{Chu, P. S.} 2006, Smith 1975).  \index{authorindex}{Smith, A. F. M.}  While we have only 
		covered one changepoint here in two examples, it is common to have multiple changepoints that need to be estimated 
		and sometimes an unknown number of them (Chib 1998,\index{authorindex}{Chib, S.} Yang and Kuo 2001, 
		\index{authorindex}{Yang, T. Y.} \index{authorindex}{Kuo, L.} Chen and Gupta 1997, \index{authorindex}{Chen J.} 
		\index{authorindex}{Gupta A. K.} Braun, Braun, and Muller 2000, \index{authorindex}{Braun J. V.} 
		\index{authorindex}{Braun R. K.} \index{authorindex}{Muller H. G.} Stephens 1994, 
		\index{authorindex}{Stephens, D. A.} Suchard \etal 2003, Fearnhead 2006, Green 1995). 
		\index{authorindex}{Suchard, M. A.}\index{authorindex}{Fearnhead, P.} \index{authorindex}{Green, P. J.}
        Finally, see the paper by Carlin \etal (1992) for a hierarchical model implementation of changepoint models.
\end{examplelist}
		
\subsection{Summary of Properties of the Gibbs Sampler}
We finish this section with a summary of the properties of the Gibbs sampler that make it the most commonly used MCMC kernel 
specification.\index{subjectindex}{Gibbs sampler!summary of properties} These properties are covered in greater detail in 
Chapter~\ref{MCMC.Theory.Chapter}, for those interested in the underlying technical issues.
\begin{bayeslist}
    \item   Since the Gibbs sampler conditions on values from the last iteration of its chain values, it clearly constitutes 
	    a Markov chain.

    \item   The Gibbs sampler has the true posterior distribution of the parameter vector as its limiting distribution. 

    \item   The Gibbs sampler is a homogeneous Markov chain: the consecutive probabilities are independent of $n$, the current 
            length of the chain.

    \item   The Gibbs sampler converges at a geometric rate: the total variation distance between an arbitrary time and the 
	    point of convergence decreases at a geometric rate in time ($t$).  

    \item   The Gibbs sampler is an ergodic Markov chain.
\end{bayeslist}
See Roberts\index{authorindex}{Roberts, G. O.} and Polson\index{authorindex}{Polson, N. G.} (1994) and Tierney
\index{authorindex}{Tierney, L.} (1994) for the (very general) conditions.  Of
course the number of iterations required is still a function of the complexity of the model (hierarchies, hyperprior assignments, 
data characteristics, etc.), and therefore a motivation for having convergence diagnostics (Chapter~\ref{MCMC.Utilitarian.Chapter}).
Casella\index{authorindex}{Casella, G.} and George\index{authorindex}{George, E. I.} (1992) provide a very clear basic introduction 
to the Gibbs sampler and its properties (the original title of the conference paper was ``Gibbs for Kids''), a piece of which we 
have already seen in Chapter~\ref{Intro.Chapter}.  Also, a very useful discussion of the Gibbs sampler and its relation to other 
MCMC techniques is provided by Smith\index{authorindex}{Smith, A. F. M.} and Roberts\index{authorindex}{Roberts, G. O.} (1993).
\index{subjectindex}{Gibbs sampler|)}

\index{authorindex}{Geman, S.} \index{authorindex}{Geman, D.} 
Credit for introducing the Gibbs sampler on finite state spaces is usually given to Geman and Geman (1984), 
but Ulf Grenander \index{authorindex}{Grenander, U.} (a student of 
Harald Cram\'{e}r) actually applied it to Bayesian modeling in a well-known but unpublished paper in 1983.  Early restricted 
versions, typically labeled as the \emph{heatbath algorithm} in statistical physics can be found in Creutz (1979), Creutz, Jacobs 
and Rebbi (1983), and Ripley (1979).
\index{authorindex}{Creutz, M.} \index{authorindex}{Jacobs, L.} \index{authorindex}{Rebbi, C.} \index{authorindex}{Ripley, B. D.}

\section{The Metropolis-Hastings Algorithm}\index{subjectindex}{Metropolis-Hastings algorithm|(}
The full set of conditional distributions for the Gibbs sampler are often quite easy to specify from the hierarchy of the model 
since conditional relationships are directly articulated in such statements.  However, the Gibbs sampler of Geman
\index{authorindex}{Geman, S.}\index{authorindex}{Geman, D.} and Geman (1984) obviously does not work when the complete conditionals 
for the $\T$ parameters do not have an easily obtainable form.  In these cases a chain can be produced for these parameters using 
the Metropolis-Hastings algorithm\index{subjectindex}{Metropolis-Hastings algorithm} from statistical physics 
(Chib\index{authorindex}{Chib, S.} and Greenberg\index{authorindex}{Greenberg, E.} 1995; Metropolis\index{authorindex}{Metropolis, N.} 
\etal 1953; Hastings\index{authorindex}{Hastings, W. K.} 1970; Peskun\index{authorindex}{Peskun, P. H.} 1973) which is often applied 
in fields completely unrelated to the original application (e.g., Cohen\index{authorindex}{Cohen, J.} \etal 1998).  

\subsection{Background}\index{subjectindex}{Metropolis-Hastings algorithm!origins}
The original work by Metropolis\index{authorindex}{Metropolis, N.} \etal postulated a two-dimensional enclosure with $n=10$ molecular 
particles, and sought to estimate the state-dependent total energy of the system at equilibrium. The central problem that they 
confronted is that there is an incredibly large number of locations for the molecules in the system that must be accounted for and 
this number grows exponentially with time.  The key contribution of Metropolis\index{authorindex}{Metropolis, N.} \etal is to model 
the system by generating moves that are more likely than others based on positions that are calculated using uniform probability 
generated candidate jump points.  The moves are accepted probabilistically and likely final states are determined after a set of 
periods where many such decisions are made.  Therefore the simulation produces an estimated force based on a statistical, rather than 
deterministic, arrangement of particles.  The critical assumptions are already familiar to us: any molecular state can be reached 
from another (ergodicity), and state changes are induced probabilistically with an instrumental distribution. The result, after 
convergence, is a distribution of particles from which energy calculations can be made.

%Hastings (1970) as well as Peskun\index{authorindex}{Peskun, P. H.} (1973) generalized the 
%Metropolis\index{authorindex}{Metropolis, N.} \etal algorithm by suggesting that other 
%distributions such as the normal or Poisson could be used to provide potential 
%jumping positions for each element of the chain.  They both showed that the 
%state space for the algorithm could include continuous and discrete forms, which 
%is a vast improvement in the statistical applicability.  In addition, a limiting 
%restriction of the original Metropolis algorithm was that the candidate-generating 
%distribution used to suggest potential jumping points had to be symmetrical in the 
%original 1953 construct.  This means that for two points in the state space, 
%$\theta_1$ and $\theta_2$, $q(\theta_1|\theta_2) = q(\theta_2|\theta_1)$.
%However, Hastings showed that the bias introduced by introducing asymmetry can be 
%compensated for in the acceptance ratio; see \eqref{MH.acceptance.ratio} below.
%\index{subjectindex}{asymmetric candidate distribution}\index{bias}

\subsection{Description of the Algorithm}
The simplest Metropolis-Hastings algorithm for a single selected parameter vector works as follows.  Suppose we have a $J$-length 
parameter vector, $\T \in \TT^J$ to estimate, with $J$ determining the dimension of the state space and the posterior of 
interest, $\pi(\theta)$.  At the $t^{th}$ step of the Markov chain when the chain is at the position $\T$, draw $\T'$ from a 
distribution over the same support.  This distribution is called the instrumental, jumping, or proposal distribution 
\index{subjectindex}{Metropolis-Hastings algorithm!instrumental distribution}
\index{subjectindex}{Metropolis-Hastings algorithm!jumping distribution (see instrumental)}
\index{subjectindex}{Metropolis-Hastings algorithm!proposal distribution (see instrumental)}
and is denoted $q_t(\T'|\T)$.  There is an obvious dependency on the data in both the posterior and the candidate-generating forms, 
but we will suppress this for notational clarity.  Note that, unlike the Gibbs sampler, we are producing a multidimensional candidate
value all at once and not serially throughout the $j=1,\ldots,J$.  It must be possible to determine the reverse function value, 
$q_t(\T|\T')$, and under the original constraints of Metropolis\index{authorindex}{Metropolis, N.} \etal the two conditionals need 
to be equal (symmetry), although we now know that this is not necessary.  \index{subjectindex}{Metropolis-Hastings algorithm!symmetry}

Define the \emph{acceptance ratio} (sometimes called the acceptance function) to be the following:
\index{subjectindex}{Metropolis-Hastings algorithm!acceptance ratio}
\begin{equation}\label{MH.acceptance.ratio}
	a(\T',\T) =  \frac{\pi(\T')}{\pi(\T)} \frac{q_t(\T|\T')}{q_t(\T'|\T)}.
\end{equation}
At time $t$, the decision that produces the $t+1^{st}$ point in the chain is probabilistically determined according to: 
\renewcommand{\arraystretch}{4} 
\begin{equation}\label{Metropolis.Hastings.Rule}
        \T^{[t+1]} =
        \begin{cases}
                \T' & \text{with probability}     \qquad \min( a(\T',\T^{[t]}), 1) \\[12pt]
                \T^{[t]} & \text{with probability}\qquad 1-\min( a(\T',\T^{[t]}), 1).
        \end{cases}
\end{equation} \renewcommand{\arraystretch}{1}
\noindent Several features of this algorithm are interesting.  Observe that because of the ratio in the decision above, we only 
have to know the posterior distribution $\pi(\T)$, up to a constant of proportionality.  Note also that unlike the Gibbs 
sampler, the Metropolis-Hastings algorithm does not necessitate movement on every iteration: the next chosen location may be the
current location.  Finally, it is easy to see that in the case of symmetry in the candidate-generating density, 
\index{subjectindex}{Metropolis-Hastings algorithm!symmetry} $q_t(\T|\T')=q_t(\T'|\T)$, the acceptance ratio simplifies to
a ratio of just the posterior densities at the two points, which is the original 1953 construct.  

We can describe a single Metropolis-Hastings iteration from the symmetric form of (\ref{Metropolis.Hastings.Rule}) in the following 
steps: \index{subjectindex}{Metropolis-Hastings algorithm!steps}
\begin{enumerate}
    \item   Sample $\T'$ from $q(\T'|\T)$, where $\T$ is the current location.
    \item   Sample $u$ from $u[0:1]$.
    \item   If $a(\T',\T) = \pi(\T')/\pi(\T) > u$ then accept $\T'$.
    \item   Otherwise keep $\T$ as the new location.
\end{enumerate}
Obviously we want to choose the $q()$ distribution so that it is easy to sample from, but it is also important that 
$\pi(\T')/q(\T'|\T)$ is fully known up to some arbitrary constant independent of $\T$.  So three things can happen 
here: we can sample a value of higher density and move with probability one, we can sample a value of lower density but move anyway 
by drawing a small uniform random variable in Step 2 above, or we can draw a uniform random variable larger than the ratio of 
posteriors and therefore stay in the same location.  One interesting feature of the Metropolis-Hastings algorithm, in contrast to 
EM, is that it is ``okay'' to move to lower density points, albeit probabilistically.  So the algorithm will always move to higher 
density points but will move to lower density points probabilistically based on the ratio difference between the current point and 
the proposal.  This algorithm describes the full posterior density, so it is necessary at times to move from a high-density point 
to a low-density point to traverse the space.  Conversely, the EM algorithm never makes this kind of decision and therefore is 
only a mode finder rather than a method of fully sampling from the target distribution.\index{subjectindex}{Metropolis-Hastings 
algorithm!compared to EM}

It can be shown that the Gibbs sampler is a special case of Metropolis-Hastings where the probability of accepting the candidate
value is always one (Gelman\index{authorindex}{Gelman, A.} 1992, 436; Tanner\index{authorindex}{Tanner, M. A.} 1996, p.182).
However, it is a Metropolis-Hastings algorithm where the full conditionals are required, and therefore is more restrictive in one
sense (Gamerman\index{authorindex}{Gamerman, D.} and Lopes\index{author}{Lopes, H. F.} (2006), Besag 
\index{authorindex}{Besag, J.} \etal 1995, Tierney\index{authorindex}{Tierney, L.} 1991).  Furthermore, Billera and Diaconis
\index{authorindex}{Billera, L. J.} \index{authorindex}{Diaconis, P.} (2001) show that the Metropolis-Hastings algorithm is
the optimal variant from those in its ``natural class of related algorithms,'' which would obviously include tools like the Gibbs
sampler.

\subsection{Metropolis-Hastings Properties} \index{subjectindex}{Metropolis-Hastings algorithm!chain properties}
Consider a posterior of interest, $\pi(\T)$ (suppressing conditionalities for notational convenience) with $\int\pi(\T) d\T=1$, 
for $\T$ on some state space, $\mathbf{S}$, which is defined on a d-dimensional Lebesgue measure: $\Omega \subseteq \Re^d$.  The 
motivation for seeking an iterative solution through MCMC techniques is that this $\pi(\T)$ distribution is analytically 
complicated or unwieldy (Gelman\index{authorindex}{Gelman, A.} 1992), so we want a procedure that eventually arrives at this 
distribution through simulation.  The Metropolis-Hastings algorithm provides this function with a two-part transition kernel 
that has the property that it is closed with respect to the limiting distribution of $\pi(\T)$:
\begin{equation}\label{reversibility.equation}
    \pi(\T)p(\T,\T') =  \pi(\T')p(\T',\T) \qquad \forall \T,\T' \in \Omega,
\end{equation}
where $p(a,b)$ defines a transition kernel\index{subjectindex}{Metropolis-Hastings algorithm!transition kernel}
from state $a$ to state $b$.  This is called the \emph{reversibility} condition for $p()$  
\index{subjectindex}{reversibility} and also the \emph{detailed balance equation}.\index{subjectindex}{detailed balance equation}
The detailed balance equation guarantees that the Markov chain will eventually reach a single limiting distribution.  The values 
for $\pi$ are simply the posterior distribution evaluated at the two points: $\T$ and $\T'$, and $p(\T,\T')$ is 
the appropriately sized transition kernel from $\T$ to $\T'$.  Robert\index{authorindex}{Robert, C. P.} and Casella
\index{authorindex}{Casella, G.} (2004, Chapter~7) provide a proof that under very general conditions, virtually any conditional 
distribution over the appropriate support will provide a candidate jumping distribution
\index{subjectindex}{Metropolis-Hastings algorithm!candidate distribution} that provides a Metropolis-Hastings chain that will 
eventually converge to the limiting distribution of $\pi(\T)$ provided that (\ref{reversibility.equation}) holds.  This is 
the key theoretical importance of the algorithm because it shows that the right thing \emph{will} happen if we let the chain run 
long enough.

A less general, and therefore less useful, condition is \emph{symmetry}: $p(\T,\T') = p(\T',\T)$.
\index{subjectindex}{symmetry}  This was originally the specified requirement related to the distributional relationship between 
the two points, but the contribution of Hastings \index{authorindex}{Hastings, W. K.} (1970) was to demonstrate that this is not 
strictly necessary and reversibility can be substituted.  

\subsection{Metropolis-Hastings Derivation}
There are really two parts to the transition kernel, a jumping density $q(\T'|\T)$, and a jumping probability $d(\T,\T')$ 
(or jumping decision):
	\index{subjectindex}{Metropolis-Hastings algorithm!transition kernel!two parts}
	\index{subjectindex}{jumping density (proposal density)}
	\index{subjectindex}{jumping probability}
\begin{equation}\label{jumping.equation.1}
    p(\T,\T') = q(\T'|\T)d(\T,\T'),
\end{equation}
which determine the distribution of new $\T'$ values to move to and the probability of making such a move, respectively.  The 
distribution used for $q(\T'|\T)$ is arbitrary, but it must be straightforward to draw from.  We sample from $q(\T'|\T)$ to get 
potential values of the chain to jump to.  The form of this jumping distribution \index{subjectindex}{jumping density (proposal 
density)} plays only a part in determining values that we might jump to, and therefore it is not the \emph{decision} to jump. By 
symmetric logic we can also define the reverse jump:
\begin{equation}\label{jumping.equation.2}
    p(\T',\T) = q(\T|\T')d(\T',\T).
\end{equation}
The decision to jump or not represents a second level of randomization as determined by the probability $d(\T,\T')$.  The 
candidate jumping point is favored if its probability is large relative to the posterior probability associated with remaining 
at the current point. 

A decision rule can be derived by starting with (\ref{jumping.equation.1}) solved for $d(\T,\T')$: 
\index{subjectindex}{Metropolis-Hastings algorithm!decision (acceptance) rule}
\begin{equation}
    d(\T,\T') = \frac{ p(\T,\T') }{ q(\T'|\T) },    \nonumber
\end{equation}
and then inserting (\ref{reversibility.equation}) solved for $p(\T,\T')$:
\begin{equation}
    d(\T,\T') = \frac{ \pi(\T')p(\T',\T) }{ \pi(\T) q(\T'|\T) }. \nonumber
\end{equation}
Using $p(\T',\T) = q(\T|\T')d(\T',\T)$, this can be arranged as:
\begin{equation}\label{hastings.formulation}
    \frac{ d(\T,\T') }{ d(\T',\T) } = \frac{ \pi(\T')q(\T|\T') }{ \pi(\T) q(\T'|\T) }.
\end{equation}
An acceptance rule (Hastings 1970)\index{authorindex}{Hastings, W. K.} that meets this criteria and accommodates the situation 
where we require for a jump $d(\T',\T) > d(\T,\T')$ is:
\begin{equation}\label{final.MH.form}
    \alpha(\T',\T) = \text{min}\left[
                        \frac{ \pi(\T')  q(\T|\T') }
                             { \pi(\T) q(\T'|\T) }, 1 \right].
\end{equation}
This is exactly (\ref{Metropolis.Hastings.Rule}) restated where $q(\T',\T) = q_t(\T|\T')$ and 
$q(\T,\T') = q_t(\T'|\T)$.  If we were willing to substitute symmetry for reversibility, we would get the 
original simplified rule from Metropolis\index{authorindex}{Metropolis, N.} \etal which is similar, but much more intuitive:
\begin{equation}
\alpha(\T',\T) = \text{min}\left[
                        \frac{ \pi(\T') }
                             { \pi(\T)  }, 1 \right],
\end{equation}
because of cancellation.  This states that the chain will move with probability one in a direction of higher posterior probability if 
offered by the jumping distribution,\index{subjectindex}{jumping density (proposal density)} and will move with probability 
$a=\pi(\T')/\pi(\T)$ to the new point otherwise.  Therefore low values of $a$ will often result in staying at the same chain 
value for that dimension.  Due to the two levels of randomization, three things can now happen during each chain iteration: move to the 
new point with probability one, generate a uniform random variable (bounded by zero and one) that is less than $\alpha(\T',\T)$ 
thus moving to the new point, or generate a uniform random variable greater than $a$ and then stay at the current point.

Furthermore, Chib and Greenberg (1995, pp.328-329) \index{authorindex}{Chib, S.} \index{authorindex}{Greenberg, E.} give a very nice 
intuitive way to justify the detailed balance equation.  Suppose \eqref{reversibility.equation} was ``out of balance'' in that:
\index{subjectindex}{detailed balance equation}
\begin{equation}
    \pi(\T)p(\T,\T') >  \pi(\T')p(\T',\T).
\end{equation}
This means that the algorithm moves from $\T$ to $\T'$ more often than it should and moves from $\T'$ to $\T$ less often than it should.  
We know from \eqref{final.MH.form} that the decision to jump to $\T'$ is really governed by $p(\T,\T') = q(\T'|\T)\alpha(\T',\T)$ (the 
probability of drawing $\T'$ times the probability of deciding to accept it), so we really need to make $\alpha(\T,\T')$ large enough to 
rectify:
\begin{equation}
    \pi(\T)q(\T'|\T)\alpha(\T',\T) >  \pi(\T')q(\T|\T')\alpha(\T,\T'),
\end{equation}
meaning
\begin{equation}
\alpha(\T',\T) >  \frac{ \pi(\T')q(\T|\T') }{ \pi(\T)q(\T'|\T) } \alpha(\T,\T').
\end{equation}
Since $\alpha(\T,\T')$ is a probability function, it is bounded by zero and one.  Therefore the right-hand-side
that makes the inequality into an equality is the minimum value of either: $\pi(\T')q(\T|\T')/\pi(\T)q(\T'|\T)$
(when $\alpha(\T,\T')$ has to equal one), or one (when $\alpha(\T,\T')$ has to be equal to the inverse of
the fraction on the right-hand-side).  Thus the defining equation of the Metropolis-Hastings decision is specified.  Since $\T$
and $\T'$ are arbitrarily chosen here, this argument works for any two values in the sample space.

\subsection{The Transition Kernel}
With a little bit of trouble, we can rigorously define the properties of the 1953 Metropolis transition kernel.  
\index{subjectindex}{Metropolis-Hastings algorithm!transition kernel!properties}  Start by defining an indicator function for the 
event that the $\delta'$ point is accepted: $I(\delta') = 1$ if $\delta'$ accepted, $0$ otherwise (although this is a slightly 
different notation for the indicator function than was used before, it makes the explication below more clean).  Thus the 
probability of transitioning from $\T = \T^{[k]}$ to the proposed jumping value $\T' = \T^{[k+1]}$ at the $k^{th}$ 
step is:
\begin{align}
    p(\T,\T') &= p(\T^{[k+1]} = \T', I(\T')|\T^{[k]} = \T)    \nonumber \\
                      &= p(\T^{[k+1]} = \T'|\T^{[k]} = \T)p(I(\T'))   \nonumber \\
              &= q(\T'|\T) \text{min}\left[1,
                \frac{ \pi(\T') }{ \pi(\T) } \right]
                \qquad \T \ne \T'.
\end{align}
The probability calculation for transitioning from $\T = \T^{[k]}$ to the current value (that is, not moving at all) is only 
slightly more complicated because it can occur two ways: a successful transition to the current state and a failed transition to a 
different state.  The first event has probability zero in continuous state space, but is worth covering for discrete applications.  
This probability is:
\begin{align}\label{actual.MH.transaction.function}
        p(\T,\T) &= \underbrace{ p(\T^{[k+1]} = \T, I(\T)|\T^{[k]} = \T) }_{
					\text{moving back to same point} }
              + \underbrace{ p(\T^{[k+1]} \ne \T, \neg I(\T)|\T^{[k]} = \T) }_{
					\text{not moving} }  \nonumber \\[7pt]
             &= q(\T,\T) + \sum_{\T'\ne \T}q(\T',\T)
                \left (1-\text{min}\left[1, \frac{ \pi(\T') }{ \pi(\T) } \right] \right).
\end{align}
In both of these calculations, the simpler situation of symmetry is assumed, but moving to the reversibility assumption is just a 
matter of substituting $(q(\T|\T')\pi(\T'))/(q(\T'|\T)\pi(\T))$ for $\pi(\T')/\pi(\T)$.

A critical component of the choice for the jumping distribution\index{subjectindex}{jumping density (proposal density)} is the 
specified variance.  \index{subjectindex}{Metropolis-Hastings algorithm!jumping distribution variance} If this variance is too 
large, then the jumping distribution will be too wide relative to the target distribution and each successive step will move 
too far in some direction causing us to move awkwardly through the sample space in exaggerated steps.  It is also possible to 
stipulate a jumping distribution variance that is too small causing overly cautious small steps through the sample space.  In 
this case we will converge slowly and mix poorly through the limiting distribution once we have converged.  

\subsection{Example: Estimating a Bivariate Normal Density}\index{subjectindex}{Metropolis-Hastings algorithm!bivariate normal example}
This example is somewhat oversimplified in order to show the mechanics of the Metropolis-Hasting algorithm.  Suppose, according 
to the example in Chib\index{authorindex}{Chib, S.} and Greenberg\index{authorindex}{Greenberg, E.} (1995, 333), we wanted 
to simulate the bivariate normal distribution: $\mathcal{N}\left[ \begin{smallmatrix} 1\\2 \end{smallmatrix} \biggm| 
\begin{smallmatrix} 1.0 & -0.9 \\ -0.9 & 1.0 \end{smallmatrix} \right]$.  Obviously we could do this by more direct means, but 
the goal here is to demonstrate the workings of Metropolis-Hastings.  The specified jumping distribution
\index{subjectindex}{jumping density (proposal density)!Student's$-t$} is a bivariate Student's-$t$ and we will start from four overdispersed 
positions as well as at the mode of the distribution.  The point here is to show that even though we deliberately disadvantage 
the process with regard to starting points and a heavy-tailed jumping distribution, the end-product is descriptive of our 
expectations.  

The algorithm is given in \R\ by:\label{metropolis.normal.example.code}
\begin{R.Code}
metropolis <- function(theta.matrix,reps,I.mat)  \{
   for (i in 2:reps)  \{
     theta.star <- mvrnorm(1,theta.matrix[(i-1),],I.mat)/
                        (sqrt(rchisq(2,5)/5))
     a <-dmultinorm(theta.star[1],theta.star[2],c(0,0),I.mat)/
         dmultinorm(theta.matrix[(i-1),1],theta.matrix[(i-1),2],
                        c(0,0),I.mat)
     if (a > runif(1)) theta.matrix[i,] <- theta.star
     else theta.matrix[i,] <- theta.matrix[(i-1),]
   \}     
   theta.matrix
\}
\end{R.Code} 

Figure~\ref{Metrop.Fig} shows the last 200 iterations of the algorithm in the first panel and the path of the chain in the 
second.  Notice that as with the Gibbs sampler example, convergence occurs rather quickly to the posterior region for this 
simple setup.  Unlike the Gibbs sampler, each movement here is \emph{not} a straight line parallel to one of the axes since 
the jumping decision takes place simultaneously in two dimensions.  It is difficult to see here, but the other difference 
is that the chain is not required to move at each iteration.\\[0.5in]
\index{subjectindex}{Metropolis-Hastings algorithm|)}

\begin{figure}[h]
	\centerline{ \hspace{0.12in} \epsfig{file=./Images/mcmc_intro.figure03.ps,height=5.86in,width=3.02in,clip=,angle=270} }
	\caption{\textsc{Metropolis-Hastings, Bivariate Normal Simulation}} \label{Metrop.Fig}
\vspace{-11pt}
\end{figure}

\section{The Hit-and-Run Algorithm}\label{hit.and.run.section}
The Hit-and-Run algorithm is a special case of the Metropolis-Hastings algorithm that separates the move decision into a direction
decision and a distance decision.  This makes it especially useful in tightly constrained parameter space because we can tune the
jumping rules to be more efficient (Gelman \etal 1996).\index{authorindex}{Gelman, A.}  It is also helpful when there are several
modes of nearly equal altitude.   This discussion is introduced here as an example of the flexibility of the Metropolis-Hastings
algorithm.  Summarize the steps as following, starting from an arbitrary point $\T_k$, at time $k$:\\
\hspace{0.25in}\begin{minipage}[t]{0.8\textwidth}
\begin{bayeslist}
            \item[{\bf Step 1:}]    Generate a multidimensional direction, $\mathbf{Dr}_k$, on the
                    surface of a unit hypersphere from the distribution
                    $f(\mathbf{Dr}|\T^{[k]})$.
            \item[{\bf Step 2:}]  Generate a signed distance, $Ds_k$, from density
                    $g(Ds|\mathbf{Dr}_k,\T)$.
%\end{bayeslist}
%\end{minipage}\\
%\hspace{0.25in}\begin{minipage}[t]{0.8\textwidth}
%\begin{bayeslist}
            \item[{\bf Step 3:}]    Set the candidate jumping point to:
                        $\T' = \T^{[k]} + Ds_k \mathbf{Dr}_k$
                    and calculate:
                                \begin{equation}
                                        a(\T',\T^{[k]}) = \frac{\pi(\T'|\X)}{\pi(\T^{[k]}|\X)}. \nonumber
                                \end{equation}
            \item[{\bf Step 4:}]    Move to $\T^{[k+1]}$ according to:
                                    \renewcommand{\arraystretch}{4}
                                    \begin{equation}\label{Hit.and.Run.Rule}
                                        \T_j^{[t+1]} =
                                        \begin{cases}
                                            \T'      & \text{with probability}\qquad   \min( a(\T',\T^{[k]}), 1 ) \9
                                            \T^{[k]} & \text{with probability}\qquad 1-\min( a(\T',\T^{[k]}), 1 ).
                                        \end{cases}\nonumber 
                                    \end{equation}\\ \renewcommand{\arraystretch}{1}
\end{bayeslist}
\end{minipage}

There are several important assumptions to consider.  \emph{First}, for all $\mathbf{Dr}_i$, $f\left(\mathbf{Dr}|\T^{[k]}\right)>0$.
This just means that the directional distribution must be positive for all outcomes.  \emph{Second}, the distance distribution, 
$g(Ds|\mathbf{Dr},\T)$ must also be strictly greater than zero and have the property:
\begin{equation}
            g(Ds|\mathbf{Dr},\T) = g(-Ds|-\mathbf{Dr},\T).    
\end{equation}
This gives a new form of the detailed balance equation: \index{subjectindex}{detailed balance equation}
\begin{equation}
                            g(||\T^{[k]}-\T'||)a(\T',\T^{[k]}) \pi(\T^{[k]}|\X) =
                            g(||\T'-\T^{[k]}||)a(\T^{[k]},\T') \pi(\T'|\X)
\end{equation}
Meeting these conditions means that the hit-and-run algorithm defines an ergodic Markov chain with stationary distribution 
$\pi(\T|\X)$. 

Typically $f(\mathbf{Dr}|\T^{[k]})$ is chosen to be uniform but others are possible, and the $a(\T',\T^{[k]})$ criterion can 
be made much more general.  One advantage to this algorithm over standard Metropolis-Hastings variant is that 
$g(Ds|\mathbf{Dr},\T)$ is also flexible and disengaged from the direction decision, which makes it very tunable (Chen and 
Schmeiser 1993, Smith 1996).  In fact, it can also be made \emph{adaptive} as the chain matures: adaptive directional sampling. 
\index{authorindex}{Chen, M-H.} \index{authorindex}{Schmeiser, B.} \index{authorindex}{Smith, R. L.}

\begin{examplelist}
	\item	\label{hit.run.example} We use this algorithm to simulate again from a bivariate normal.  Typically we would use hit-and-run 
            with more problematic forms, but this example shows the integrity of the process.  The problem is made more
            interesting specifying a strong correlation (0.95), which motivates some tuning of the algorithm.  Accordingly, random
            directions are drawn favoring the first and third quadrants.  Consider the following \R\ code:
            \index{subjectindex}{Metropolis-Hastings algorithm!hit and run example}

		    \begin{R.Code}
hit.run <- function(theta.matrix,reps,I.mat)  \{
    for (i in 2:reps)  \{
        u.vec <- c(runif(1,0,pi/2),    runif(1,pi/2,pi),
                   runif(1,pi,3*pi/2), runif(1,3*pi/2,2*pi))
        u.dr <- sample(u.vec,size=1, prob=c(1/3,1/6,1/3,1/6))
        g.ds <- rgamma(1,1,1)  
        xy.theta <- c(g.ds*cos(u.dr),g.ds*sin(u.dr)) 
                                + theta.matrix[(i-1),]
        a <- dmultinorm(xy.theta[1],xy.theta[2],
                 c(0,0),I.mat)/dmultinorm(theta.matrix[(i-1),1],
                 theta.matrix[(i-1),2],c(0,0),I.mat)
        if (a > runif(1)) theta.matrix[i,] <- xy.theta
        else theta.matrix[i,] <- theta.matrix[(i-1),]
    \}
    theta.matrix
\}
  		    \end{R.Code} 

		Now we can run this function for our simple example and graph.  Details on this step are found in the 
		{\bf Computational Addendum} to this chapter.  This produces Figure~\ref{Hit.and.Run.Demo} where the contour bands are 
		produced from the true target distribution, which we would ordinarily not know, and the points are the last 
		5,000 steps of the algorithm.  There are several \emph{tuning parameters} set here in the implementation of 
		the algorithm: favoring positive slope direction by two-to-one in the picking of direction, a gamma 
		distribution for the candidate-generating distribution, and the parameters of this gamma.  Often when writing 
		MCMC algorithms from scratch, these decisions deserve close inspection.  

		\begin{figure}[h!]
		\parbox[c]{\linewidth}{ \hspace{0.75in}
			\epsfig{file=./Images/mcmc_intro.figure04.ps,height=4.1in,width=4in,clip=,angle=270}  }
		\caption{\textsc{Hit-and-Run Algorithm Demonstration}} \label{Hit.and.Run.Demo}
		\end{figure}
\end{examplelist}

\hspace{0.25in}\begin{minipage}[t]{0.8\textwidth}$\quad$\end{minipage}

\section{The Data Augmentation Algorithm}\label{data.augmentation.section} \index{subjectindex}{data augmentation|(}
Tanner\index{authorindex}{Tanner, M. A.} and Wong\index{authorindex}{Wong, W. H.} (1987) introduced data augmentation (sometimes 
called substitution sampling) as a method for dealing with missing data or unknown parameter values by augmenting known information 
with candidate values much in the same way that EM does and iteratively improving the quality of these augmented quantities.  In 
fact, data augmentation can be used instead of EM\index{subjectindex}{EM algorithm} to estimate models with missing data when more 
than the mode of the likelihood function is required.  Data augmentation is an MCMC technique that successively substitutes 
improved estimates conditioned on the previous state and therefore forms a Markov chain.

Much like the situation in which we applied the EM algorithm, suppose that we are interested in estimating a single-dimension (for 
now) parameter $\theta$.  We observe some relevant data but lack the complete set: $\X = [\X_{obs},\X_{mis}]$, where all of the 
data (observed or not) is conditional on $\T$.  Data augmentation requires that we know the parametric form of the posterior 
$p(\theta|\X)$ corresponding to the complete data specification, and the predictive form for the missing data according to 
$p(\X_{mis}|\X_{obs})$.  The algorithm proceeds by augmenting the observed data with simulated values of the missing data, 
obtained by cycling through these conditions according to the algorithm now described.

Start by defining the \emph{posterior identity}, which is the desired quantity stated as if we could integrate out the missing 
data: \index{subjectindex}{data augmentation!substitution}\index{subjectindex}{data augmentation!posterior identity}
\begin{equation}\label{da.posterior.identity}
	p(\theta|\X_{obs}) = \int_{\X_{mis}} p(\theta|\X_{obs},\X_{mis}) p(\X_{mis}|\X_{obs})d\X_{mis}.
\end{equation}
We can also define the \emph{predictive identity} by asserting that there is some unknown parameter, $\phi$ on the sample space 
of $\theta$, critical to generating the unobserved data but integrated out:\index{subjectindex}{data augmentation!predictive identity}
\begin{equation}\label{da.predictive.identity}
	p(\X_{mis}|\X_{obs}) = \int_{\Phi} p(\X_{mis}|\phi,\X_{obs}) p(\phi|\X_{obs})d\phi.
\end{equation}
Now insert \eqref{da.predictive.identity} into \eqref{da.posterior.identity} for the 
last term and interchange the order of integration:
\begin{align}\label{da.substitution}
	p(\theta|\X_{obs}) &= \int_{\X_{mis}} p(\theta|\X_{obs},\X_{mis}) 
					\left[  \int_{\Phi} p(\X_{mis}|\phi,\X_{obs}) p(\phi|\X_{obs})d\phi 
			        	\right] d\X_{mis}  \nonumber \\
			   &= \int_{\Phi} \underbrace{
					\int_{\X_{mis}} p(\theta|\X_{obs},\X_{mis}) p(\X_{mis}|\phi,\X_{obs})d\X_{mis}
					}_{K(\theta,\phi)}
			      		 p(\phi|\X_{obs})d\phi.
\end{align}
Here Tanner\index{authorindex}{Tanner, M. A.} and Wong\index{authorindex}{Wong, W. H.} use the shorthand $K(\theta,\phi)$ to make 
the notation cleaner, not to imply that this is a joint probability (it is really a \emph{transition kernel}!).  
\index{subjectindex}{transition kernel}

The form of \eqref{da.substitution} implies that an iterative algorithm could be constructed that generates values of $\theta$ 
given an approximation for $\X_{mis}$, and then generates new values of $\X_{mis}$, given this $\theta$.  Specifically, data 
augmentation at the $i^{th}$ iteration is (beginning with candidate values for $\X_{mis}$): 
\index{subjectindex}{data augmentation!imputation step}
\index{subjectindex}{data augmentation!posterior step}
\begin{bayeslist}
    \item {\bf [Imputation-Step:]} 
		\begin{bayeslist2}
			\item 	generate $\theta^{[i]}$ from $p^{[i-1]}(\theta|\X_{obs})$,
			\item	generate $m$ values of $\X_{mis}$ from  $p(\X_{mis}|\theta^{[i]},\X_{obs})$.
		\end{bayeslist2}		
	
    \item {\bf [Posterior-Step:]} 
		\begin{bayeslist2}
			\item 	update the parametric approximation using $\X_{mis,1},\ldots,\X_{mis,m}$:
				\begin{equation}
					p^{[i]}(\theta|\X_{obs}) = \frac{1}{m}\sum_{j=1}^{m}p(\theta|\X_{obs},\X_{mis,i}) .
				\end{equation}
		\end{bayeslist2}
\end{bayeslist}
In very similar fashion to importance sampling, there are two interrelated researcher-generated specifications here: the 
tolerance value for determining convergence ($ p^{[i]}(\theta|\X_{obs}) -  p^{[i+1]}(\theta|\X_{obs})$), and the number of 
simulation values at each step ($m$).  The second decision is perhaps more crucial, and is a (now) familiar balance between 
speed and accuracy.  In their original article, Tanner\index{authorindex}{Tanner, M. A.} and Wong\index{authorindex}{Wong, W. H.} 
provide $m$ values of 1600 and 6400 for relatively simple model specifications.  It is therefore recommended that a similar value 
can be used as an initial parameter.  With computers becoming increasingly faster, it is unlikely that this will place a heavy 
computation burden on the average system.  Since larger values of $m$ give better intermediate approximations, there is 
necessarily a trade-off between longer runs and longer calculations at each run.  In fact, if $m=1$, then data augmentation is 
actually Gibbs sampling, and obviously the emphasis is then purely on the length of runs.
\index{subjectindex}{data augmentation!as Gibbs sampling}

Convergence of the data augmentation algorithm is demonstrated in Tanner\index{authorindex}{Tanner, M. A.} and Wong.
\index{authorindex}{Wong, W. H.}  Rosenthal\index{authorindex}{Rosenthal, J. S.} (1993) showed that this convergence is on the 
order of the log of the number of missing cases in the data for cases like this example.  
\index{subjectindex}{data augmentation!convergence}  This is particularly encouraging because it means that even for the very 
largest data sets that we see in the social and behavioral sciences, data augmentation will likely be a reasonable computation 
process.  Also, including latent data from the data augmentation procedure does not preclude model comparison or the generation 
of the standard model tests discussed in Chapter~\ref{Testing.Chapter}.  For instance, Raftery\index{authorindex}{Raftery, A. E.} 
(1996, p.182) gives the details of Bayes Factor tests when one or both of the models include such latent data.
\index{subjectindex}{data augmentation!latent data}

There are enough similarities between data augmentation and EM that one might wonder when one is more applicable than the other.
\index{subjectindex}{data augmentation!versus EM}  Both algorithms rely upon using the likelihood function under the most simple 
circumstances possible, and making these circumstances simple by completing the data with successively improved estimates of the 
missingness.  EM makes more sense when the objective is simply to get the mode of the posterior because it is faster (less 
within-step calculations to perform).  However, if the goal is to describe the complete posterior distribution, then data 
augmentation is more appropriate.  Since data augmentation is a special case of Gibbs sampling and unlike Gibbs sampling, it is 
not directly implemented in \bugs\ \index{subjectindex}{bugs@\bugs!data augmentation} (see Congdon\index{authorindex}{Congdon, P.} 
[2001, p.114] for a nifty workaround, however), then one might have a natural preference for Gibbs sampling in practice unless 
one wanted to treat the missingness more explicitly.\index{subjectindex}{data augmentation|)}

\index{authorindex}{Tanner, M. A.} \index{authorindex}{Wong, W. H.} \index{authorindex}{Liu, J. S.} \index{authorindex}{Albert, J. H.} 
\index{authorindex}{Chib, S.} \index{authorindex}{Swendsen, R. H.} \index{authorindex}{Wang, J. S.} \index{authorindex}{Liu, J. S.} 
\index{authorindex}{Wong, W. H.} \index{authorindex}{Kong, A.} \index{authorindex}{Rosenthal, J. S.} \index{authorindex}{Imai, K.}
\index{authorindex}{van Dyk, D.} \index{authorindex}{Gelman, A.}  
Finally, data augmentation (Tanner and Wong 1987) can help with mixing properties even though it is most often posited as a means of 
dealing with missing data in Bayesian models (Liu 2001, pp.135-8).  Albert and Chib (1993), Swendsen and Wang (1987), Liu, 
Wong, and Kong (1994), Carlin and Polson (1991), Rosenthal (1993), and more recently, Imai and van Dyk (2004), and Gelman \etal (2008).
The basic idea behind data augmentation
in the context here is that if an MCMC algorithm to marginalize $\pi(\T|\X)$ is mixing slowly, then it is sometimes possible to find 
another convenient random variable $\UP$ such that the algorithm operates more efficiently through sampling from $\pi(\T,\UP|\X)$ by 
alternating between $\pi(\T|\UP,\X)$ and $\pi(\UP|\T,\X)$.  Chib (1992)\index{authorindex}{Chib, S.} explains how this can be 
quite simple and elegant when $\UP$ is a latent feature in the model specification.  Meng and van Dyk (1999), as well as van Dyk and 
Meng (2001), \index{authorindex}{Meng, X-L.} \index{authorindex}{van Dijk, H. K.} extend this idea with conditional and marginal 
versions of data augmentation to take advantage of unidentifiable parameters in order to improve the rate of convergence.

\index{subjectindex}{Mars Rover|(}
\begin{examplelist}
	\item	{\bf Data Augmentation for Decision-Making in the Mars Rover.}\label{Rover.Example}\index{subjectindex}{example!Mars Rover}
		Planetary Rovers operate in environments where human exploration is expensive, dangerous, or impossible.  So NASA 
		needs to implement \emph{local} analysis and decision-making in Rovers due to bandwidth limitations (3.5K to 12K per 
		second to Mars) and delay in telemetry and control signals (about 7 minutes one-way travel time to Mars, available 
		3 hours per day).  This motivates the idea that the Rover should ``learn'' about its environment as a way to update 
		and improve the quality of the decision-making process on its own.  The Ames Research Center/CMU innovation is the 
		addition of Bayesian updating within Rover circuitry to enable autonomous actions: obstacle avoidance, path planning, 
		visual tracking, and stereo processing.

		Why might this be relevant to social scientists?  Proxies (agents or subordinates) need to be autonomous under some 
		circumstances, communicating at irregular intervals and with limited information.  Another tie-in is that formal models 
		of principal/agent relationships often assume asymmetric information and divergent goals.  So real-time analysis can be 
		improved with semi-autonomous decision-making where Bayesian updating maximally leverages incomplete knowledge.
		
		The important criteria for robotic updating are: the current status of internal systems and resources, current 
		environmental effects, and the ramifications of actions on resources and status.  Note that Rover states are discrete 
		modes, but measured parameters are defined on continuous spaces.  All diagnosis here is done as a Bayesian 
		belief-updating system:
		\begin{bayeslist}
			\item	begin with some prior belief over possible states,
			\item	collect observations on status and environment,
			\item	update the distributions to reflect new evidence.
		\end{bayeslist}
		Interestingly, the sequence of states is actually Markovian.  We now formalize this in convenient notation in order to 
		show the Bayesian learning process.  For details, see the research papers from the Carnegie Mellon team: Verma, Langford, 
		and Simmons (2001), Verma, Thrun, and Simmons (2003), and Verma, Gordon, Simmons, and Thrun (2004), as well as the technical 
		reports from JPL/Caltech and Ames/NASA: Volpe, Nesnas, Estlin, Mutz, Petras, and Das (2001), Volpe and Peters (2003), 
		Dearden and Clancy (2002), and Estlin, Volpe, Nesnas, Mutz, Fisher, Engelhardt, and Chien (2001).
                \index{authorindex}{Verma, V.} \index{authorindex}{Langford, J.} \index{authorindex}{Simmons, R.}
                \index{authorindex}{Thrun, S.} \index{authorindex}{Volpe, R.} \index{authorindex}{Nesnas, I. A. D.}
                \index{authorindex}{Estlin, T.} \index{authorindex}{Mutz, D.} \index{authorindex}{Petras, R.} \index{authorindex}{Das, H.}
		\index{authorindex}{Peters, S.} \index{authorindex}{Dearden, R.} \index{authorindex}{Clancy, D.}
                \index{authorindex}{Estlin, T.} \index{authorindex}{Volpe, R.} \index{authorindex}{Nesnas, I. A. D.}
                \index{authorindex}{Mutz, D.} \index{authorindex}{Fisher, F.} \index{authorindex}{Engelhardt, B.} 
		\index{authorindex}{Chien, S.} \index{authorindex}{Verma, V.} \index{authorindex}{Gordon, G.} 
		\index{authorindex}{Simmons, R.} \index{authorindex}{Thrun, S.}

		At time $t$ the true multivariate state of the Rover systems is denoted $\X_t$, but the observed state by instruments 
		is denoted $\OO_t$ (sensors are noisy and limited).  The state of the Rover is Markovian since we know that: 
		\begin{equation*}
			p(\X_t|\X_0,\X_1,\ldots,\X_{t-1}) = p(\X_t|\X_{t-1}), 
		\end{equation*}
		and the observations are also conditionally independent of all but the last state: 
		\begin{equation*}
		p(\OO_t|\X_0,\X_1,\ldots,\X_{t-1}) = p(\OO_t|\X_{t-1}).  
		\end{equation*}
		Bayesian ``filtering'' (data augmentation with $\X_{t-1}$) for this particular state-estimating problem is done by:
		\begin{align}
		    \pi(\X_t|&\OO_{1:t-1},\OO_t) 
			= p(\OO_t|\X_t,\OO_{1:t-1}) \frac{ p(\X_t|\OO_{1:t-1}) }{ p(\OO_t|\OO_{1:t-1}) } \nonumber \\
			&\propto \underbrace{ p(\X_t|\OO_{1:t-1}) }_{\text{prior}}
				 \underbrace{ p(\OO_t|\X_t) }_{\text{likelihood}} \nonumber \\
			&= \int p(\X_t|\OO_{1:t-1},\X_{t-1})p(\X_{t-1}|\OO_{1:t-1})
					           d\X_{t-1} \, p(\OO_t|\X_t) \nonumber \\
			&= \int \underbrace{ p(\X_t|\X_{t-1}) }_{\text{transition model}}
					           \underbrace{ p(\X_{t-1}|\OO_{1:t-1}) }_{\text{previous status}}
					           d\X_{t-1} \hspace{-11pt}
						   \underbrace{ p(\OO_t|\X_t) }_{\text{observation model}}
		\end{align}
		This works because:
		\begin{align}
  			\int p(\X_t|\OO_{1:t-1},\X_{t-1})& p(\X_{t-1}|\OO_{1:t-1}) d\X_{t-1} \nonumber \\
				&= \int \frac{ p(\X_t,\OO_{1:t-1},\X_{t-1}) }{ p(\OO_{1:t-1},\X_{t-1}) } 
                      		\frac{ p(\OO_{1:t-1},\X_{t-1}) }{ p(\OO_{1:t-1}) } d\X_{t-1}  \nonumber \\
				&= \int p(\X_t,\OO_{1:t-1},\X_{t-1})/p(\OO_{1:t-1}) d\X_{t-1} \nonumber \\
				&= \int p(\X_t,\X_{t-1}|\OO_{1:t-1}) d\X_{t-1} \nonumber \\
				&= p(\X_t|\OO_{1:t-1})  
		\end{align}
		Often the dimensionality of this posterior, $\pi(\X_t|\OO_{1:t})$, is too great for calculation in real-time, so 
		the Rover uses a recursive \emph{particle filter} (importance sampling):
		\begin{equation}
		    \pi(\X_t|\OO_{1:t}) \approx n^{-1}\sum_{i=1}^{n} \delta (\X_t - \x_t^{[i]})
		\end{equation} 
		where $\delta()$ is the Dirac delta function, and $\x_t^{[i]}$ are samples drawn from some proposal 
		distribution.  The algorithm uses $p(\X_t|\X_{t-1})$ as the proposal distribution and $p(\OO_t|\X_t)$ 
		for importance weights, and then proceeds:
		\begin{enumerate}
		    \item for $i=1:n$, draw $\x_t^{[i]} \sim p(\X_t|\x_{t-1}^{[i]})$ and set
			  $\omega_{t}^{[i]} = p(\OO_t|\X_t^{[i]})$.
		    \item from this set, accept values $\x_t^{[i]}$ with probability proportional
			  to $\omega_{t}^{[i]}$.
		\end{enumerate}  

		These transaction functions map a present mode to a PMF for future modes.  So state change 
		probabilities are estimated then updated with inherent uncertainty, and autonomous decisions are 
		made based on the most recent posterior.  
		So all of these calculations occur in the context of executing general instructions: ``go there,'' 
		``investigate that,'' etc.  The Rover then proceeds towards its objective while Bayesianly updating 
		the posterior for $\X_t$, where intermediate decisions (movement, direction, sensing) are made 
		autonomously with criteria such as: safety, power conservation, noting interesting phenomenon.  
		In this fashion the Bayesian characteristics are apparent.
\end{examplelist}
\index{subjectindex}{Mars Rover|)}
\vspace{-0.5in}

                \index{authorindex}{Metropolis, N.}\index{authorindex}{Rosenbluth, A. W.}\index{authorindex}{Rosenbluth, M. N.}
                \index{authorindex}{Teller, A. H.}\index{authorindex}{Teller, E.}
\section{Historical Comments}\index{subjectindex}{Markov chain Monte Carlo (MCMC)!history}
The background of the development of modern MCMC methods is interesting unto itself.  The first important event was the publication 
of a 1953 paper by Nicholas Metropolis and his colleagues: Arianna Rosenbluth, Marshall Rosenbluth, Augusta Teller, and Edward Teller 
(who we know also made notable 
contributions in nuclear physics related to rather large explosions).  Because the paper was published in the \emph{Journal of 
Chemical Physics} and because it was applied exclusively to the problem of particles moving around a square, interest was restricted 
primarily to physics.  

Metropolis\index{authorindex}{Metropolis, N.} \etal were interested in obtaining the positions and therefore the potential between 
all molecules in an enclosure and noted that even very modest sized setups lead to integrals of very high dimensions.  Specifically, 
if $\hbar_{ij}$ represents the shortest distance between particles $i$ and $j$, and $V(\hbar_{ij})$ is the associated potential, 
then the total potential energy of the whole system is given by: $E=\frac{1}{2}\sum_{i=1}^{n}\sum_{j=1,j \ne i}^{n}V(\hbar_{ij})$.  
Due to some simplifications this leads to an integral of ``only'' 200 dimensions for the force of the system.  So rather than try to 
track and calculate future positions, they arrived at the idea of setting a molecule movement criterion in a formal model sense and 
then simulating a series of potential positions. In other words, it was sufficient to know where the molecules were 
\emph{probabilistically} at some future point in time as opposed to \emph{exactly}.

The Metropolis\index{authorindex}{Metropolis, N.} \etal paper was slow to permeate other disciplines including statistics partly 
because the authors appear not to have been aware of the widespread applicability of their technique.  This is a presumed explanation 
for why the authors chose not to generalize it beyond the application given. The key to the dissemination of the algorithm was the
refinement and generalization done by Hastings (1970) some time later.  He showed that reversibility can be substituted for symmetry 
in the approximation distribution, applicable to continuous state spaces, and he makes the ideas accessible to statisticians.  In 
addition, Peskun\index{authorindex}{Peskun, P. H.} (1973) should be credited with further introducing the Metropolis algorithm to the 
statistics community and proving a number of important properties, including principles of reversibility.

The Geman\index{authorindex}{Geman, S.}\index{authorindex}{Geman, D.} and Geman (1984) paper introduced a new use for the Gibbs 
distribution in simulation and applies the tool to restoration of degraded images.  This paper is very difficult to work through and 
most people do not persevere. Instead, the  landmark Gibbs sampling paper, as far as widespread effects are concerned, is that of 
Gelfand\index{authorindex}{Gelfand, A. E.} and Smith\index{authorindex}{Smith, A. F. M.} (1990).  They demonstrate how widely useful 
Gibbs sampling is in terms of setting up Markov chains to estimate posterior distributions.  While nothing is entirely new in this 
paper, the synthesis and integration of Gibbs sampling into Markov chain Monte Carlo theory for the first time is an invaluable 
contribution.

One key reason for the explosion of academic attention to MCMC that occurred in the 1990s is the substantial improvement in computing 
power on the average desktop.  This point cannot be overstated.  By definition these techniques are computer-intensive, and it is 
hard to imagine earlier researchers being pleased with either the speed of their microcomputers or the convenience of their campus 
mainframes.  Fortunately Moore's Law (doubling of computing power every two years) continues to apply.

The overall impact on statistics and applied statistics cannot be reasonably overstated.  One long-time observer notes that 
``the Bayesian `machine' together with MCMC is arguably the most powerful mechanism ever created for processing data and knowledge'' 
(Berger\index{authorindex}{Berger, J. O.} 2001).  Essentially the advent of MCMC freed the Bayesian analyst from the Faustian choice 
of accepting oversimplification of the assumptions in order to merely get a tractable answer. Because of these relatively simple
tools, models of seemingly endless complexity can be estimated. There are some additional issues to worry about, such as practical
considerations with Markov chain convergence (Chapter~\ref{MCMC.Utilitarian.Chapter}), but these are adequately handled with
modern software.
\vspace{-0.15in}

\subsection{Full Circle?}
An inquiring mind may have realized that many of the properties used to analyze and exploit MCMC techniques are \emph{frequentist} in 
nature.  \index{subjectindex}{Markov chain Monte Carlo (MCMC)!frequentist properties}  Principles used here such as the central limit 
theorem, the law of large numbers, general asymptotic analysis, and \index{subjectindex}{central limit theorem}
\index{subjectindex}{law of large numbers} \index{subjectindex}{transition invariance} 
transition invariance, are all basic principles from traditional non-Bayesian statistics.  
Specifically, the tool that revolutionized Bayesian statistics is in fact a frequentist construction.  Efron
\index{authorindex}{Efron, B.} (1998) notes that Fisher's work directly implied several modern statistical computing techniques that 
Fisher could not have employed for purely mechanistic reasons.  These include bootstrapping (the bootstrap plug-in principle 
is anticipated by the calculation of \emph{Fisher Information} and Rubin [1981]\index{authorindex}{Rubin, D. B.} shows how the bootstrap can 
be made explicitly Bayesian),\index{subjectindex}{Fisher Information} empirical calculation of confidence/credible intervals, empirical 
Bayes (developing a prior using the data), and Bayes Factors.
\index{subjectindex}{empirical Bayes}\index{subjectindex}{Bayes Factor}\index{subjectindex}{Fisher Information}
\vspace{-0.15in}

\section{Exercises}
\begin{exercises}
    \item Find the values of $\alpha_1$, $\alpha_2$, and $\alpha_3$ that make the following a transition matrix:
				\begin{equation*}
        				\left[ \begin{array}{rrr} 0.4 & \alpha_1 & 0.0 \\ 
								  0.3 & \alpha_2 & 0.6 \\ 
								  0.0 & \alpha_3 & 0.4 \\ \end{array} \right].
				\end{equation*}

            % NEW
    \item   For the following simple transition matrix:
            \begin{equation*}
                    \begin{array}{r}  \theta_1 \\ \theta_2 \\ \end{array}
                    \left[ \begin{array}{rr} 0 & 1 \\ 1 & 0 \\ \end{array} \right].     
            \end{equation*}
            For an initial state, $[0.25,0.75]$, describe repeated applications of the transition matrix: $\Pmat^2, \Pmat^3,
            \Pmat^4,\ldots$.  Is this an ergodic Markov chain?

    \item 	Using the transition matrix from Section~\ref{simple.MC.example}, run the Markov chain mechanically (step 
            by step) in \R\ using matrix multiplication.  Start with at least two very different initial states. Run 
            the chains for at least ten iterations.

            % NEW
    \item   Consider a simple Ehrenfest urn problem \index{subjectindex}{Ehrenfest urn problem} with two urns and only two balls.  
            For simplicity start with one ball in each urn, then at each step one of the two balls is selected with probability
            0.5 and then placed in the urn that it currently is not in.  So there are only three possible states: $[2,0], [1,1], [0,2]$ 
            Show that this is a Markov chain, provide the transition matrix, and derive the unique stationary distribution.
            \begin{comment}
            \begin{equation*}
                \left[ \begin{array}{rrr} 0 & 1 & 0 \\ 0.5 & 0 & 0.5 \\ 0 & 1 & 0 \\ \end{array} \right], 
                \quad (1/4, 1/2, 1/4).
            \end{equation*}
            \end{comment}

    \item For the following transition matrix, which classes are closed?  
				\begin{equation*}
        				\left[ \begin{array}{rrrrr} 0.50 & 0.50 & 0.00 & 0.00 & 0.00 \\ 
								    0.00 & 0.50 & 0.00 & 0.50 & 0.00 \\ 
								    0.00 & 0.00 & 0.50 & 0.50 & 0.00 \\ 
								    0.00 & 0.00 & 0.75 & 0.25 & 0.00 \\ 
								    0.50 & 0.00 & 0.00 & 0.00 & 0.50 \\ 
					\end{array} \right]
					\nonumber
				\end{equation*}

            % NEW
    \item   Males in three professions have the following probabilities that their primary son follows them into the same 
            profession: Professor $p=0.4$, Plumber $p=0.8$, Playwright $p=0.2$.  If the son does not pick the same 
            profession as the patriarch, he picks with even probability from the other two professions.  Each protagonist
            possesses at least one male progeny.  Produce the transition matrix for this plan and provide the steady state.

    \item   \label{matrix.solve} From the following transition matrix, calculate the stationary distribution
			for proportions:
			\begin{equation*}
        				\left[ \begin{array}{rrr} 0.0 & 0.4 & 0.6 \\ 
								  0.1 & 0.0 & 0.9 \\ 
								  0.5 & 0.5 & 0.0 \\ \end{array} \right].
					\nonumber
			\end{equation*}
			What is the substantive conclusion from the zeros on the diagonal of this matrix?

            % NEW
            \index{subjectindex}{iid}
    \item   An AR(1) stochastic process (autoregressive) is commonly defined as:
            \begin{equation*}
                \theta^{[t+1]} = \varphi\theta^{[t]} + \Omega_t,
            \end{equation*}
            where the $\Omega_t$ values are generated iid from $\mathcal{N}(0,\tau^2)$ for finite $\tau^2$, and the 
            distribution of the $\theta$ values is also iid with finite variance $\sigma^2$.  From the covariance 
            of generated values $m-1$ away from each other, $\Cov(\theta^{[t+m]},\theta^{[t]}) = \varphi^m\Var(\theta)$, 
            using the central limit theorem,\index{subjectindex}{central limit theorem} show that if this process is 
            stationary that:
            \begin{equation*}
                \theta \sim \mathcal{N}(0,\sigma^2), 
            \end{equation*}
            where:
            \begin{equation*}
                    \sigma^2 = \frac{ \tau^2 (1+\varphi) }{ (1-\varphi^2)(1-\varphi) },
            \end{equation*}
            and state the necessary restriction necessary on $\varphi$.

    \item 	Develop a Bayesian specification using \bugs\ to model the following counts of the number of social contacts 
            for children in a daycare center with the objective of estimating $\lambda_i$, the contact rate:\vspace{11pt}

            \parbox[c]{\linewidth}{
                \blstable
                \begin{tabular}{l|rrrrrrrrrr} 
                            			    & \multicolumn{10}{c}{} \\[-15.6pt]  \\
                    Person 			        & 1  & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 \\
                    Age ($a_i$)    		    & 11 & 3 & 2 & 2 & 7 & 5 & 6 & 9 & 7 & 4 \\
                    Social Contacts ($x_i$) & 22 & 4 & 2 & 2 & 9 & 3 & 4 & 5 & 2 & 6 \\
                                        	& \multicolumn{10}{c}{} \\[-1.16pt]  
                \end{tabular}
                \bls
                \vspace{11pt}
            }

            Use the following specification:
            \begin{align}
                \lambda_i &\sim \mathcal{G}(\alpha,\beta)      \nonumber\\
                \alpha    &\sim \mathcal{G}(1,1)                \nonumber\\
                \beta     &\sim \mathcal{G}(0.1,1)               \nonumber
            \end{align}
            to produce a posterior summary for $\lambda_i$.

            % NEW
    \item   Show that for a stationary Markov chain that a set of batches (a set of consecutive draws) of the same
            batch length have the same joint distribution.

    \item   The table below provides the results of nine postwar elections in Italy by proportion per political party.  
            The listed parties are: 
            \begin{bayeslist}
                \item   Democrazia Cristiana ({\bf DC}), 
                \item   Partito Comunista Italiano ({\bf PCI}), 
                \item   Partito Socialista Italiano ({\bf PSI}), 
                \item   Partito Socialista Democratico Italiano ({\bf PSDI}), 
                \item   Partito Repubblicano Italiano ({\bf PRI}), 
                \item   Partito Liberale Italiano ({\bf PLI}),
                \item   Others.
            \end{bayeslist}
            The ``Others'' category is a collapsing 
            of smaller parties: Partito Radicale ({\bf PR}), Democrazia Proletaria ({\bf DP}), Partito di Unit\`{a} 
            Proletaria per il Comunismo ({\bf PdUP}), Movimento Sociale Italiano ({\bf MSI}), South Tyrol Peoples Party 
            ({\bf SVP}), Sardinian Action Party ({\bf PSA}), Vald\^{o}taine Union ({\bf UV}), the Monarchists 
            ({\bf Mon}), and the Socialist Party of Proletarian Unity ({\bf PSIUP}).  In two cases parties presented
    		joint election lists and the returns are split across the two parties here.  The compositional data suggest 
            a sense of stability for postwar Italian elections even though Italy has averaged more than one government 
            per year since 1945.\\[11pt]

			\parbox[c]{\linewidth}{	\hspace{-0.2in}
    				\blstable
    				\begin{scriptsize}
    				\begin{tabular}{lrrrrrrrr|r}
    				Party        & 1948  & 1953  & 1958  & 1963  & 1968   & 1972  & 1976  & 1979  & 1983 \4
    				\hline
    				{\bf DC}     & 0.485 & 0.401 & 0.424 & 0.383 & 0.3910 & 0.388 & 0.387 & 0.383 & 0.329 \4
    				{\bf PCI}    & 0.155 & 0.226 & 0.227 & 0.253 & 0.2690 & 0.272 & 0.344 & 0.304 & 0.299 \4
    				{\bf PSI}    & 0.155 & 0.128 & 0.142 & 0.138 & 0.0725 & 0.096 & 0.096 & 0.098 & 0.114 \4
    				{\bf PSDI}   & 0.071 & 0.045 & 0.045 & 0.061 & 0.0725 & 0.051 & 0.034 & 0.038 & 0.041 \4
    				{\bf PRI}    & 0.025 & 0.016 & 0.014 & 0.014 & 0.0200 & 0.029 & 0.031 & 0.030 & 0.051 \4
    				{\bf PLI}    & 0.038 & 0.030 & 0.035 & 0.070 & 0.0580 & 0.039 & 0.013 & 0.019 & 0.029 \4
    				{\bf Others} & 0.071 & 0.154 & 0.113 & 0.081 & 0.1170 & 0.125 & 0.095 & 0.128 & 0.137 \4
    				\hline
    				\multicolumn{10}{c}{Source: Instituto Centrale di Statistica, Italia}\\
    				\end{tabular}
    				\end{scriptsize}
                    \vspace{11pt}
    				\bls
            }        

    	    Develop a multinomial-logistic model for $p_{ij}$ as the proportion received by party $i$ and election $j$ with 
            \bugs, using time as an explanatory variable and specifying uninformative priors.

            % NEW
    \item   Section~\ref{gibbs.sampling.section} described carefully how the Gibbs sampler must use the most recent
            draws as values on the right-hand-side of the conditional distribution for subsequent draws.  Modify the \R\ code  
            for the Gibbs sampling in Example~\ref{tweed.example} such that the conditioned-upon values of $\lambda$ and $\phi$
            are from the previous step not the current step in the draw for $k$.  Is this now a transient Markov chain?

    \item   Blom,\index{authorindex}{Blom, G.} Holst,\index{authorindex}{Holst, L.}\index{authorindex}{Sandell, D.} and 
            Sandell (1994) define a ``homesick'' Markov chain as one where the probability of returning to the starting state 
            after $2m$ ($m>1$) iterations is at least as large as moving to any other state: $p^{2m}(x_0,x_{\neg 0}) \le 
            p^{2m}(x_0,x_0)$.  Does the Markov chain defined by the following transition matrix have homesickness?
            \index{subjectindex}{Markov chain!homesick}
			\begin{equation*}
        				P=\left[ \begin{array}{rrrrr} 0.5 & 0.5 & 0.0 & 0.0 \\ 
								      0.5 & 0.0 & 0.5 & 0.0 \\ 
								      0.0 & 0.5 & 0.0 & 0.5 \\ 
								      0.0 & 0.0 & 0.5 & 0.5 \\ 
					\end{array} \right]
					\nonumber
			\end{equation*}
			Do Markov chains become less homesick over time?	
            
            % NEW
    \item   Using the four Metropolis-Hastings samplers from Exercise~\ref{chib.and.greenberg.exercise}, modify
            the \R\ plotting procedures in the {\bf Computational Addendum} for this chapter to graph these in four 
            partitions of the same graph.

    \item   Koppel\index{authorindex}{Koppel, J. G. S.} (1999) studies political control in hybrid organizations 
			(semi-governmental) through an analysis of government purchase of a specific type of 
			venture capital funds: investment funds sponsored by the Overseas Private Investment 
			Corporation (OPIC).  The following data (opic.df in \texttt{BaM}provide three variables as of January 1999. 
            %NEED TO ADD TO BaM [added]

            \vspace{11pt}
            \parbox[c]{\linewidth}{
    			\blstable
    			\begin{scriptsize}\begin{normalfont}
    			\begin{tabular}{lcccc}
				    Fund					& Age	& Status	& Size (\$M)	 \\
				    \hline
				    AIG Brunswick Millennium     	 	& 3	& Investing	& 300	\\
				    Aqua International Partners		& 2	& Investing	& 300	\\
				    Newbridge Andean Capital Partners	& 4	& Investing	& 250 	\\
				    PBO Property				& 1	& Investing	& 240	\\
				    First NIS Regional			& 5	& Investing	& 200	\\
				    South America Private Equity Growth	& 4	& Investing	& 180	\\
				    Russia Partners				& 5	& Investing	& 155 	\\
				    South Asia Capital			& 3	& Investing	& 150	\\
				    Modern Africa Growth and Investment	& 2	& Investing	& 150	\\
				    India Private Equity			& 4	& Investing 	& 140	\\
				    New Africa Opportunity			& 3	& Investing	& 120	\\
				    Global Environmental Emerging II	& 2	& Investing	& 120	\\
				    Bancroft Eastern Europe			& 3	& Investing	& 100	\\
				    Agribusiness Partners International	& 4	& Investing	& ~95	\\
				    Caucus					& 1	& Raising	& ~92	\\
				    Asia Pacific Growth			& 7	& Divesting	& ~75	\\
				    Global Environmental Emerging I		& 5	& Invested	& ~70	\\
				    Poland Partners				& 5	& Invested	& ~64	\\
				    Emerging Europe				& 3	& Investing	& ~60	\\
				    West Bank/Gaza and Jordan		& 2	& Raising	& ~60	\\
				    Draper International India		& 3	& Investing	& ~55	\\
				    EnterArab Investment			& 3	& Investing 	& ~45	\\
				    Israel Growth				& 5	& Investing 	& ~40	\\
				    Africa Growth      			& 8	& Divesting	& ~25	\\
				    Allied Capital Small Business		& 4	& Divesting	& ~20	\\
                \end{tabular}
                \end{normalfont}\end{scriptsize}
				\bls
	        }			
            \vspace{11pt}

            Develop a model using \bugs\ where the size of the fund is modeled by the age of the fund and its investment 
            status according to the specification:
			\begin{align}
					Y_i &\sim \mathcal{N}(m_i,\tau)		\nonumber\\
					m_i &= \beta_0 + \beta_1 X_{1i} + \beta_2 X_{2i}  + \epsilon_i   \nonumber\\
					\epsilon &\sim \mathcal{N}(0,k),		\nonumber
			\end{align}
			where: $Y$ is the size of the fund, $X_1$ is the age of the fund, and $X_2$ is a dichotomous explanatory variable 
            equal to one if the fund is investing and zero otherwise.  Set a value for the constant $k$ and an appropriate 
            prior distribution for $\tau$.  Summarize the posterior distributions of the unknown parameters of interest.

            % NEW
            \index{subjectindex}{multiple-try Metropolis algorithm} 
            \index{authorindex}{Liu, J. S.} \index{authorindex}{Liang, F.} \index{authorindex}{Wong, W. H.} 
    \item   Another special case of the Metropolis-Hastings algorithm is the multiple-try Metropolis algorithm
            (Liu, Liang, and Wong 2000) with local optimization
            steps.  It is designed to be more computationally efficient with difficult posterior shapes.
            Assuming the current position to be $\theta$, the steps for a single iteration are:
            \begin{bayeslist}
                \item   From a proposal density, $q(\theta'|\theta)$, draw $\theta_1^*,\ldots,\theta_k^*$.
                \item   Define a non-negative arbitrary function $\lambda(\theta_i^*,\theta)$.
                \item   For each $\theta_i^*$ calculate $\omega(\theta_i^*,\theta) = \pi(\theta_i^*)q(\theta|\theta_i^*)\lambda(\theta_i^*,\theta)$.
                \item   Draw a single $\theta^*$ proportional to these weights.
                \item   Set $\theta_1 = \theta$ (current chain position), and draw $\theta_1,\ldots,\theta$\ from
                        $q(\theta|\theta^*)$
                \item   Accept $\theta^*$ with the Metropolis decision probability: 
                        \begin{equation*} 
                            r_q = \min\left[ 1, \frac{\omega(\theta_1^*,\theta) + \cdots \omega(\theta_k^*,\theta)}
                                                     {\omega(\theta_1,\theta^*) + \cdots \omega(\theta_k,\theta^*)}
                                  \right]
                        \end{equation*}
                        or reject with probability $1-r_q$.
            \end{bayeslist}
            Implement this algorithm in \R\ to produce samples from a target density $\pi(\theta)$ that is a correlated central 
            bivariate Student's-$t$ distribution with $\rho=0.9$ and $\nu=5$:
            \begin{equation*}
                \pi(\theta_a,\theta_b) = \frac{1}{2\pi\sqrt{1-\rho^2}}
                                         \left[ 1 + \frac{ \theta_a^2 + \theta_b^2 - 2\rho\theta_a\theta_b }
                                                         { \nu(1-\rho^2) } \right]^{-\frac{\nu}{2}-1}
            \end{equation*}
            (see Appendix~B for a more general multivariate non-central definition).
            \index{subjectindex}{posterior distribution!Student's-$t$}
            \index{subjectindex}{Student's-$t$}

    \item   (Norris\index{authorindex}{Norris, J. R.} 1997) A Markov chain is \emph{reversible} if the distribution of 
            $\theta_{n}|\theta_{n+1}=t$ is the same as the $\theta_{n}|\theta_{n-1}=t$.  
            \index{subjectindex}{Markov chain!reversible}
			This means that direction of time does not alter the properties of the chain.  Show that the following irreducible 
            matrix does not define a reversible Markov chain.
			\begin{equation*}
        				\left[ \begin{array}{rrr} 0 & p & 1-p \\ 
								  1-p & 0 & p \\ 
								  p & 1-p & 0 \\ \end{array} \right].
					\nonumber
			\end{equation*}
			See Besag\index{authorindex}{Besag, J.} \etal (1995) for details on reversibility.

            % NEW
    \item   \label{chib.and.greenberg.exercise}
            (Chib and Greenberg 1995). \index{authorindex}{Chib, S.} \index{authorindex}{Greenberg, E.}
            To produce draws from a bivariate normal target distribution that is normal with:
            \begin{equation*}
                \mu=c(1,2), \qquad\text{and}\qquad 
                    \SI= \left[ \begin{array}{cc} 1.0 & 0.9 \\ 0.9 & 1.0 \end{array} \right], 
            \end{equation*}
            write a Metropolis-Hastings algorithm
            in \R\ with the following alternative candidate-generating strategies with the goal of a 40\% to 50\% acceptance rate:
            \begin{bayeslist}
                \item   a random walk where the offset is a bivariate uniform with ranges $[-0.75\range 0.75]$ for the first
                        dimension and $[-1,1]$ for the second dimension.
                \item   a random walk where the offset is a bivariate normal with mean $(0,0)$ and variance
                        $\left[ \begin{smallmatrix} 0.6 & 0 \\ 0 & 0.4 \end{smallmatrix} \right]$.
                \item   a pseudorejection scheme where independent candidates are filtered through an acceptance-rejection
                        \index{subjectindex}{acceptance-rejection method} step using the dominating function given by \\
                        $ch(x) c(2\pi)^{-1}|\D|^{-\half}\exp\left[-\half (x-\mu)'\D(x-\mu)\right]$, where 
                        $D = \left[ \begin{smallmatrix} 2 & 0 \\ 0 & 2 \end{smallmatrix} \right]$ and $c=0.9$ (use $\mu=c(1,2)$
                        from above).
                \item   an autoregressive density with $\theta^{[t+1]} = \mu - (\theta^{[t]} - \mu) + \OM$, where $\OM$ is
                        an independent bivariate uniform drawn in both dimensions from $[0\range 1]$.  Note that this reflects 
                        the draw to the other side of $\mu$ then applies an offset.
            \end{bayeslist}

    \item   (Grimmett and Stirzaker 1992) \index{authorindex}{Grimmett, G. R.}\index{authorindex}{Stirzaker, D. R.}
			A random walk\index{subjectindex}{random walk} is recurrent if the mean size of the jumps is zero.  Define a 
            random walk\index{subjectindex}{random walk} on the integers by the transition from integer $i$ to either 
            integer $i+2$ or $i-1$ with probabilities:
			\begin{equation*}
					p(i,i+2) = p, \qquad	p(i,i-1) = 1-p.	\nonumber
			\end{equation*}
			A random walk\index{subjectindex}{random walk!recurrent} is recurrent if the mean recurrence time, 
            $\sum nf_{ii}(n)$, is finite, otherwise it is transient.  What values of $p$ make this random walk recurrent?	

            % NEW
    \item   The following data give the number of casualties for 103 suicide attacks in Israel with explosives over a three-year 
            period from November 6, 2000 to November 3, 2003 when there was a steep drop (the early period of the first 
            ``Intifada'').  These data are provided by the International Policy Institute for Counter-Terrorism, and subsetted
            by Harrison (2006), \index{authorindex}{Harrison, M.} and modeled in Kyung \etal (2011).\\
            \index{authorindex}{Kyung, M.}
            % NEED TO ADD TO BaM [added 5/14/2015]

            \blstable
            \parbox[l]{\linewidth}{
            \begin{normalfont}
            \begin{tabular}{rrrrrrrrrr}
                0  &  3& 81& 38& 29&126&  6& 10&  1&  1 \\
                67 & 50&  3& 27&  0&  2&  0& 63& 15& 58 \\
                57 &  0&  0&  0&  0&123&  4& 71& 71& 20 \\
                17 & 65&  4& 49&  5& 35& 57& 71&  0& 12 \\
                67 & 59&  5& 52& 62&  0& 75&  0&  0&106 \\
                30 &  0&  3& 45&  4& 31& 32&180&  0&  1 \\
                91 & 49& 61& 51&  3&  0&  1&  9&  0&  2 \\
                151& 26&  8&  8& 75&199& 12&  2&  2&  1 \\
                93 &  0& 13& 21&145&  0&  0& 13&  0&  2 \\
                141&  2& 65&  0&105&  0& 61&  6& 27& 53 \\
                20 &  5&  0&   &   &   &   &   &   &    \\
            \end{tabular}
            \end{normalfont}
            \vspace{11pt}
            }
            \bls

            There are two modes, with the larger one at zero.  Fit a two-component gamma mixture model using data augmentation for 
            $p$ (the probability of being in the right-mode) with a beta distribution, and with a binary assignment vector as part        
            of the Gibbs sampler.  At iteration $t$ the steps are:
            \begin{bayeslist}
                \item   Draw $p$ from $\mathcal{BE}(\alpha + n_1, \beta + n_2)$.
                \item   For each case $i$, draw $I_i \sim \mathcal{BR}(p)$.
                \item   If $I_i = 0$, draw from $\mathcal{G}(\alpha_0,\beta_0)$.
                \item   If $I_i = 1$, draw from $\mathcal{G}(\alpha_1,\beta_1)$.
                \item   Determine $n_1$ and $n_2$ from the $I_i$ assignments.
            \end{bayeslist}
            Here $\alpha$, $\beta$, $\alpha_0$,$\beta_0$, $\alpha_0$, and $\beta_0$ are hyperprior parameters and $n = n_1 + n_2$
            at that point in the sampler (assign starting values).
\end{exercises}

\section{Computational Addendum: Simple \texttt{R} Graphing Routines for MCMC}\
This addendum gives the background for the graphing \R\ code in this chapter.  In the next chapter we develop Bayesian MCMC
solutions for more complex, and realistic, models The \bugs\ package is recommended in general for implementing MCMC estimation
models.

Figure~\ref{Gibbs.Fig} was produced using the following function for graphing the path of a Gibbs sampler above in two chosen
dimensions.  You must give it values indicating  which two columns of the input matrix to graph since it is a two-dimensional
plot.  For larger dimensional joint posteriors, modifications to this function are easy to perform.
\index{subjectindex}{R@\R!plotting Markov chains}

\begin{R.Code}
plot.walk.G <- function(walk.mat,sim.rm,X=1,Y=2)  \{
    plot(walk.mat[1,X],walk.mat[1,Y],type="n", xlim=range(walk.mat[,X]),
        ylim=range(walk.mat[,Y]), xlab="",ylab="")
    for(i in 1:(nrow(walk.mat)-1))  \{
        segments(walk.mat[i,X],walk.mat[i,Y],
                 walk.mat[(i+1),X],walk.mat[i,Y])
        segments(walk.mat[(i+1),X],walk.mat[i,Y],
                 walk.mat[(i+1),X],walk.mat[(i+1),Y])
    \}
\}
\end{R.Code}

% postscript("Book.Bayes.III/Images/mcmc_intro.figure02.ps")
\begin{R.Code}
par(mfrow=c(1,2),mar=c(2,3,1,1),oma=c(1,1,3,1))
plot.walk.G(theta.matrix[1:100,],X=1,Y=3)
mtext(outer=TRUE,side=2,cex=1.3,expression(k))
mtext(outer=FALSE,side=3,cex=1.3,expression(lambda),line=2)
plot.walk.G(theta.matrix[1:100,],X=2,Y=3)
mtext(outer=FALSE,side=3,cex=1.3,expression(phi),line=2)
\end{R.Code}
% dev.off()


The following \R\ code produced Figure~\ref{Metrop.Fig} for this Metropolis-Hastings output in
two dimensions.  It takes a matrix where the rows indicate chain iterations.

\begin{R.Code}
plot.walk.MH <- function(walk.mat)  \{
    plot(walk.mat[1,1],walk.mat[1,2],type="n",
        xlim=round(range(walk.mat[,1])*1.2),
        ylim=round(range(walk.mat[,2])*1.2),
        xlab="",ylab="")
    for(i in 1:(nrow(walk.mat)-1))  \{
        segments(walk.mat[i,1],walk.mat[i,2],
                 walk.mat[(i+1),1],walk.mat[(i+1),2])
    \}
\}
\end{R.Code}

% postscript("Book.Bayes.III/Images/mcmc_intro.figure03.ps")
\begin{R.Code}
par(mfrow=c(1,2),mar=c(2,2,2,2),oma=c(1,1,3,1))
plot(theta.matrix[801:1000,],pch=".",xlab="",ylab="",
    xlim=c(-3,3), ylim=c(-3,3),cex=3)
plot.walk.MH(theta.matrix[801:1000,])
mtext(outer=TRUE,side=3,cex=1.2,
    "Metropolis-Hastings Demonstration, Bivariate Normal")
\end{R.Code}
% dev.off()

The previous function needs to produce bivariate normal density values, and a function for doing
this follows.  It is currently written only for bivariate calculations, but it can easily be
vectorized to accommodate higher dimensions.\index{subjectindex}{R@\R!bivariate normal density}

\begin{R.Code}
dmultinorm <- function(xval,yval,mu.vector,sigma.matrix)  \{
   normalizer <- (2*pi*sigma.matrix[1,1]*sigma.matrix[2,2]
                  *sqrt(1-sigma.matrix[1,2]^2))^(-1)
   like <- exp(-(1/(2*(1-sigma.matrix[1,2]^2)))* (
               ((xval-mu.vector[1])/sigma.matrix[1,1])^2
               -2*sigma.matrix[1,2]*(((xval-mu.vector[1])
               /sigma.matrix[1,1])*
               ((yval-mu.vector[2])/sigma.matrix[2,2]))
               +((yval-mu.vector[2])/sigma.matrix[2,2])^2 ))
   normalizer*like
\}
\end{R.Code}

This last code segment implements the Hit-and-Run example along the graph on page~\pageref{Hit.and.Run.Demo}.

%postscript("Book.Bayes.III//Images/mcmc_intro.figure04.ps")
\begin{R.Code}
num.sims <- 10000
Sig.mat <- matrix(c(1.0,0.95,0.95,1.0),2,2)
walks<-rbind(c(-3,-3),matrix(NA,nrow=(num.sims-1),ncol=2))
walks <- hit.run(walks,num.sims,Sig.mat)
z.grid <- outer(seq(-3,3,length=100),seq(-3,3,length=100),
	                FUN=dmultinorm,c(0,0),Sig.mat)
contour(seq(-3,3,length=100),seq(-3,3,length=100),z.grid,
                    levels=c(0.05,0.1,0.2))
points(walks[5001:num.sims,],pch=".")
\end{R.Code}
%dev.off()


\thispagestyle{empty}
\chapter{Assessing Model Quality}\label{Model.Quality.Chapter}
\setcounter{examplecounter}{1}

\section{Motivation}
The third step in Bayesian analysis is to evaluate the model fit to the data and determine the sensitivity of the posterior distribution 
to the assumptions.  \index{subjectindex}{evaluating model fit} This is typically an interactive and iterative process in which different 
approaches are taken until there is reasonable evidence that the conclusions are both statistically reliable \emph{and} stable under 
modest changes in the assumptions.  So the emphasis in this chapter is on \emph{model adequacy}: the suitability of a single model under
consideration.  The subsequent chapter is really about \emph{model testing}: determining why we should prefer one model specification
over another.  Both of these are critical considerations, and it is important to keep them distinct.

Model checking is a critical part of the estimation process since there is nothing in the procedures outlined in previous chapters that 
would \emph{per se} prevent one from producing incorrect and overly sensitive posterior distributions for the unknown parameters of 
interest.  In George Box's\index{authorindex}{Box, G. E. P.} (1995) words: ``Statistics has no reason for existence except as a catalyst 
for scientific inquiry in which only the last stage, when all the creative work has already been done, is concerned with a final fixed 
model and rigorous test of conclusions.''  Furthermore, Box (1980) also notes that ``No statistical model can safely be assumed
adequate.''  Accordingly, this chapter provides various methods for assessing the quality of ``final''
models in the sense that other considerations, such as data collection, parametric specification, and variable selection, have already
been determined and our goal is to understand the quality of those choices.

The quality of a Bayesian posterior inference is attributable to three model assignments: the prior, the likelihood function, and the 
loss function (if the latter is explicitly specified).  \index{subjectindex}{loss function} \index{subjectindex}{likelihood function}
Prior specifications include the assigned parametric form as well as the prior distribution's parameter values, and in the case of hierarchical 
models the \emph{hyperprior specifications}:\index{subjectindex}{hyperprior!specification} higher level prior distributions assigned to 
the first level of priors.  For a given prior, there is a range of possible outcomes, including: the prior is subsumed by a large data 
set and therefore not particularly important, the prior is moderately influential even with reasonable sample size, and the prior strongly 
affects the form of the posterior.  The second model assignment is the specified parametric form of the likelihood function,  which itself 
ranges from relatively unimportant to mostly determining the shape of the posterior.  Finally, if a loss function is stipulated, the 
characteristics of this function can also affect posterior
	conclusions.\footnote{The implications of loss function robustness are not considered here in detail and the reader is referred 
	to the general discussion on page~\pageref{bayesian.decision.theory.section}, and to: Dey and Micheas (2000), Dey, Lou, and Bose 
	(1998), Mart\'{\i}n, R\'{\i}os Insua, and Ruggeri (1998), and Ramsey and Novick (1980).  Kadane and Chuang (1978), extended 
	in Kadane and Srinivasan (1996), provide an integrated way of looking at the prior and the loss function together by introducing 
	the broader idea of posterior stability.}
\index{authorindex}{Dey, D. K.}%
\index{authorindex}{Micheas, A.}%
\index{authorindex}{Lou, K.}%
\index{authorindex}{Bose, S.}%
\index{authorindex}{Rios Insua, D.@R\'{\i}os Insua, D.}
\index{authorindex}{Ramsey, J. O.}%
\index{authorindex}{Novick, M. R.}%
\index{authorindex}{Kadane, J. B.}%
\index{authorindex}{Ruggeri, F.}%
\index{authorindex}{Chuang, D. T.}%
\index{authorindex}{Srinivasan, C.}%
\index{subjectindex}{loss function!robustness}%

\index{subjectindex}{local robustness}     \index{subjectindex}{model checking}    \index{subjectindex}{model checking} 
\index{subjectindex}{sensitivity analysis} \index{subjectindex}{global robustness}
After looking at some simple comparative methods, this chapter focuses on three related approaches to model checking in applied Bayesian 
work: sensitivity analysis, global robustness, and local robustness.  
Sensitivity analysis is the \emph{informal} process of altering assumptions according to researcher intuition with the objective of 
determining the extent to which these changes modify the posterior distribution.  In particular, does varying the prior parameters or 
modestly changing the form of the prior itself lead to vastly different conclusions from the posterior?  Robustness is posterior 
insensitivity to broad classes of user-specified assumptions and is therefore a desirable quality of Bayesian 
	models.\footnote{Since robustness increases automatically with increases in sample size, then robustness evaluation is less  
	important in models with large samples.  Conversely, models with modest sample size (say less than 100, but this is 
	data-dependent) should be thoroughly analyzed in this regard.}
\index{subjectindex}{robustness!definition}
Robustness evaluation is the \emph{systematic} process of determining the degree to which posterior inferences are affected by both 
potential misspecification of the prior and influential data points.  It is actually important to remember the distinction between
sensitivity analysis and robustness (Skene, Shaw, and Lee 1986).\index{authorindex}{Skene, A. M.}\index{authorindex}{Shaw, E. H.}
\index{authorindex}{Lee, T. D.} Global robustness evaluation 
performs a formal analysis of a large class of priors to determine the subsequent \emph{range} of the inferences.  
Local robustness generally uses differential calculus to determine the volatility of specific reported results.  

The distinction between global and local robustness does not need to be confined to the prior distributions under consideration for a 
Bayesian model.  Any aspect of the specification is equally suspect and therefore deserving scrutiny.  Smith
\index{authorindex}{Smith, A. F. M.} (1986), for example, expresses \emph{model criticism} distinctness in very general terms 
(quoting, p.97):\index{subjectindex}{model criticism}
\begin{bayeslist}
	\item	\emph{global} criticism, which basically asks the question ``should we abandon the current
		framework completely, despite the fact we have nothing at all to propose in its place?''
	\item	\emph{local} criticism, which asks ``should the model be modified or extended in this or
		that particular direction?''
\end{bayeslist}
The definition of global criticism may be a little bit stark here since it is almost always true that the
researcher has potential alternatives.   Smith actually argues that his given characterization of global robustness
checking is too limited and not particularly Bayesian in spirit.  Note that this is essentially the Fisherian
setup where models are tested against a vague null concept that is not specified but represents a lack of systematic
effect.  The underlying philosophy that undergirds either approach is that a single model applied to some data
produces a single conclusion, but a range of models applied to some data produces a range of conclusions.  The
degree to which this range of conclusions differs for modest changes in basic assumptions warns the researcher
about the delicateness of their findings.

This chapter also covers the posterior predictive distribution\index{subjectindex}{posterior predictive distribution} as a way to
investigate the quality of models through unseen data that they imply.  Since all unknown quantities are treated
probabilistically, Bayesian posterior results can be averaged across models to give more robust coefficient estimates as they
cover multiple-model space.  Bayesian model averaging\index{subjectindex}{Bayesian model averaging} can therefore be used to
reduce the uncertainty inherent in presenting a single specification.  Chapter~\ref{Testing.Chapter} also shows how the
Kullback-Leibler\index{subjectindex}{Kullback-Leibler distance} distance can be used to compare distributional distances.  This
allows us to measure differences between priors and posteriors.

\subsection{Posterior Data Replication}
One very simple way to evaluate model quality is to produce a replicate dataset under the estimated model and compare it to the 
observed data using summary statistics or graphical analysis.  This can be done analytically, but is much easier with simulation.
While the core ideas of Monte Carlo simulation and MCMC estimation are covered in later chapters, we have been systematically
introducing some of these ideas throughout the early part of this text.  Here we will see a preview of two important tools in 
this section: the \bugs\ language for estimating Bayesian models with Markov chain Monte Carlo, and the use of simulation across
posterior distributions as a means of describing the implications of the model.  

The process works as follows.  \emph{First}, produce
a posterior distribution for all unknown quantities in the customary fashion, where the outcome variable vector of interest, $\y$, is
modeled by observed predictors, $\X$, and parameters $\B$, according to the general specification $f(\y|\X,\B)$.  \emph{Second}, fix $\X$
at these observed values and draw $m$ replicated values of $\B$ from the corresponding posterior distribution: $\B^{(1)}, \B^{(2)},
\ldots,\B^{(m)}$.  This last step can be done analytically, but is almost trivially easy if the model has been estimated with
MCMC procedures described already on pages~\pageref{first.gibbs} and \pageref{cumsum.wars.fig}, since these procedures give empirical
draws from the coefficient posterior distributions and we can simply sample from these.  \emph{Third}, take such a sample of posterior
draws and treat them as fixed to fully define a single version of the posterior model and calculate the implied outcome variable vector,
$f(\y^{\text{rep}(i)}|\X,\B^{(i)})$, for posterior draws $i=1,2,\ldots,m$.  \emph{Finally}, plot and compare the $\y^{\text{rep}(i)}$ with the
observed $\y$ where large deviations imply poor model fit.  There are obviously many variations to this procedure that correspond
to different model characteristics (see the discussion in Gelman and Hill [2007, pp.517-524], with \R\ and \bugs\ code).

\begin{examplelist} % NEED TO ADD TO BaM
	\item   {\bf Posterior Comparison for a Model of Abortion Attitudes in Britain.}\label{example:abortion.attitudes}
            \index{subjectindex}{example!abortion attitudes}
            To illustrate this model-checking procedure, we develop a model of support for abortion under different scenarios
            using survey data from Britain in consecutive years from 1983 to 1986.  The panel data for 264 respondents is
            collected annually by McGrath and Waterton (1986) where seven scenarios are provided and these respondents have the
            option of expressing support or disagreement for abortion (see also the reanalysis by Knott \etal [1990]).  The full
            collection of these seven queries do not fall into an obvious ordinal scale, so we will treat them here as nominal and
            judge total support for abortion as a binomial test for each respondent at each wave of the panel.  The scenarios are:
            (1) the woman decides on her own that she does not wish to have the child, (2) the couple agree that they do not wish
            to have the child, (3) the woman is not married and does not wish to marry the man, (4) the couple cannot afford any
            more children, (5) there a strong chance that the baby has a biological defect, (6) the woman's health is seriously
            endangered by the pregnancy, and (7) the woman became pregnant as a result of rape.  So each respondent has the
            ability to produce an outcome from $0$ to $7$.  These data are available in the \R\ package \texttt{BaM}.

		Naturally we do not want to treat the repeated annual trials as independent since this would ignore correlation within 
		subjects.  So the model is given by the following specification for $i=1,\ldots,264$ respondents across $j=1,\ldots,4$ 
		panel waves:
		\begin{align}\label{abortion.model.specification}
			y_{ij}               &\sim \mathcal{BN}(n,p_{ij})	& & \nonumber\4
			\text{logit}(p_{ij}) &= \B_{0,j} + \B_{1,i}X_{1,i}	& & \nonumber\4
			\B_{0,j} 	     &\sim \mathcal{N}(\mu_0, \tau_0)	& 
			\B_{1,i} 	     &\sim \mathcal{N}(\mu_1, \tau_1)	& & \nonumber\4
			\mu_0 	     	     &\sim \mathcal{N}(0,100)		&
			\mu_1 	     	     &\sim \mathcal{N}(0,100)		& & \nonumber\4
			\tau_0		     &\propto \mathcal{C}_{half}(25)	&
			\tau_1		     &\propto \mathcal{C}_{half}(25)	
		\end{align}
		where the second term in the normals is a variance and half-Cauchy priors are positive-support, zero-centralized forms 
        (location)
		with a scale term equal to $A=25$, $f(\tau) = (1+\tau/A)^{-1}$, $\tau>0$.  This is a form recommended by Gelman (2006)
		as an alternative to gamma distributions with small parameters for providing low-information priors for variances terms.
		Here $X_{1,i}$ is the $i$th person's self-identified religion: Catholic (1), Protestant (2), Other (3), and No Religion
		(4), and $n=7$ at each wave for each person.  The \jags\ code is:
        \begin{Bugs.Code}
model  \{
    for (i in 1:PEOPLE)  \{
        for (j in 1:WAVES)  \{
            logit(p[i,j]) <-  b0[j] + b1[i]*x1[i];
            r[i,j] ~ dbin(p[i,j], n[i]);
        \}
        b0[i] ~ dnorm(mu0, nu0);
        b1[i] ~ dnorm(mu1, nu1);
    \}
    mu0   ~ dnorm(0.0,1.0E-2);
    mu1   ~ dnorm(0.0,1.0E-2);
    tau0  ~ dnorm(0,1)T(0,);
    tau1  ~ dnorm(0,1)T(0,);
    sigma ~ dgamma(2,2);
    nu0   <- 1/(25*tau0/sqrt(sigma));
    nu1   <- 1/(25*tau1/sqrt(sigma));
\}
\end{Bugs.Code}
        While this is getting a little ahead of ourselves, it is useful to look at the \jags\ code before
        Chapter~\ref{Software.Chapter}. Notice that the code inside the ``for'' loop is essentially how we would describe the
        model statistically on paper. That is, $p$ comes from a linear additive component with a link function, then the outcome
        variable is given a distributional assumption.  The term \texttt{n[i]} is indexed for generality but does not need to be
        since all respondents are given seven statements at each wave.  The Gibbs sampler is run for 20,000 iterations dispensing
        with the first 10,000 values.  All convergence diagnostics point towards convergence.  This is a hierarchical model in the
        sense of those described in Chapter~\ref{Hierarchical.Chapter}, so in addition to obtaining posterior distributions for
        the two $\mu$ and $\tau$ parameters, there will be $n=264$ posterior distributions for each $\B$.  Therefore we will not
        report the full posterior results beyond Table~\ref{Abortion.Table}, and will concentrate instead on assessing fit.  

		\begin{table}[h] 
        \parbox[c]{\linewidth}{
            \begin{center}
            \hspace{-55pt}
		    \tabletitle{\textsc{Model Summary, Abortion Attitudes in Britain}}\label{Abortion.Table}
		    \renewcommand{\arraystretch}{1.2}
		    \begin{tabular}{lrrrrr}
					& \multicolumn{5}{c}{Posterior Quantiles} \\ \cline{2-6} 
		                           & 0.025    & 0.25      & 0.50     & 0.75   & 0.975 \\
		    \hline
			    $\mu_0\qquad\quad$ & -0.6708 & -0.2563 & -0.0294 & 0.1960 & 0.6245	\\
			    $\tau_0$           &  0.0283 &  0.0500 &  0.0641 & 0.0789 & 0.1113	\\
			    $\mu_1$		   &  0.3029 &  0.4129 &  0.4728 & 0.5291 & 0.6336	\\
			    $\tau_1$	   &  0.0303 &  0.3136 &  0.6726 & 1.1507 & 2.2453	
		    \end{tabular} \end{center} 
        }
        \end{table} \bls

		More importantly, consider Figure~\ref{Data.Comparison.Fig} where the jittered observed data values from the last wave
		of the panel are plotted against nine (uniformly) randomly selected jittered iterations from the recorded Markov chain.
		This is not a \emph{test} for fit but indicates both positive and negative aspects of the results.  The closer the 
		results are to the upward sloping diagonal, the better the fit.  Thus for high support, the model fits well and for low
		support the model fits less well.  This makes sense since the questions are diverse and mixed support is therefore much
		harder to model than uniformly or nearly-uniformly strong support.  Notice also the few values in the radically wrong
		corners of the graphs.  More covariate information in the data would likely tighten the fit around the diagonal.  Note that
		in the case where the outcome variable of interest is continuously measured, the graphical display is more straightforward
		with lines or points instead of these discrete categories.

        	\begin{figure}[h]
        	\vspace{4pt}
                \centerline{ 
                	\epsfig{file=Images/robust.figure01.ps,height=4.65in,width=3.35in,clip=,angle=270}  }
        	\caption{\textsc{Outcome Comparison: Observed versus Simulated}} \label{Data.Comparison.Fig}
        	\end{figure}
\end{examplelist}

\index{authorindex}{Fahrmeir, L.} \index{authorindex}{Tutz, G.} \index{authorindex}{Gill, J.}
\index{authorindex}{McCullagh, P.} \index{authorindex}{Nelder, J. A.}
\subsection{Likelihood Function Robustness}
The specification of the likelihood function is generally not very controversial in specifying Bayesian or non-Bayesian models
because it is often narrowly defined by the form of the outcome variable and is well-studied as a contributor to model quality in
the generalized linear models literature (Fahrmeir and Tutz 2001; Gill 2000; McCullagh and Nelder 1989).  Conversely, frequentist
criticism (and therefore Bayesian defensiveness) is often centered on the posterior implications of prior assumptions.  It should
be noted, however, that the likelihood function component of the model is no less suspect than the prior specification and worth
attention as well since nearly all researchers ``directly announce the likelihood itself without deriving it''
(Poirer\index{authorindex}{Poirer, D. J.} 1988, p.131). Also, the likelihood selection process is no less subjective than the
prior selection process (de Finetti 1974, 1975).  \index{authorindex}{de Finetti, B.} \index{subjectindex}{likelihood!robustness}
For example, the selection of a Poisson link function versus a negative binomial link function accords significantly less
attention and defensive efforts than the choice of a prior for the same model.

Shyamalkumar\index{authorindex}{Shyamalkumar, N. D.} (2000) suggests a means of checking likelihood robustness across related likelihood 
classes, and he shows that it is relatively simple to define a finite class of related likelihood models with greater robustness properties.  
For instance, it is well known that prior specifications with wider tails have better robustness properties than conjugate choices 
(Berger\index{authorindex}{Berger, J. O.} 1984, 1985).  So Shyamalkumar recommends a Cauchy comparison class by matching up normal and 
Cauchy quantiles (for instance the interquartile range of a $\mathcal{N}(\theta,1)$ distribution matches those of a 
$\mathcal{C}(\theta,0.675)$).\label{cauchy.matching}  It is also possible to formulate this neighborhood specification nonparametrically 
(Lavine\index{authorindex}{Lavine, M.} 1991a).  

\section{Basic Sensitivity Analysis} \index{subjectindex}{sensitivity analysis|(}
A very simple and helpful method for assessing posterior model quality is to vary the
prior or other assumptions in some \emph{ad hoc} but intuitive way and observe the 
implications with respect to posterior quantities of interest.  If reasonably large 
changes in model assumptions are seen to have a negligible effect on the calculated 
posterior density, then we can comfortably establish that the data are sufficiently 
influential or that the varied assumption is sufficiently benign (or perhaps both) 
to eliminate further worry about the subjective influence of assumptions.  Conversely, 
a far more alarming scenario occurs when one makes mild changes to the established 
model assumptions and dramatic changes are observed in the summary measures of the 
posterior.

\index{authorindex}{Kong, A.} \index{authorindex}{Liu, J. S.} \index{authorindex}{Wong, W. H.}
\subsection{Global Sensitivity Analysis} \index{subjectindex}{sensitivity analysis!global}
There are essentially two types of Bayesian sensitivity analysis: global and local (Leamer\index{authorindex}{Leamer, E. E.} 1978).  Global 
sensitivity analysis is a broad approach that evaluates a wide range of: alternative prior specifications 
(Berger\index{authorindex}{Berger, J. O.} 1984, 1990), forms of the link function (Draper\index{authorindex}{Draper, D.} 1995), missing data 
implications (Kong, Liu, and Wong 1994), 
error sensitivity (Polasek 1987), and perturbations of the likelihood and prior specifications (Kass\index{authorindex}{Kass, R. E.} and 
Raftery\index{authorindex}{Raftery, A. E.} 1995).  The purpose of global sensitivity is to vary the \emph{widest} possible range of 
assumptions, although some authors have used the term to describe analysis of the sensitivity of the posterior to differences provided 
within a given \emph{family} of priors (Polasek\index{authorindex}{Polasek, W.} 1987).

Leamer's\index{authorindex}{Leamer, E. E.} original objective in global sensitivity analysis is to determine the maximum amount
that the model assumptions can be varied without dramatically changing the posterior inferences (1985, p.308).  Thus substantive
inferences are deemed to be reliable only if the range of assumptions is wide enough to be realistic and the corresponding
posterior inferences are narrow enough that they provide consistent results.  This is a naturally appealing idea because it would
indicate which specifications are relatively ``fragile'' with respect to researcher-specified model assumptions, although Pagan
(1987) \index{authorindex}{Pagan, A.} believes that this approach does not go far enough since it is restricted to altering the
parameters of the prior distribution only.  Unfortunately the idea of absolute global sensitivity analysis opens a vast array of
alternative specifications and tests to consider, even restricting oneself to the prior only.  Often there are too many of these
alternatives to consider in a reasonable amount of time, and there is also the additional challenge of clearly reporting these
results to readers.

From many published works, we can see that the obvious, but not solitary, target of sensitivity analysis is the form of the prior
distribution.  Routinely, a flat prior of some sort is substituted for an informative prior.  \index{subjectindex}{prior
distribution!informative}  The observed change in the posterior can range from trivial to substantial.  It should be noted,
however, that if the informed prior has a substantial theoretical basis, large changes to the posterior are not \emph{a priori} a
sufficient reason to disqualify it from the analysis.  Conversely, strongly informative priors that are specified purely for
mathematical convenience should be evaluated carefully if they produce a radically different posterior form than a reference prior
of some kind.  \index{subjectindex}{prior distribution!informative}

\subsubsection{Specific Cases of Global Prior Sensitivity Analysis} \index{subjectindex}{sensitivity analysis!global!priors}
The classical prior juxtaposition is to specify an alternative uninformative prior (typically uniform) over the support of the parameter 
of interest and compare it with the stipulated informative prior.  If there is a substantial difference in the form of the posterior, 
then this is something that often needs to be explained to readers.  It is not necessary to use the uniform as a comparison prior, but
many researchers find it to be a convenient comparison form.  An obvious question arises regarding how much of a change in the posterior
results indicates strong prior influence from the informative prior.  First, if the difference changes the \emph{substantive} conclusions
from the model, then the informed prior should be earnestly defended.  Consider the difference in the posteriors for Teacher Salaries
in the two models of educational effects in Section~\ref{example:education.effects}: a posterior mean and standard deviation of
$(0.073, 0.053)$ with uninformed priors (Table~\ref{replication.table}), versus $(0.382, 0.099)$ with informed priors.  With a sample
size of 7301 cases we can safely assume that the posteriors are normally distributed for this linear model specification.  Accordingly,
the first posterior has 0.0842 of the density below zero while the second posterior has 0.0006 of the density below zero.  
Often the differences are not this stark, and sometimes the differences are merely trivial.  In such settings we would be less 
concerned with the influence of the prior distribution since the shrinkage of the posterior to the prior is small.
\index{subjectindex}{prior distribution!informative} \index{subjectindex}{prior shrinkage}

\index{authorindex}{Lange, K. L.} \index{authorindex}{Little, R. J. A.} \index{authorindex}{Taylor, J. M. G.} 
If the researcher-specified prior is of the normal form, then $t$-distributions, varying by the degrees of freedom, can be used as wider 
tail alternatives (Lange, Little, and Taylor 1989).  One advantage of the $t$-distribution approach is that it avoids problems associated 
with specifying improper uniform priors over the real numbers.  Jeffreys\index{authorindex}{Jeffreys, H.} (1961) recommends using the 
Cauchy distribution as a prior in this context since its very heavy tail structure implies a high level of conservatism, although there 
is often not a substantial comparative difference between an informative prior relative to a uniform prior and an informative prior 
relative to a Cauchy prior.\index{subjectindex}{prior distribution!informative}

\subsubsection{Global Sensitivity in the Normal Model Case} \index{subjectindex}{sensitivity analysis!global!normal model}
A form of sensitivity analysis with the normal model was performed previously with simulated data in Table~\ref{Sim.Table.1}.  That example 
demonstrated that an intentionally misspecified prior affected posterior quantities of interest: HPD regions and point estimates.  The 
obvious point was that as the sample size increased from 10 to 1,000, the misspecification of the prior parameters on the normal prior 
mattered considerably less.

Suppose we assign an infinitely diffuse prior\index{subjectindex}{prior distribution!diffuse} for an unknown mean in the normal model 
discussed in Chapter~\ref{Normal.Model.Chapter} as a way to assess the impact of some other imposed prior.  The chosen prior is 
$p(\mu) = 1, \; \mu \in [-\infty\range\infty]$, meaning that a density of one is assigned for values of $\mu$ along the entire real line.   
An important question is whether it is even possible to achieve a comparative posterior distribution with such an improper prior.  The 
posterior here is proportional to the likelihood in the following simple manner:
\begin{align}
    \pi(\mu|\mathbf{x}) &\propto p(\mathbf{x}|\mu)p(\mu)  \nonumber \\[5pt]
    	&= \prod_{i=1}^{n}\exp\left[-\frac{1}{2\sigma_0^2}(x_i-\mu)^2\right]\times 1
            \nonumber  \\
        &= L(\mu|\mathbf{x}),       
\end{align}
which can be shown to be finite over the real line:
\begin{equation}\label{finite.mean.posterior}
    \int_{-\infty}^{\infty}L(\mu|\mathbf{x})d\mu
        = \int_{-\infty}^{\infty} \prod_{i=1}^{n}(2\pi\sigma_0^2)^{-\frac{n}{2}}
            \exp\left[ \frac{1}{-2\sigma_0^2}(x_i - \mu)^2 \right] d\mu,
\end{equation}
with $\sigma^2_0$ known.  Therefore this prior leads to a posterior that is just the same conclusion that a non-Bayesian likelihood 
analysis would produce.  This is interesting because it is one of a few cases where Bayesian analysis and non-Bayesian analysis agree 
perfectly, despite starting from different assumptions.

\subsubsection{Example: Prior Sensitivity in the Analysis of the 2000 U.S. Election 
	in Palm Beach County} \index{subjectindex}{example!2000 presidential election}
Returning again to the linear regression example of the voting results from the 2000 presidential election (see
page~\pageref{pbc.model.code} forward in Chapter~\ref{Linear.Chapter} for the setup \R\ code), we can graphically compare the
posterior implications of the uniform prior specification versus the conjugate prior developed in
Section~\ref{linear.regression.model.section} as means of global sensitivity analysis.  

\begin{figure}[h]
	\centerline{ \epsfig{file=Images/robust.figure02.ps,height=4.90in, width=3.35in,clip=,angle=270}  }
	\vspace{-18pt}
	\caption{\textsc{Marginal Posteriors, Palm Beach County Model}}\label{PBC.Prior.Sensitivity.Fig}
\end{figure}

Figure~\ref{PBC.Prior.Sensitivity.Fig} shows the marginal posterior density for each posterior dimension (omitting the constant) where 
the dotted line results from the conjugate prior and the solid line results from the uniform prior.  It is clear from these figures that 
there is very little difference in the marginal posteriors for the two prior specifications with the possible exception of the 
\texttt{technology} variable.  Despite these similarities, it is important to note that there may be substantive reasons to prefer one
over the other in terms of what is being assumed about voter behavior.

\subsubsection{Problems with Global Sensitivity Analysis} \index{subjectindex}{sensitivity analysis!global!problems}
To Leamer\index{authorindex}{Leamer, E. E.} (1984, 1985) such analysis is ``global'' when the test of sensitivity to the form of 
the prior includes a wide variety of prior forms.  The motivation is that no single prior can be assumed to fully describe prior knowledge 
or expert opinion about the distribution of parameters, and therefore specifying a neighborhood of priors around the original gives an 
important picture of the ramifications of uncertain prior information.  Furthermore, if in order to get a suitably narrow HPD region, the 
set of prior neighborhood specifications had to be severely limited, then this would also be evidence of fragility.

Global sensitivity is often very difficult to achieve in practice  because it is not always clear what qualifies as a ``neighborhood prior,'' 
and what range of these alternative priors constitutes sufficient testing criteria (O'Hagan\index{authorindex}{O'Hagan, A.} and Berger 
\index{authorindex}{Berger, J. O.} 1988, Walley, Gurrin, and Burton\index{authorindex}{Walley, P.} \index{authorindex}{Gurrin, L.}
\index{authorindex}{Burton, P.} 1996).  This reduces to a difficult decision as to what should be considered a reasonably wide range of 
included prior specifications without including those that are highly unreasonable for mathematical or substantive reasons.  Sometimes 
global sensitivity analysis is just not possible due to ``an inability to sufficiently refine the usually subjective inputs of model, prior, 
and loss'' (Berger\index{authorindex}{Berger, J. O.} 1986a).  \index{subjectindex}{loss function}

Furthermore, the primary difficulty is that global sensitivity applies to simultaneous neighborhood generalization across all
dimensions of the parameter vector.  There can therefore be an enormous number and variety of specifications to consider and
evaluate with difficult inclusion criteria.  As Poirer notes, Leamer's advice is ``easier to preach than to practice''
(Poirer\index{authorindex}{Poirer, D. J.} 1988, p.130).  However, systematic Monte Carlo methods can considerably ease this task
(Canova\index{authorindex}{Canova, F.} 1994).  One very creative approach to handling this problem is the idea of ``backward''
sensitivity analysis or \emph{prior partitioning} \index{subjectindex}{prior partitioning} as advocated by
Carlin\index{authorindex}{Carlin, B. P.} and Louis\index{authorindex}{Louis, T. A.} (2009, p.188-194).  The idea is to fix the
posterior at some interesting or realistic configuration and see what types of priors are consistent with reaching this posterior
given the data.  

\subsection{Local Sensitivity Analysis}\label{local.sensitivity.section} \index{subjectindex}{sensitivity analysis!local}
Local sensitivity analysis is the more modest and realizable process of making minor changes in the prior parameterization while looking at 
the subsequent posterior effects.  This has the advantage of realizing many of the benefits of global sensitivity analysis such as 
indicating posterior fragility, but at a much lower cost in terms of effort and reporting.  The distinction between global and local 
sensitivity analysis is not as formal as the distinction between global and local robustness evaluation studied below, but the central 
point is that the basic form of the prior is not altered in local sensitivity analysis.
\index{subjectindex}{sensitivity analysis!global versus local}

Generally modifying the parameters of the prior is done to produce a more diffuse form of the prior than that used in the initial analysis.  
The associated argument is that if the posterior does not appreciably change, then this is support for the initially specified form of the 
prior as one that does not interject substantial subjective prior information into the subsequent posterior.  Naturally this is 
data-dependent and therefore application-specific.

\subsubsection{Normal-Normal Model} \index{subjectindex}{sensitivity analysis!local!normal model}
Returning to the normal model example from Chapter~\ref{Normal.Model.Chapter} with variance known and a normal prior on the unknown mean,
we specify a prior with parameters $\mathcal{N}(m,s^2)$.  This produces a normal posterior according to:
\begin{equation}\label{normal.normal.posterior.expression}
    \pi(\mu|\mathbf{X},\sigma^2) \sim \mathcal{N}\left[
        \left(\frac{m}{s^2} + \frac{ n\bar{x} }{ \sigma_0^2 }\right)
	\bigg/\left(\frac{1}{s^2} + \frac{n}{\sigma_0^2}\right),
    \left(\frac{1}{s^2} + \frac{n}{\sigma_0^2}\right)^{-1} \right].
\end{equation}
The sensitivity to the prior mean, $m$, can be analyzed by adding and subtracting one prior standard deviation, $m_1^*=m-s$, $m_2^*=m+s$, 
and recalculating the posterior distribution two more times using $m_1^*$ and $m_2^*$ separately as substitutes for $m$.  It is clear from 
the form of \eqref{normal.normal.posterior.expression} that this provides a location shift of the posterior whose scaled distance is 
averaged into the sample mean scaled by the known variance.  In other words, if this distance is large relative to information provided by 
the sample, the location shift of the posterior will be large.  Conversely, if this distance is small relative to sample
information ($n\bar{x}$), then the mode of the posterior will be relatively stable.

\index{subjectindex}{sensitivity analysis!local!mean manipulation}
From the form of the posterior distribution for $\mu$ in \eqref{normal.normal.posterior.expression}, we can also see that adding or 
subtracting some fixed quantity from the prior mean $m$ does not affect the posterior variance in any way  because the normal distribution 
is a location-scale family distribution, meaning that one parameter is strictly a description of the measure of centrality, and the other 
strictly a measure of dispersion (see Casella\index{authorindex}{Casella, G.} and Berger\index{authorindex}{Berger, R. L.} 2002, Chapter 3; 
Lehmann\index{authorindex}{Lehmann, E. L.} 1986, Chapter 1).  Here conjugacy provides invariancy of this property, meaning that a conjugate 
location-scale prior gives a location-scale form of the posterior.  There are actually situations where Bayesian invariance of location-scale 
family characteristics does not depend on conjugacy (Dawid\index{authorindex}{Dawid, A. P.} 1979).

\index{subjectindex}{sensitivity analysis!local!variance manipulation}
If we want to alter the variance parameter of the prior in order to assess posterior sensitivity, we need only to change the $s^2$ prior 
parameter.  A typical procedure for variance parameters in this setting is to double or halve its value, $s_1^{2*} = s^2/2, s_2^{2*} = 2s^2$  
and observe the effect on the resulting posterior.  It is easy to see from the form of \eqref{normal.normal.posterior.expression} that this 
change alters the dispersion of the posterior, subject to the relative weighting of the variance from the likelihood function; that is, the 
greater $n$ is the less important the prior variance specification becomes (Lindley\index{authorindex}{Lindley, D. V.} 1972).

\subsubsection{Local Sensitivity Analysis Using Hyperparameter Changes}\label{Hyperparameter.Sensitivity}
\index{subjectindex}{sensitivity analysis!local!hyperparameter manipulation}
The normal-normal model described previously is actually quite a simple case in that as a location-scale family distribution, the effect 
of altering one parameter is independent of the other.  In more general specifications the distinction is not always as clear.  For 
example, in the beta-binomial model of Chapter~\ref{Model.Chapter} it is not obvious how one would alter the $A$ and $B$ parameters in 
\eqref{joint.betabinomial.distribution}, except to note from posterior quantities on page~\pageref{posterior.betabinomial.point.estimate} 
that it can make a large difference.  In such cases it is essential to employ substantive theoretical motivations for specifying ranges of 
the parameter space.

Figure~\ref{Expo.Beta.Fig} shows the effect of changes of the prior parameters of a model with an exponential likelihood function and a 
conjugate gamma prior, as described in Section~\ref{exponential.conjugacy.example}.  The posterior distribution for the unknown exponential 
PDF parameter is given by: $\pi(\theta|\mathbf{x}) \propto \theta^{(\alpha+n)-1}\exp\left[-\theta\left(\sum x_i+\beta\right)\right].$
In the first panel, $\alpha$ is set at 10 and $\beta$, the ``scale parameter,''  is varied to show the effect on the resulting posterior 
distribution for $\theta$ for fixed $\sum x_i$ and $n$.  In the second panel, $\beta$ is fixed at 4 and $\alpha$, the ``shape parameter,'' 
is varied.  Since the posterior is flattened and elongated for higher values of $\alpha$ and lower values of $\beta$, then it is clear that 
arbitrary simultaneous changes in both $\alpha$ and $\beta$ can be difficult to interpret.
\index{subjectindex}{sensitivity analysis!local!beta-binomial}

\begin{figure}[h]
	\centerline{  \epsfig{file=Images/robust.figure03.ps, height=5.185in,width=3.5in,clip=,angle=270}  }
	\caption{\textsc{Exponential Model for $\theta$ with Gamma Prior}} \label{Expo.Beta.Fig}
\end{figure}

A better means of analyzing posterior sensitivity to changes in prior parameters that do not have an immediate interpretation (unlike the 
normal model) is to relate these parameters to the variance of the PDF or PMF of the prior.  The variance of the prior distribution of 
$\theta$ in the exponential-gamma model is $\alpha/\beta^2$, suggesting that we can vary this amount by some proportional quantity and view 
the subsequent sensitivity of the posterior.  Starting with the prior parameter specification $\alpha=10$, $\beta=2$ we now vary the 
total \emph{prior} variance by 50\% in both directions by modifying either parameter singly.  This exercise is demonstrated in 
Figure~\ref{Expo.Sensitivity.Fig} where the posterior for $\alpha=10$, $\beta=2$ is located centrally in both panels with the 95\% HPD 
region shaded.   The first panel varies $\alpha$ and the second panel varies $\beta$, in both cases enough to alter the prior variance by 
50\% in either direction.

\begin{figure}[h]
	\centerline{ \epsfig{file=Images/robust.figure04.ps,height=5.185in,width=3.5in,clip=,angle=270}  }
	\caption{\textsc{Exponential Model Sensitivity}} \label{Expo.Sensitivity.Fig}
\end{figure}

What we can see from this exercise is that the exponential-gamma posterior is relatively insensitive to modest modifications of the prior 
specification.  If this were not true, Figure~\ref{Expo.Sensitivity.Fig} would show dramatic changes in the location or dispersion of the 
posterior.  This approach to local sensitivity analysis is generally very easy to perform and often provides convincing diagnostic evidence 
to support some researcher-generated prior specification.

\begin{table}[h]
\parbox[c]{\linewidth}{
    \begin{footnotesize}
    \begin{center}
    \hspace{-77pt}
    \tabletitle{\textsc{Recidivism by Crime, Oklahoma, 1/1/85 to 6/30/99}}\label{Recidivism.Data}
    \begin{tabular}{lrrrrrrrrr}
    Crime Type 				& \multicolumn{3}{c}{Released} 	& \multicolumn{3}{c}{Returned} 
					    & \multicolumn{3}{c}{Percentage}\\
    \hline
    Unauthorized use of motor vehicle   &$\;$& 1522 &$\;$       &$\;$& ~621 &$\;$  	&$\;$& 40.8 &$\;$ \\
    Burglary I 				&& ~821 & 		&& ~314 & 	    && 38.3 & \\
    Burglary II 				&& 7397 & 		&& 2890 & 	    && 39.1 & \\
    Larceny 				&& 9557 & 		&& 3525 & 	    && 36.9 & \\
    DUI-2nd offense 			&& 8891 & 		&& 3212 & 	    && 36.1 & \\
    Murder I 				&& ~182 & 		&& ~~65 & 	    && 35.7 & \\
    Forgery 				&& 2529 & 		&& ~882 & 	    && 34.9 & \\
    Escape 					&& 2747 & 		&& ~951 & 	    && 34.6 & \\
    Robbery 				&& 3001 & 		&& 1035 & 	    && 34.5 & \\
    Bogus check 				&& 1341 & 		&& ~414 & 	    && 30.9 & \\
    Weapons 				&& 1950 & 		&& ~558 & 	    && 28.6 & \\
    Bribery 				&& ~~~7 & 		&& ~~~2 & 	    && 28.6 & \\
    Possession/Obtaining drugs 		&& 7473 & 		&& 2125 & 	    && 28.4 & \\
    Assault 				&& 3133 & 		&& ~848 & 	    && 27.1 & \\
    Other nonviolent crimes 		&& 1637 & 		&& ~429 & 	    && 26.2 & \\
    Fraud 					&& 1239 & 		&& ~307 & 	    && 24.8 & \\
    Arson 					&& ~439 & 		&& ~106 & 	    && 24.2 & \\
    Distribution of drugs 			&& 8775 & 		&& 2066 & 	    && 23.5 & \\
    Embezzlement 				&& ~804 & 		&& ~188 & 	    && 23.4 & \\
    Other violent crimes 			&& 1081 & 		&& ~234 & 	    && 21.7 & \\
    Rape 					&& 1269 & 		&& ~262 & 	    && 20.7 & \\
    Gambling 				&& ~~20 & 		&& ~~~4 & 	    && 20.0 & \\
    Manslaughter 				&& ~846 & 		&& ~167 & 	    && 19.7 & \\
    Kidnapping 				&& ~162 & 		&& ~~31 & 	    && 19.1 & \\
    Murder II 				&& ~271 & 		&& ~~51 & 	    && 18.8 & \\
    Sex offenses 				&& 2143 & 		&& ~362 & 	    && 16.9 & \\
    Drug trafficking 			&& ~426 & 		&& ~~33 & 	    && ~7.8 & \\
    \end{tabular} \end{center} \end{footnotesize} 
}
\end{table} \bls

\begin{table}[h]
\parbox[c]{\linewidth}{
    \begin{center}
    \hspace{-66pt}
    \tabletitle{\textsc{$\lambda$ Posterior Summary, Recidivism Models}}\label{Recidivism.Results.Table}
    \renewcommand{\arraystretch}{1.2}
    \begin{tabular}{l|cccc}
    \multicolumn{1}{c}{Prior}       & 2.5\% Quantile 	& Mean		& Median 	& 97.5\% Quantile \\
    \hline
    $\qquad \mathcal{G}(2,2)$	& 3.7765 		& 4.4743 	& 4.4630 	& 5.2004 \\
    $\qquad \mathcal{G}(1,4)$	& 4.0534 		& 4.7677 	& 4.7644 	& 5.4945 \\
    $\qquad \mathcal{G}(7,1)$	& 3.8328 		& 4.4557 	& 4.4369 	& 5.1765 \\
    $\qquad \mathcal{U}(0,100)$	& 3.5177 		& 4.1255 	& 4.1019 	& 4.8808 \\
    \end{tabular} \end{center} 
}
\end{table} \bls

\subsection{Global and Local Sensitivity Analysis with Recidivism Data}\index{subjectindex}{example!sensitivity analysis of recidivism model}
In criminology and the public policy study of the penal system a key concern is the rate of recidivism: convicted criminals committing 
additional crimes after release from prison (see the running discussion in Chapter~\ref{Intro.Chapter}).  This example looks at state-level 
recidivism data from Oklahoma over the period from January 1, 1985 through June 30, 1999.  Table~\ref{Recidivism.Data} shows these
data (by descending percentage) as collected by the Oklahoma Department of Corrections (\texttt{http://www.doc.state.ok.us}), and
provided in the \R\ package \texttt{BaM} as \texttt{recidivism}.

It is clear from the table that recidivism rates differ according to the crime of original conviction, but it is not unreasonable
to assert that there also exist some underlying social factors that affect all rates.  Although there are several obvious ways to
look at these data, we develop a simple Poisson model of the \emph{log} of the recidivism rate (log of the Number Returned
variable).  Actually four models are produced, three with differently parameterized gamma priors producing the posterior:
$\pi(\lambda|\X) \propto \lambda^{(\alpha+\sum x_i)-1}\exp[-\lambda(\beta+n)]$, and one with a bounded uniform prior producing the
posterior: $\pi(\lambda|\X) \propto \lambda^{\sum x_i}\exp[-\lambda n]$.   The posterior results are summarized in
Table~\ref{Recidivism.Results.Table} with quantiles $(2.5, 50, 97.5)$ and the mean for the four prior forms.

\begin{figure}[h]
    \parbox[c]{\linewidth}{
        \begin{center}
	    \epsfig{file=Images/robust.figure05.ps, height=4.60in,width=4.05in,clip=,angle=270}  
	    \caption{\textsc{Comparison of Posteriors, Recidivism Model}} \label{Recidivism.Figure}
        \end{center}
    }
\end{figure}

Table~\ref{Recidivism.Results.Table} shows that there is not much of a difference across
the prior specifications, including the uninformative uniform prior.  This is evidence that the model is
reasonably insensitive to differing conjugate prior parameterizations and to different prior forms.  
Also, the uniform prior can be considered a reference prior in the sense discussed in 
Chapter~\ref{Prior.Chapter} and therefore it appears that the conjugate gamma prior is not overly
influential in determining the posterior relative to the uniform reference.  Further evidence is seen
in Figure~\ref{Recidivism.Figure}, which shows 1,000 empirical replications from each of the posterior 
distributions in Table~\ref{Recidivism.Results.Table} using a histogram and a smoothed density estimate.
\index{subjectindex}{smoothing}
\index{subjectindex}{sensitivity analysis|)}

\section{Robustness Evaluation} \index{subjectindex}{robustness!evaluation|(}
\index{subjectindex}{resistance} \index{subjectindex}{influence}\index{subjectindex}{outliers}
This section looks at two definitions of robustness: insensitivity to 
misspecification of the prior and insensitivity to influential data-points 
through the likelihood function.  There is some confusion about the language 
of robustness, resistance, and sensitivity analysis.  Classical robustness 
focuses on the linear model's sensitivity to influential outliers and seeks to 
mitigate this effect.  There is now a vast literature on robust methods for 
identifying and handling influential outliers in the linear modeling context 
(Andrews 1974; Andrews \etal 1972; Barnett and Lewis 1978; Belsley, Kuh, and Welsch
1980; Cook and Weisberg 1982; Emerson and Hoaglin 1983; Hamilton 1992; Hampel 
1974; Hampel \etal 1986; Huber 1972, 1973, 1981; Rousseeuw and Leroy 1987), 
	\index{authorindex}{Andrews, D. F.}%
	\index{authorindex}{Barnett, V.}%
	\index{authorindex}{Lewis, T.}%
	\index{authorindex}{Belsley, D. A.}%
	\index{authorindex}{Kuh, E.}%
	\index{authorindex}{Welsch, R. E.}%
	\index{authorindex}{Cook, D. R.}%
	\index{authorindex}{Weisberg, S.} 
	\index{authorindex}{Emerson, J. D.}%
	\index{authorindex}{Hoaglin, D. C.}%
	\index{authorindex}{Hamilton, L. C.}%
	\index{authorindex}{Hampel, F. R.}%
	\index{authorindex}{Huber, P. J.}%
	\index{authorindex}{Rousseeuw, P. J.}%
	\index{authorindex}{Leroy, A. M.}%
including some from an explicitly Bayesian context (Bradlow, Weiss, and Cho 1998; 
Chaloner and Brant 1988; Guttman 1973; Guttman, Dutter, and Freeman 1978; Pettit and Smith 
	\index{authorindex}{Bradlow, E. T.}%
	\index{authorindex}{Weiss, R. E.}%
	\index{authorindex}{Cho, M.}%
	\index{authorindex}{Chaloner, K.}%
	\index{authorindex}{Brant, R.}%
	\index{authorindex}{Guttman, I.}%
	\index{authorindex}{Dutter, R.}%
	\index{authorindex}{Freeman, P. R.}%
	\index{authorindex}{Pettit, L. I.}%
	\index{authorindex}{Smith, A. F. M.}%
1985).  The more appropriate term for this work is actually ``resistance'' reflecting the goal of making inferences more resistant
to particular data cases, and robustness is correctly defined as low sensitivity to violations of underlying model assumptions.

Bayesian robustness is somewhat broader than the classical approach.\index{subjectindex}{Bayesian robustness}
The primary objective of Bayesian robustness is to determine the \emph{posterior} sensitivity to substantively reasonable changes
in model assumptions and thereby understand the context of uncertainty\index{subjectindex}{uncertainty} introduced by the prior,
the likelihood function, and if appropriate, the loss function.  \index{subjectindex}{loss function} The modern formal approach to
Bayesian robustness, due primarily to Berger\index{authorindex}{Berger, J. O.} (1984), develops a mechanical framework for
altering assumptions and taking these assumptions through the Bayesian inferential machinery to determine their posterior
implications.  Specifically, a nonrobust model specification is one in which ``reasonable'' changes in the prior, likelihood
function, or loss function, produce large changes in the \emph{range} of the posterior quantity of interest.
\index{subjectindex}{robustness!Bayesian model specification}

An associated goal is the identification of data values that strongly influence posterior
inferences,\index{subjectindex}{influence} and the complexity of this endeavor is greatly magnified as the dimension of the
problem is increased.  Generally computationally intensive tools are required for this analysis (Bradlow, Weiss, and Cho
\index{authorindex}{Bradlow, E. T.} \index{authorindex}{Weiss, R. E.} \index{authorindex}{Cho, M.} 1998), and it is common that
time-saving summary measures are substituted for complete data expositions such as the AIC\index{subjectindex}{Akaike information
criterion (AIC)} implications of subsetting (Kitagawa and Akaike 1982).  \index{authorindex}{Kitagawa, G.}
\index{authorindex}{Akaike, H.} A radically different, and substantially more complex, approach performs \emph{a priori} analysis
to determine robustness properties from the initial assumptions rather than as \emph{post hoc} model adjustments.  One such method
by Seidenfeld, Schervish, and Kadane (1995) develops a complete Bayesian decision-theoretic axiomatic construct to derive
preferences from utility and probability statements.  Therefore the robustness properties are a direct result of the initial
setup.  \index{authorindex}{Seidenfeld, T.} \index{authorindex}{Schervish, M. J.} \index{authorindex}{Kadane, J. B.} 

\subsection{Global Robustness}\index{subjectindex}{global robustness|(}
The most common method for determining global robustness is the \emph{range} of the posterior quantity of interest (Wasserman
\index{authorindex}{Wasserman, L.} 1992).  For instance, we can see how much the bounds of the 95\% HPD region change under
different prior assumptions.  Other statistics of obvious interest include the posterior mean, various quantiles, and measures of
dispersion.  If modest changes to a specified prior distribution, say varying its parameters somewhat, produce dramatic changes in
the range of these posterior quantities, then it is an indication that we should be cautious about the use of this particular
prior, or at least that we should report this instability to our readers.\index{subjectindex}{global robustness!HPD changes}

\index{subjectindex}{global robustness!classes of priors} \index{subjectindex}{uncertainty!prior} 
\index{subjectindex}{prior distribution!$\epsilon$-contamination neighborhood}  
Berger\index{authorindex}{Berger, J. O.} (1984) introduced the idea of global robustness as an explicit recognition that no single prior 
distribution can be shown to be absolutely correct and a ``class'' of priors therefore provides more information about model uncertainty.  
The most frequent approach uses the so-called $\epsilon$-contamination neighborhood priors.  Starting with a specified prior, $p_0(\theta)$, 
identify a wider class of ``contamination'' forms that includes this prior but broadens the range of specific forms considerably: $Q$ 
containing alternatives $q(\theta)$.  It is also necessary to give a subjective probability indicating how \emph{uncertain} we are 
about the selection of $p_0(\theta)$; label this probability $\epsilon$.  The class priors evaluation is the subset of $Q$ given by all 
forms of $p(\theta)$ that satisfy:
\begin{equation}
	\Gamma(Q,\epsilon) = \{p(\theta)\range\; p(\theta) = (1-\epsilon)p_0(\theta) 
						+ (\epsilon)q(\theta), q(\theta) \in Q\}.	
\end{equation}
Thus $\Gamma(Q,\epsilon)$ is an $\epsilon$-weighted compromise between the initial prior and the selected overarching class of 
alternatives.  The primary question, of course, is how do we define $Q$?  Choices include all possible contaminants (Berger 
\index{authorindex}{Berger, J. O.} and Berliner\index{authorindex}{Berliner, L. M.} 1986), all symmetric and unimodal forms (Basu
\index{authorindex}{Basu, S.} 1994, Sivaganesan and Berger 1989), unimodality-preserving forms (Moreno\index{authorindex}{Moreno, E.} and 
Gonz\'{a}lez\index{authorindex}{Gonz\'{a}lez, A.} 1990), normals (Berger\index{authorindex}{Berger, J. O.} and Berliner
\index{authorindex}{Berliner, L. M.} 1986; see however, the warning in Lavine 1991a), more than one $Q$ (Bose 1994a), mixtures (Bose 1994b), 
hierarchical classes (Moreno and Pericchi 1993; Sivaganesan 2000), and quantile restraints on $Q$ (Moreno and Cano 1991; Moreno and
Pericchi 1991, Ruggeri 1990).
	\index{authorindex}{Lavine, M.}
	\index{authorindex}{Bose, S.}
	\index{authorindex}{Moreno, E.}
	\index{authorindex}{Pericchi, L. R.}
	\index{authorindex}{Sivaganesan, S.}
	\index{authorindex}{Cano, J. A.}
	\index{authorindex}{Ruggeri, F.}

Notice that $\epsilon$ is constant across the parameter space for $\theta$, and it may be the case that particular regions such as the 
tails have higher or lower researcher confidence in $p_0(\theta)$.  To accommodate this goal, a function, $\epsilon(\theta)$ can be defined 
so that prior uncertainty\index{subjectindex}{uncertainty!prior} differs over $\Theta$ (Berger 1994; Liseo, Petrella and Salinetti 1996). 
	\index{authorindex}{Berger, J. O.}
	\index{authorindex}{Liseo, B.}
	\index{authorindex}{Petrella, L.}
	\index{authorindex}{Salinetti, G.}

To show the effects of different contamination classes, Moreno\index{authorindex}{Moreno, E.} (2000) gives the following
hypothetical analysis.  Suppose the data consist of a single data point distributed $\mathcal{N}(\theta,1)$, the prior is
specified as $\mathcal{N}(0,2)$, and we set $\epsilon=0.2$ (an atheoretic but common value in such analyses).  Consider three
contaminants of the form: all distributions ($q(\theta) \in Q_{\text{All}}$), all distributions with the same median as the prior
($q(\theta) \in Q_{\text{Median}}$), and all distributions with the same quartiles as the prior ($q(\theta) \in
Q_{\text{Quartiles}}$).  The posterior quantity of interest is the 95\% HPD region for $\theta$, and Table~\ref{Recidivism.Table}
shows the most extreme upper and lower bounds reached over each contaminating class for five different observed data values
(calculations are found in Moreno\index{authorindex}{Moreno, E.} [2000, p.50]).

\begin{table}[h]
\parbox[c]{\linewidth}{
    \begin{center}
    \hspace{-55pt}
    \tabletitle{\textsc{Oklahoma Recidivism 95\% HPD Regions, 1/1/85 to 6/30/99}}\label{Recidivism.Table}
    \begin{tabular}{rcl@{:}rcl@{:}rcl@{:}r}
    \multicolumn{1}{c}{$x$} && \multicolumn{2}{c}{$Q_{All}$} 
		            && \multicolumn{2}{c}{$Q_{Median}$} 
			    && \multicolumn{2}{c}{$Q_{Quartiles}$} \\[3pt]
    \hline
        & $\qquad$ & \multicolumn{2}{c}{} & $\qquad$ & \multicolumn{2}{c}{} & $\qquad$ & \multicolumn{2}{c}{} \\[-8pt]
    0.5  & $\qquad$ & [0.82 & 0.96] & $\qquad$ & [0.84 & 0.96] & $\qquad$ & [0.91 & 0.96] \\
    1.0  & $\qquad$ & [0.77 & 0.97] & $\qquad$ & [0.82 & 0.96] & $\qquad$ & [0.87 & 0.96] \\
    2.5  & $\qquad$ & [0.71 & 0.97] & $\qquad$ & [0.79 & 0.96] & $\qquad$ & [0.86 & 0.96] \\
    3.0  & $\qquad$ & [0.36 & 0.98] & $\qquad$ & [0.52 & 0.97] & $\qquad$ & [0.66 & 0.97] \\
    4.0  & $\qquad$ & [0.13 & 0.99] & $\qquad$ & [0.20 & 0.99] & $\qquad$ & [0.50 & 0.97] \\
    \end{tabular}
    \end{center} 
}
\end{table}

This example was selected because it illustrates several aspects of robust Bayesian analysis 
with $\epsilon$-contaminations.  \emph{First}, notice that the more we restrict the contamination
class, the smaller the posterior range: the ranges decrease going right across the table.
This means that there is a trade-off in this analysis between generalizeability of the 
robustness and practical utility: a very broad $Q$ may lead to a large range and a very 
narrowly defined $Q$ may lead to a small range, but neither one of these results tells as
much as a broad class leading to a narrow range or a narrow class leading to a broad range.
\emph{Second}, since the data value is always greater than the prior mean here, the infimum of the 
posterior HPD regions is the more variable component, and this is true \emph{even as the
data value gets larger} reflecting variance scale uncertainty as well as location uncertainty.
\emph{Finally}, it is a clear (and welcome) result that the data matter in all of this: the range 
increases as the data point moves away from the prior mean.
	\index{subjectindex}{uncertainty!scale}
	\index{subjectindex}{uncertainty!location}

Several problems often occur in performing this analysis in real data-analytic situations:
the specification of the contamination class may be difficult in cases where the 
obvious choices are not reasonable, the calculations of posterior bounds can be foreboding in 
high-dimension problems, and the results may unfortunately be inconclusive.

This is an active research area and there are well-developed alternatives to $\epsilon$-contamination 
forms, including those based on banding with standard CDF forms (Basu and DasGupta 1995), 
general quantile-restrained classes (Berger and O'Hagan 1988; O'Hagan and Berger 1988), 
quantile-restrained classes with robust prior properties (Moreno, Martinez, and Cano 1996), 
	\index{authorindex}{Berger, J. O.}%
	\index{authorindex}{O'Hagan, A.}%
	\index{authorindex}{Moreno, E.}%
	\index{authorindex}{Mart\'{\i}nez, C.}%
	\index{authorindex}{Cano, J. A.}%
	\index{authorindex}{Lavine, M.}%
	\index{authorindex}{Maritz, J. S.}%
	\index{authorindex}{Morris, C. N.}%
	\index{authorindex}{Dey, D. K.}%
	\index{authorindex}{Birmiwal, L. R.}%
	\index{authorindex}{Regazzini, E.}%
	\index{authorindex}{Fortini, S.}%
density-ratio classes (Lavine 1991b), Empirical Bayes (Maritz 1970; Morris 1983b), asymptotic 
Bayes risk (Berger 1984), entropy (Dey and Birmiwal 1994), and concentration functions 
(Regazzini 1992, Fortini and Ruggeri\index{authorindex}{Ruggeri, F.} 2000).
\index{subjectindex}{global robustness|)}

\subsection{Local Robustness}\index{subjectindex}{local robustness}
Because the scope of performing global sensitivity in a \emph{completely} rigorous and thorough manner is beyond the resources of many 
research projects, there is a motivation for a more globally limited, but still informative, paradigm for assessing robustness in Bayesian 
models (Basu, Jammalamadaka, and Liu 1996; Cuevas and Sanz 1988; Delampady and Dey 1994; Gelfand and Dey 1991, Sivaganesan 1993).  
	\index{authorindex}{Basu, S.}%
        \index{authorindex}{Jammalamadaka, S. R.}%
        \index{authorindex}{Liu, W.}%
	\index{authorindex}{Cuevas, A.}%
	\index{authorindex}{Sanz, P.}%
	\index{authorindex}{Delampady, M.}%
	\index{authorindex}{Dey, D. K.}%
	\index{authorindex}{Gelfand, A. E.}%
	\index{authorindex}{Sivaganesan, S.}%
	\index{subjectindex}{prior distribution!informative}%
Local robustness evaluates the rate of change in posterior inferences that occur due to very small (infinitesimal) perturbations in the 
prior (Ruggeri\index{authorindex}{Ruggeri, F.} and Wasserman\index{authorindex}{Wasserman, L.} 1993; Gustafson 1996).  Because this 
process uses derivative quantities rather than integrals, it is often much easier to calculate than alternatives. 
\index{subjectindex}{infinitesimal perturbations}\index{authorindex}{Gustafson, P.}

Define the locally perturbed prior to be a weighting of the original prior and the perturbance:
\index{subjectindex}{prior distribution!locally disturbed} \index{subjectindex}{robustness!prior specification}
\begin{equation}\label{perturbed.prior}
	p^*(\theta) = (1-\epsilon)p(\theta) + (\epsilon)q(\theta),
\end{equation}
where $q(\theta)$ is a single identified disturbance prior here (rather than one of a class as in global robustness), and $\epsilon$ is 
not an expression of doubt about the original prior but a weighting of how much to perturb with $q(\theta)$.  Given the likelihood 
function, $L(\theta|\x)$, the subsequent $\epsilon$-weighted (mixture) posterior is:
\begin{equation}\label{perturbed.posterior}
	\pi_{\epsilon}(\theta|\x) = \frac{ (1-\epsilon)p(\theta)L(\theta|\x) 
					+ (\epsilon)q(\theta)L(\theta|\x) }
			      { (1-\epsilon)m_p(\x) + (\epsilon)m_q(\x) }
\end{equation}
(O'Hagan\index{authorindex}{O'Hagan, A.} 1994).  This is the standard nonproportional derivation of the Bayesian posterior, but weighted 
by the marginal posterior densities of the data using the original prior ($m_p(\x)$) and the disturbance prior ($m_q(\x)$):
\begin{equation}
	m_p(\x) = \int_\Theta p(\theta)L(\theta|\x)d\theta, \qquad\qquad
	m_q(\x) = \int_\Theta q(\theta)L(\theta|\x)d\theta.
\end{equation}
Further define, according to Gustafson\index{authorindex}{Gustafson, P.} and Wasserman\index{authorindex}{Wasserman, L.} (1995), the 
difference between the $\epsilon$-weighted posterior and the nondisturbed posterior as $\epsilon$ goes to zero:
\index{subjectindex}{posterior distribution!$\epsilon$-weighted}
\begin{align}\label{posterior.sensitivity}
	D(q) &= \underset{\epsilon \to 0}{\text{lim}}
		(\pi_{\epsilon}(\theta|\x)-p(\theta)L(\theta|\x)/m_p(\x))
		\nonumber\9
	     &= \left( \frac{q(\theta)L(\theta|\x)}{m_q(\x)} 
		      -\frac{p(\theta)L(\theta|\x)}{m_p(\x)} \right)
		\frac{ m_q(\x) }{ m_p(\x) },
\end{align}
which gives $||D(q)||$ as the local (infinitesimal) posterior sensitivity from disturbing $p(\theta)$ by $q(\theta)$, and under very general 
conditions can be thought of as the derivative of the posterior with prior $p(\theta)$ in the direction of $q(\theta)$ (see Diaconis
\index{authorindex}{Diaconis, P.} and Freedman\index{authorindex}{Freedman, D. A.} 1986, especially p.13).
\index{subjectindex}{local (infinitesimal) posterior sensitivity}
\index{subjectindex}{prior distribution!disturbance}

There are many possible choices for the disturbance prior, and Gustafson\index{authorindex}{Gustafson, P.} (2000, 75) gives a tabularized 
list for the best-known examples.  One advantage to this approach is that it can readily be programmed in a language like \R\ to cycle over 
many different alternative disturbance specifications, as well as to find worst-case behavior or average behavior across ranges of priors. 
\index{subjectindex}{local robustness|)}

\subsubsection{Bayesian Linear Outlier Detection} \index{subjectindex}{Bayesian linear model!outliers}
Outlier detection and robustizing is very straightforward in the Bayesian linear model.  The basic idea, from Chaloner
\index{authorindex}{Chaloner, K.} and Brant\index{authorindex}{Brant, R.} (1988), Zellner and \index{authorindex}{Zellner, A.} Moulton 
\index{authorindex}{Moulton, B. R.} (1985), and Zellner (1975), is to look at the modeled distribution of the error term after producing 
the posterior.  Returning to the linear specification in Bayesian terms given in Section~\ref{linear.regression.model.section} with the 
improper prior $p(\theta,\sigma^2) = \frac{1}{\sigma}$, define the vector of residuals resulting from the insertion of maximum likelihood 
into (\ref{joint.posterior.uninformed.linear}):
\begin{equation}
    \EP = \y - \X\hat{\mathbf{\theta}}.    
\end{equation}
The ``hat'' matrix (Amemiya\index{authorindex}{Amemiya, T.} 1985, Chapter~1) is: \index{subjectindex}{hat matrix}
\begin{equation}
    \mathbf{H} = \X(\X'\X)^{-1}\X',       
\end{equation}
where we are principally interested in the diagonal values, $h_{ii}$, of the $\mathbf{H}$ 
	matrix.\footnote{The $h_{ii}$ values are termed ``leverage points'' by Cook\index{authorindex}{Cook, D. R.} and Weisberg
	\index{authorindex}{Weisberg, S.} (1982, p.15), and are particularly useful in determining outlying values in a high-dimensional 
	model where visualization is difficult.  These values have several important characteristics: \emph{(1)} the greater $h_{ii}$ 
	the more that the $i^{th}$ case determines the relationship between $\y$ and $\hat{\y}$, \emph{(2)} the greater  $h_{ii}$ the
    	smaller the variance of the $i^{th}$ residual, \emph{(3)} defining the model standard error by $s^{2} = \EP'\EP/(n-k)$ and 
	therefore the $i^{th}$ contribution as $(s_{i})^{2} = (1 - h_{ii})s^{2}$, then the $i^{th}$ jackknifed residual (also called 
	an externally studentized residual) is calculated from $s_{(i)}$, which is an estimate of $\sigma$ when the regression is run 
	omitting the $i^{th}$ case: $t_{(i)} = \EP_i/(s_{(i)}\sqrt{1 - h_{ii}})$.  This statistic can also be thought of as the 
	residual weighted inversely proportional to the jackknifed standard error.}
\index{subjectindex}{jackknifed standard error} \index{subjectindex}{jackknifed residual (externally studentized residual)}
Outliers are defined to be those having residuals with high posterior probabilities of exceeding some arbitrary cutoff on a normalized 
scale, $k$.  For a candidate point $i$, identify cutoff points, which identify the distance from the $k$-defined tail and the posterior 
residual value in both directions:
\begin{equation}
    z_{1i} = \frac{ k - \sqrt{\sigma}\EP_i }{ \sqrt{h_{ii}} }  \qquad\qquad
    z_{2i} = \frac{ k + \sqrt{\sigma}\EP_i }{ \sqrt{h_{ii}} }, 
\end{equation}
and the associated standard normal CDF values:
\begin{equation}
    \Phi(z_{1i}) = \Phi\left(\frac{ k - \sqrt{\sigma}\EP_i }{ \sqrt{h_{ii}} }\right)  \qquad\qquad
    \Phi(z_{2i}) = \Phi\left(\frac{ k + \sqrt{\sigma}\EP_i }{ \sqrt{h_{ii}} }\right). 
\end{equation}
The probability of interest is thus the weighted posterior area of the support of the variance term bounded by these cutoff points:
\begin{equation}
    p(|\EP_i|>k\sigma|\X,\y) = \int(1-\Phi(z_{1i})-\Phi(z_{2i}))  \pi(\sigma|\X,\y) d\sigma,
\end{equation}
where $\pi(\sigma|\X,\y)\propto\sigma^{-(n-k)-1}\exp\left[-\frac{1}{2\sigma^2}\hat{\sigma}^2(n-k)\right]$.
This quantity can be compared for each data point to $2\Phi(-k)$ where larger values are inferred to be high probability outliers 
(Chaloner\index{authorindex}{Chaloner, K.} and Brant\index{authorindex}{Brant, R.} 1988, p.652).  The probability that a given point is an
outlier is therefore determined by either a large absolute model residual, $|\EP_i|$, or a small hat value.

Polasek\index{authorindex}{Polasek, W.} (1984) provides a diagnostic for the general least squares estimator with weights 
given for the variance on the prior $p(\EP) \sim \mathcal{N}(\mathbf{0},\SI)$, and a normal
prior independent of $\SI$: $p(\B) \sim \mathcal{N}(\mathbb{B},\mathbf{S})$.  Weighting
in linear model analysis is very useful not only to compensate for heteroscedasticity,\index{subjectindex}{heteroscedasticity} 
but also as a diagnostic tool (Birkes\index{authorindex}{Birkes, D.} and 
Dodge\index{authorindex}{Dodge, Y.} 1993; 
Carroll\index{authorindex}{Carroll, R. J.} and 
Ruppert\index{authorindex}{Ruppert, D.} 1988; the essays in Hoaglin, Mosteller, and Tukey 1983).  
	\index{authorindex}{Hoaglin, D. C.}%
	\index{authorindex}{Mosteller, F.}%
	\index{authorindex}{Tukey, J. W.}%
The resulting setup from Polasek gives the conditional posterior for $\B$: \index{subjectindex}{Polasek linear diagnostic}
\begin{align}
	\pi(\B&|\SI,\X,\y) 	\nonumber\\
	      &= \mathcal{N}\bigl( (\mathbf{S}^{-1} + \X'\SI^{-1}\X)^{-1}
			           (\mathbf{S}^{-1}\mathbb{B} + \X'\SI^{-1}\X),
					(\mathbf{S}^{-1} + \X'\SI^{-1}\X)^{-1} \bigr)
		 \label{polasek.linear.model}
\end{align}
which is not only very easy to work with, but also gives a simple way to test local 
sensitivity by changing the weights in the $\SI$ matrix.  Polasek gives the local 
sensitivity of the posterior mean and precision by taking the derivative of these 
with respect to the prior precision on $\EP$:   
\begin{align}
	\frac{\partial}{\partial \SI^{-1}}&\bigl[ \mathbf{S}^{-1} + \X'\SI^{-1}\X \bigr]
		= -\text{vec}(\X)\text{vec}(\X)'(\mathbf{S}^{-1} + \X'\SI^{-1}\X)
		\nonumber\\
	\frac{\partial}{\partial \SI^{-1}}&\bigl[ \mathbf{S}^{-1}+\X'\SI^{-1}\X)^{-1}(\mathbf{S}^{-1}\mathbb{B}+\X'\SI^{-1}\X \bigr]
		\nonumber\\
                &= \text{vec}(\X)(\mathbf{S}^{-1} + \X'\SI^{-1}\X)
		\nonumber\\
		&\qquad \times \text{vec}(\y- \X(\mathbf{S}^{-1}+\X'\SI^{-1}\X)^{-1}(\mathbf{S}^{-1}\mathbb{B}+\X'\SI^{-1}\X))',
\end{align}
which is long but not complicated.  The advantage of this setup is that once programmed, it is easy to run hypothetical priors
through the matrix algebra and see posterior effects immediately.  The unweighted local sensitivity of the $i^{th}$ data point is
found by first replacing the $\SI^{-1}$ matrix by an identity matrix substituting the $i^{th}$ one on the diagonal with the
corresponding weight from the general linear model, $w_i$, to produce $\W_i$.  The resulting unweighted estimate in the $i^{th}$
case is $b(w_i) = (\X'\W_i\X)^{-1}\X'\W_iy_i$ with the local sensitivity given by $\partial b(w_i)/\partial w_i =
(\X'\X)^{-1}\X'e_i$ where $e_i$ is the standard OLS residual for the data point $i$ (Polasek\index{authorindex}{Polasek, W.}
1984).  This is a very simple setup and these priors can easily be developed in \R\ (see the exercises).  

Outlier and influence analysis in Bayesian models is certainly not restricted to the linear case.  Kass, Tierney, and Kadane (1989) 
\index{authorindex}{Kass, R. E.} \index{authorindex}{Tierney, L.}\index{authorindex}{Kadane, J. B.} give outlier diagnostics for 
Bayesian models in general based on asymptotic principles by jackknifing out cases and looking at the subsequent change in the 
posterior expectation of some statistic of interest.  Weiss\index{authorindex}{Weiss, R. E.} (1996) takes a similar approach  but 
uses the Bayes Factor (Chapter~\ref{Testing.Chapter}) to understand the resulting influence.  Bradlow and Zaslavsky (1997) introduce 
a way of speeding up this process by importance weighting (Chapter~\ref{MC.Chapter}) approximations of full jackknifing when the 
data size is large.

\subsection{Bayesian Specification Robustness} \index{subjectindex}{robustness!model specification}
An important question is whether it is possible to construct a prior, or class of priors,
that provide desired robustness qualities before observing the data and conducting the inferential 
analysis.  Lavine\index{authorindex}{Lavine, M.} (1991a, 1991b) suggests a method for producing prior distributions 
that have similar support to some initially proposed prior, but possessing enhanced robustness 
characteristics.  Suppose that the multidimensional prior vector of coefficients can be segmented according to
$\mathbf{\theta} = [\mathbf{\theta_1},\mathbf{\theta_2}]$ on $\mathbf{\Theta}$ (the multidimensional 
sample space), and we specify a joint prior consisting of the product of a marginal and conditional prior:
$p(\mathbf{\theta}) = p^c(\mathbf{\theta_2}|\mathbf{\theta_1})p^m(\mathbf{\theta_1})$.  Lavine\index{authorindex}{Lavine, M.} (1991b)
finds four general classes of priors for the conditional prior, $p^c$, and the marginal or unconditional
prior, $p^m$.  These classes are designated $\Gamma^c$ and $\Gamma^m$, and possess robust qualities
for the forms: \emph{(1)} quantile partitioning\index{subjectindex}{partition} of prior space, \emph{(2)} the $\epsilon$-contaminated 
class (Berger\index{authorindex}{Berliner, L. M.} and 
Berliner\index{authorindex}{Berliner, L. M.} 1986), \emph{(3)} a density ratio 
form of upper and lower limits, and \emph{(4)} a density-bounded class.
The primary advantage of this approach is that the problem of finding tightly bounded posterior
inferences for a large class of priors reduces to a manageable sequence of linear optimizations
(Lavine\index{authorindex}{Lavine, M.} 1991b, p.401).
\index{subjectindex}{robustness!evaluation|)} 

\section{Comparing Data to the Posterior Predictive Distribution}\label{posterior.predictive.section}
\index{subjectindex}{posterior predictive distribution}%
Rubin\index{authorindex}{Rubin, D. B.} (1984) recommends posterior predictive distributions because regardless of the complexity of
the model specification, ``model monitoring'' is easily accomplished with simple summary statistics.  This idea integrates the two 
previous approaches of sensitivity analysis and robust evaluation because it simultaneously tests for the two by looking at posterior 
reasonableness through simulated data.  The basic idea is that if the data generating mechanism implied by the calculated posterior 
is wildly divergent from the original data or from data that we would expect given substantive knowledge, then the Bayesian analysis 
is suspect.  Furthermore, this idea can also be extended to predictive model selection (Laud and Ibrahim 1995).
\index{authorindex}{Laud, P. W.} \index{authorindex}{Ibrahim, J. G.}

First consider a \emph{prior predictive distribution} of a new data value, $x_{\new}$ before observing
the full dataset:\index{subjectindex}{prior predictive distribution}
\begin{equation}
    p(x_{\new}) = \int_{\Theta}p(x_{\new},\theta)d\theta
           = \int_{\Theta}p(x_{\new}|\theta)p(\theta)d\theta. 
\end{equation}
In other words the marginal distribution of an unobserved data value is the product of the
prior for $\theta$ and the single variable PDF or PMF, integrating out this parameter.  This
makes intuitive sense as uncertainty in $\theta$ is averaged out to reveal a distribution for
the data point.\index{subjectindex}{uncertainty!integrating over}

More usefully, from a diagnostic perspective, is the distribution of a new data point, $x_{\new}$
\emph{after} the full iid data set, $\mathbf{x}$,  has been observed: the posterior predictive
distribution.  This is produced by: \index{subjectindex}{posterior predictive distribution}
\index{subjectindex}{iid}
\begin{align}
    p(x_{\new}|\mathbf{x}) &= \int_{\Theta}p(x_{\new},\theta|\mathbf{x})d\theta   
             = \int_{\Theta}\frac{p(x_{\new},\theta|\mathbf{x})}{p(\theta|\mathbf{x})}
                 p(\theta|\mathbf{x})d\theta    \nonumber\\
                          &= \int_{\Theta}p(x_{\new}|\theta,\mathbf{x})p(\theta|\mathbf{x})d\theta.  
             =  \int_{\Theta}p(x_{\new}|\theta)p(\theta|\mathbf{x})d\theta.
                                 \label{posterior.predictive.distribution}
\end{align}
The last simplification comes from the assumption that $x_{\new}$ and $\mathbf{x}$ are independent.
The integral means that the posterior predictive distribution is the product of the single variable
PDF or PMF times the full data likelihood in which we integrate over 
uncertainty\index{subjectindex}{uncertainty!integrating over} in $\theta$
to result in a probability statement that is dependent on the observed data only.
\index{subjectindex}{posterior predictive distribution}

For example, suppose we consider the now familiar example from Chapter~\ref{Normal.Model.Chapter} in which $X_1,X_2,\ldots,X_n$ are 
distributed iid $\mathcal{N}(\mu,\sigma_0^2)$, and $\sigma_0^2$ is known but $\mu$ is unknown.  Placing a normal prior on $\mu$ 
according to $\mu \sim \mathcal{N}(m,s^2)$ gives: \index{subjectindex}{iid}
\begin{equation}
    \pi(\mu|\x) \propto \exp\left[-\frac{1}{2} \left(\frac{1}{s^2} + \frac{n}{\sigma_0^2}\right)
                   \left(\mu - \frac{ \left(\frac{m}{s^2} + \frac{n\bar{x}}{\sigma_0^2}\right) }
                                    { \left(\frac{1}{s^2} + \frac{n}{\sigma_0^2}\right) }
                \right)^2 \right].
\end{equation}
As a simplifying procedure, re-express this posterior in terms of its mean and variance:
\begin{align}
    &\pi(\mu|\x) \propto \exp \left[-\frac{1}{2\sigma_1^2}(\mu - \mu_1)^2 \right]   	  \nonumber\9
    &\qquad\qquad \text{where:}\; \sigma_1^2 = \left(\frac{1}{s^2} + \frac{n}{\sigma_0^2}\right)^{-1}   \qquad
    \mu_1 = \frac{ \left(\frac{m}{s^2} + \frac{n\bar{x}}{\sigma_0^2}\right) }
                 { \left(\frac{1}{s^2} + \frac{n}{\sigma_0^2}\right) }.                   \nonumber
\end{align}
Therefore (\ref{posterior.predictive.distribution}) for this model is:
\begin{align}
    p(x_{\new}|\mathbf{x}) &= \int_{\mu}p(x_{\new}|\mu)p(\mu|\mathbf{x})d\mu           \nonumber \9
                          &\propto \int_{\mu}\exp\left[
                            -\frac{1}{2}\left( \frac{(x_{\new}-\mu)^2}{\sigma_0^2}
                                              +\frac{(\mu-\mu_1)^2}{\sigma_1^2}
                \right) \right]d\mu.
\end{align}
This allows us to calculate summary statistics for the posterior predictive distribution:
\index{subjectindex}{posterior predictive distribution}
\begin{align}
    E[x_{\new}|\x] &= E[E(x_{\new}|\mu,\x)|\x]        \nonumber \\
                    &= E[E(\mu|\x)]                 \nonumber \\
                  &= E[\mu] = \mu_1.                \nonumber \\[5pt]
    \Var[x_{\new}|\x] &= E[\Var(x_{\new}|\mu,\x)|\x]
                 + \Var[E(x_{\new}|\mu,\x)|\x]        \nonumber \\
                &= E[\sigma_0^2|\x] + \Var[\mu|\x]  \nonumber \\
                &= \sigma_0^2 + \sigma_1^2/n.
\end{align}
These results demonstrate that the posterior predictive distribution in this example has the same expected
value as the posterior but greater variance, reflecting additional uncertainty\index{subjectindex}{uncertainty} 
about an \emph{unobserved} quantity rather than the posterior description of the \emph{observed}
quantities, $\mathbf{x}$.  Furthermore, since we know that the distribution for $x_{\new}$ is normal, then we
have the unconditional PDF:
\begin{equation}\label{unconditional.postererior.predictive}
    x_{\new}|\mathbf{x} \sim \mathcal{N}\left( \frac{ \left(\frac{m}{s^2} + \frac{n\bar{x}}{\sigma_0^2}\right) }
                                            { \left(\frac{1}{s^2} + \frac{n}{\sigma_0^2}\right) },
                                            \sigma_0^2 + \left(\frac{1}{s^2} + \frac{n}{\sigma_0^2}\right)^{-1}\frac{1}{n}
                                     \right),
\end{equation}
which we can easily draw from.

Since the parametric form for additional observations is fully defined by
(\ref{posterior.predictive.distribution}), multiple draws can be made to form a simulated data set.  If
the replicated data have distributional qualities noticeably different from the observed data, then
this is an indication of poor model fit (Gelman\index{authorindex}{Gelman, A.} \etal 2003, 
Meng, \index{authorindex}{Meng, M.} 1994a).  The posterior predictive distribution of the data can also be
used to make explicit model comparisons between considered alternatives (Chen, Dey, and Ibrahim 2000).
\index{authorindex}{Chen, M-H.} \index{authorindex}{Dey, D. K.} \index{authorindex}{Ibrahim, J. G.}

\begin{examplelist}
    \item   {\bf Economic Growth in Sub-Saharan Africa.}\label{africa.growth.example}
            \index{subjectindex}{example!African Economic Growth}
            This example makes use of data provided by Bratton and Van De Walle (1997) \index{authorindex}{Bratton, M.}
            \index{authorindex}{Van De Walle, N.} from various authoritative and reliable sources, made available through
            the \emph{Inter-University Consortium for Political and Social Research} (ICPSR 1996).  The authors collect data on
            regime transition for 47 sub-Saharan countries over the period from each colonial independence to 1989, with some
            additional variables collected for the period 1990 to 1994.  These data include 99 variables describing governmental,
            economic, and social conditions for the 47 cases.  Also included are data from 106 presidential and 185 parliamentary
            elections, including information about political parties, turnout, and openness.  We use only the average annual rate
            of growth in GNP per capita, in percent. One case is missing this value, so $n=46$ here.

            \begin{table}[h]
            \parbox[c]{\linewidth}{
              \tabletitle{\textsc{Sub-Saharan Africa Average Economic Growth Rate, 1990-1994}}\label{Africa.Growth.Table}
              \begin{small} \begin{center}\begin{tabular}{lr|lr|lr|lr}
              \multicolumn{8}{c}{}\\[-4pt]
              \multicolumn{8}{c}{Average Economic Growth by African Country, 1965-1989}\\
              \multicolumn{8}{c}{}\\[-4pt]
              \hline
              \multicolumn{8}{c}{}\\[-4pt]
                Angola       & -2.1 & Benin     & -0.1 & Botswana   &  8.5 & Burkina Faso      &  1.4 \4
                Burundi      &  3.6 & Cameroon  &  3.2 & Cape Verde &  4.0 & Ctrl. Afr. Repub. & -0.5 \4
                Chad         & -1.2 & Comoros   &  0.5 & Congo      &  3.3 & Cote d'Ivoire     &  0.8 \4
                Djibouti     &   NA & Eq.Guinea &  0.0 & Ethiopia   & -0.1 & Gabon             &  0.9 \4
                Gambia       &  0.7 & Ghana     & -1.5 & Guinea     &  0.0 & Guinea Bissau     & -3.0 \4
                Kenya        &  2.0 & Lesotho   &  5.0 & Liberia    & -2.0 & Madagascar        & -1.9 \4
                Malawi       &  1.0 & Mali      &  1.7 & Mauritani  & -0.5 & Mauritius         &  3.0 \4
                Mozambique   & -8.2 & Namibia   & -1.2 & Niger      & -2.4 & Nigeria           &  0.2 \4
                Rwanda       &  1.2 & Saotome   &  1.5 & Senegal    & -0.7 & Seychelles        &  3.2 \4
                Sierra Leone &  0.2 & Somalia   &  0.3 & S.Africa   &  3.5 & Sudan             & -2.0 \4
                Swaziland    &  2.1 & Tanzania  & -0.1 & Togo       &  0.0 & Uganda            & -2.8 \4
                Zaire        & -2.0 & Zambia    & -2.0 & Zimbabwe   &  1.2 &                   &      \\
              \end{tabular} \end{center} \end{small}
            }
            \end{table}

            We will start with the prior $\mathcal{N}(0,4)$ for $\mu$, which might be somewhat cynical, and set 
            $\sigma_0^2=7$ as if it were known.  In order to describe the posterior predictive distribution we plot 
            5000 draws from the distribution given by \eqref{unconditional.postererior.predictive}, as shown in
            Figure~\ref{Africa.PPD.Fig}.  In this figure the solid line is a density estimate from the draws and
            the dashed line shows the prior distribution. % NEED TO ADD TO BaM  

            \begin{figure}[h]
            \centerline{
            \epsfig{file=Images/robust.figure06.ps,height=4.15in,width=6.35in,clip=,angle=0}  }
            \vspace{-22pt}
            \caption{\textsc{Posterior Predictive Distribution Draws, Africa Growth}} \label{Africa.PPD.Fig}
            \end{figure}
\end{examplelist}

\section{Simple Bayesian Model Averaging}\index{subjectindex}{Bayesian model averaging|(}\label{model.averaging.section}
Anything not known for certain in the Bayesian framework is treated probabilistically.  There is no reason that model
specification, which is rarely known for certain, cannot also be treated as such.  There are two key components to this approach:
assigned priors to \emph{models}, and producing posterior distributions that are averaged across model space.  This idea of
putting priors on models makes sense since we typically have information that supports some specifications over others.  For
recent discussions see Montgomery and Nyhan (2010) and Young (2009).
\index{authorindex}{Montgomery, J. M.} \index{authorindex}{Nyhan, B.} \index{authorindex}{Young, C.}

A starting point is the idea of a mixture distribution described in detail in Section~\ref{normal.mixture.models} starting on
page~\pageref{normal.mixture.models}.\index{subjectindex}{mixture distribution}  We could create a likelihood based on 
mixtures by stipulating $K$ distinct parametric forms that might be used to explain the data generation process, weighting these 
$\omega_1,\omega_2,\ldots,\omega_K$, where $\sum\omega_i = 1$.  We will stipulate that each of the parametric forms is from the same 
parametric family, but differs in parametric assignment.  Thus $f_k(x|\T_k)$ is the $k$th specification for $x$ with values given in the 
parameter vector $\T_k$.  Now the likelihood for this mixture is given by:
\begin{equation}
	L_m(\T|\x) = \prod_{i=1}^n \sum_{k=1}^K \omega_k f_k(x_i|\T_k).
\end{equation}
\index{authorindex}{Titterington, D. M.} \index{authorindex}{Smith, A. F. M.} \index{authorindex}{Makov, U. E.} 
The likelihood function is therefore the product of the mixture sums for each data value.  Mixture models of this type are very
flexible (see Titterington, Smith, and Makov [1985]), but suffer from the problem that lots of quantities need to be specified
directly by the researcher: the mixture proportions, and the full $\T$ vector for each mixture component.  So for a mixture of ten
normals, thirty constants need to be provided.  For discussion of this issue and implementation concerns in more complicated
scenarios, see Diebolt and Robert (1994), Kalyanam (1996), Stephens (2000), and West (1992). 
\index{authorindex}{Diebolt, J.} \index{authorindex}{Robert, C. P.} \index{authorindex}{Kalyanam, K.}
\index{authorindex}{Stephens, M.} \index{authorindex}{West, M.}

Suppose now that we are choosing between: $M_1, M_2,\ldots,M_K$ and for each $k$, determine a model prior: $p(M_k)$.
Recall that the \emph{integrated likelihood} is the denominator of Bayes' Law:\index{subjectindex}{integrated likelihood}
\begin{equation}
        p(\x|M_k) = \int \underbrace{ p(\T_k|M_k,\x) p(\T_k|M_k) }_{ \text{likelihood}\times\text{prior} }d\T_k.
\end{equation}
This lets us calculate the \emph{posterior model probability} for each model, $\kappa=1\ldots K$:
\begin{equation}
            \pi(M_{\kappa}|\x) = \frac{ p(\x|M_{\kappa})p(M_{\kappa}) }{ \sum_{j=1}^K p(\x|M_j)p(M_j) }.
\end{equation}

This discussion uses a simple averaging scheme from Raftery (1995), but see also Hoeting \etal (1999).  Consider the posterior 
mean and variance for the $j^{th}$ coefficient of the model indexed by $\kappa^{th}$, ${\T}_j(\kappa)$ and $Var_{\T_j}(\kappa)$.  
Now:	\index{authorindex}{Raftery, A. E.} \index{authorindex}{Hoeting, J. A.}
\begin{equation}\label{bma.prob.nonzero}
            p(\T_j \ne 0|\x) = \sum_{\T_j \in M_\kappa} \pi(M_\kappa|\x),
\end{equation}
which is just the ``posterior probability that $\T_j$ is in the model,'' as well as:
\begin{equation}\label{bma.mean}
            E[\T_j|\x] \approx \sum_{\kappa=1}^K \T_j(\kappa) \pi(M_\kappa|\x)
\end{equation}
and:
\begin{equation}\label{bma.variance}
            \Var[\T_j|\x] \approx \sum_{\kappa=1}^K \left[
                ( \Var_{\T_j}(\kappa) + \T_j(\kappa)^2)\pi(M_\kappa|\x) )
                -  E[\T_j|\x]^2 \right],         
\end{equation}
where we apply standard definitions to these new quantities.  Note that the summation in both the expected value and the variance 
calculations are both averages when (as by convention) the prior model weights sum to one.  Criticism of this approach, in particular
generation of values such as \eqref{bma.prob.nonzero} and the subsequent summaries, is that the researcher has made decisions about
what variables to include in the various specifications and can therefore increase or decrease expected values or variances by 
strategical selection.  However, since inclusions or exclusions are overt, readers can judge the quality of resulting averages.

\section{Concluding Comments on Model Quality}\label{concluding.remarks.section}    
The quality of a statistical model is a function of several criteria:
\begin{bayeslist}
    \item   Parsimony
    \item   Completeness and generalizeability
    \item   Clarity
    \item   Robustness
    \item   Resistance
    \item   Predictive ability
    \item   Adherence to trends in the data.
\end{bayeslist}
\index{authorindex}{Barnett, V.} \index{authorindex}{Blalock, H. M.} \index{authorindex}{Box, G. E. P.}
\index{authorindex}{Howson, C.} \index{authorindex}{Urbach, P.} \index{authorindex}{Leamer, E. E.}
\index{authorindex}{Miller, A. J.} \index{authorindex}{Russell, B.}
General assessment of model quality is a vast literature in both Bayesian and classical 
statistics; landmarks include Barnett (1973), Blalock (1961), Box (1980), Howson and Urbach (1993), 
Leamer (1978), Miller (2002), and Russell (1929).  Regretfully, recommended criteria can be at odds with 
each other.  For example, parsimony comes at the expense of closeness to the data in that we 
can never \emph{reduce} the overall fit of the model by adding more explanatory variables, but 
this often creates overparameterized and theoretically confusing results 
(Kreuzenkamp\index{authorindex}{Kreuzenkamp, H. A.} and 
McAleer\index{authorindex}{McAleer, M.} 1995).  In a familiar 
example, the linear model is well-known to be fairly robust to 
mild deviations from the  underlying Gauss-Markov assumptions 
(Shao\index{authorindex}{Shao, J.} 2005), but it is 
not very resistant to outliers in the data 
(Leamer\index{authorindex}{Leamer, E. E.} 1984, 
Rao\index{authorindex}{Rao, C. R.} 
and Toutenburg\index{authorindex}{Toutenburg, H.} 1995, Chapter 9).

Model checking\index{subjectindex}{model checking} is the general procedure for comparing final conclusions with our expectations
or knowledge of reality.  Obviously we will want to dismiss any model specification that produces inferences (Bayesian or
otherwise) that do not comport with some sense of substantive reasonableness.  Where this arises most often is when certain model
features are specified for mathematical or data collection convenience.  Suppose that a conjugate prior is specified for a given
model without a great amount of substantive justification.  A reader is less inclined to be critical if it can be shown that the
consequences of the posterior distribution are in accordance with sound thinking in terms of the theoretical or situational
setting.  Conversely, one should be wary of assumptions like conjugate priors applied merely for analytical ease if they provide
dubious substantive conclusions.

Model checking, through sensitivity analysis or robustness evaluation, is not a panacea.  Bartels\index{authorindex}{Bartels, L.
M.} points out that \emph{ad hoc} nonsystematic sensitivity analysis can produce wildly wrong statements about model quality
(1997, pp.668-669).  We should therefore see these tools in a more general context as components of the broader task of checking
that also includes looking at the data, carefully considering the likelihood function used, and reasonableness checks with
posterior predictive distributions.  \index{subjectindex}{posterior predictive distribution}

The Bayesian approach to model checking also circumvents a common problem in social science data analysis.  Typically when a
social science model with a null hypothesis significance test is reported, it is presented as if only two models were ever
considered or ever deserved to be considered: the null hypothesis and the provided research hypothesis.  The quality of a research
finding is then solely judged on the ability to reject the complementary null hypothesis with a sufficiently low
p-value.\index{subjectindex}{hypothesis testing!NHST}

This process proceeds in an artificially exclusive manner regarding the selecting of a model {\bf M}: ``\ldots examining the data
in $x$ to identify the single `best' choice {\bf M}* for {\bf M}, and then proceeding as if {\bf M}* were known to be correct in
making inferences and predictions'' (Draper\index{authorindex}{Draper, D.} 1995, p.46).  However, during the development of the
reported model, many differing alternate mixes of independent variables are tested.  This is an ``illusion of theory
confirmation'' (Greenwald\index{authorindex}{Greenwald, A. G.} 1975; Lindsay\index{authorindex}{Lindsay, R. M.} 1995) because the
null hypothesis significance test is presented as evidence of the exclusivity of explanation of this single research hypothesis.
Summary statistics are reported from the $n^{th}$ equation as if the other $n-1$ never existed and this last model is produced
from a fully controlled experiment (Leamer\index{authorindex}{Leamer, E. E.} 1978, p.4).  The null hypothesis significance test thus
provides an infinitely strong bias in favor of a single research hypothesis against a huge number of other reasonable hypotheses:
all other distributional alternatives are assumed to have probability zero (Rozeboom\index{authorindex}{Rozeboom, W. W.} 1960,
Lehmann\index{authorindex}{Lehmann, E. L.} 1986, 68, Popper\index{authorindex}{Popper, K.} 1968, 113).  Worse yet are solutions
that rely upon atheoretic stepwise or other mechanical search processes; for a litany and review, see
Adams\index{authorindex}{Adams, J. L.} (1991), or Sala-I-Martin\index{authorindex}{Sala-I-Martin, X.} (1997).

Two entirely reasonable and statistically significant, competing models can lead to substantively
different conclusions using exactly the same data (Raftery\index{authorindex}{Raftery, A. E.} 1995).  This leads to the question of
determining which of the two equally plausible specifications is in some way ``better.''  The answer is
generally one in which the posterior is less sensitive to changes in the underlying assumptions
(robustness) and less sensitive to influential outliers (resistance).  The described process
of \emph{sensitivity analysis} makes small changes in the assumptions of the model, such as the
parameterization of the prior distribution, and observes whether or not unreasonably large
changes are observed in the subsequent posterior.  We can therefore determine which assumptions
are worth further investigation, and possibly which conclusions are more ``fragile.''  This 
illustrates how the Bayesian approach is more naturally inclusive of alternatives and more
overtly skeptical about assumptions.

\section{Exercises}
\begin{exercises}
    \item 	Derive the marginal posterior for $\B$ in 
		    \begin{equation*}
		    	\pi(\B|\X,\y) \propto \left[ \tilde{s} + 
		    	(\B-\tilde{\B})'(\SI^{-1}+\X'\X)(\B-\tilde{\B}) \right]^{-\frac{n+a}{2}}, 
		    \end{equation*}
		    where:
		    \begin{align*}
    			\tilde{\B} &= (\SI^{-1} + \X'\X)^{-1}(\SI^{-1}\mathbb{B} +  \X'\X\bh) \9
    			\tilde{s}  &= 2b + \hat{\sigma}^2(n-k) + (\mathbb{B}-\tilde{\B})'\SI^{-1}\mathbb{B}
                        + (\bh - \tilde{\B})'\X'\X\bh
		    \end{align*}
		    from the joint distribution given by 
		    \begin{align*}	
    			\pi(\B,&\mathbf{\sigma}^2|\X,\y) \nonumber\\
			       &\propto \sigma^{-n-a}\exp\left[ -\frac{1}{2\sigma^2} \left( \tilde{s}
        		+ (\B-\tilde{\B})'(\SI^{-1}+\X'\X)(\B-\tilde{\B}) \right) \right].
		    \end{align*}
		    See Chapter~\ref{Linear.Chapter} (page~\pageref{zellner.regression.terms}) for terminology details.
          
            % NEW
    \item   Using the \jags\ code in Example~\ref{example:abortion.attitudes} starting on
            page~\pageref{example:abortion.attitudes}, replicate Figure~\ref{Data.Comparison.Fig} on
            page~\pageref{Data.Comparison.Fig} with different variance components priors.  Is it possible to improve the visual
            fit? See Chapter~\ref{Software.Chapter} for details on running the \jags\ software.

    \item 	Derive the form of the posterior predictive distribution for the beta-binomial model.
		    \index{subjectindex}{posterior predictive distribution!beta-binomial} See Section~\ref{posterior.predictive.section}
            starting on page~\pageref{posterior.predictive.section}.
          
            % NEW
    \item   On page~\pageref{cauchy.matching} it was noted that the IQR of $\mathcal{N}(\theta,1)$ distribution matches that of
            $\mathcal{C}(\theta,0.675)$.  Find the Cauchy PDF with matching $(0.05, 0.95)$ quantiles for a 
            $\mathcal{N}(\theta,2.5)$.

    \item 	Calculate the Kullback-Leibler distance between two gamma distributions,
            $f(\X|\alpha,\beta)$, $g(\X|\gamma,\delta)$, based on the same data but different parameters.
		    \index{subjectindex}{Kullback-Leibler distance}
          
            % NEW
    \item   Prove that the integral in \eqref{finite.mean.posterior} on page~\pageref{finite.mean.posterior} is finite.

    \item 	Calculate the Polasek \eqref{polasek.linear.model} linear model diagnostic for each of the 
		    \index{authorindex}{Polasek, W.} data points in your model from Exercise~\ref{Linear.Chapter}.\ref{meier.keiser.exercise}.
		    Graphically summarize your result identifying points of concern.
          
            % NEW
    \item   Given an iid random sample $\X \sim \mathcal{N}(\theta,\sigma^2)$, define a normal conjugate neighborhood prior for $\theta$
            as $Gamma = \{ \mathcal{N}(\mu,\tau^{-1}): 0 < \tau^L < \tau < \tau^U < \infty$ \}.  The posterior expectation of $\theta$
            is $E[\theta|\X,\mu,\tau] = (n\sigma^2\bar{\X} + \tau\mu)/(n\sigma^2 + \tau)$.  Derive the range of this expectation.
            %|\bar{\X}-\mu|(n\tau(\tau^U-\tau^L))/((\tau^L+n\tau)(\tau^U+n\tau))
            \index{subjectindex}{iid}

    \item 	In Section~\ref{local.sensitivity.section} it was shown how to vary the prior variance 50\% by manipulating the 
            beta parameters individually.  Perform this same analysis with the two parameters in the Poisson-gamma conjugate 
		    specification.  Produce a graph like Figure~\ref{Expo.Sensitivity.Fig} on page~\pageref{Expo.Sensitivity.Fig}.
          
            % NEW
    \item   Show that the prior in \eqref{perturbed.prior} produces the posterior in \eqref{perturbed.posterior}
            (page~\pageref{perturbed.prior}), and plot a comparison for difference values of $\epsilon$ with simulated data.

    \item 	\label{firearms.exercise}\index{subjectindex}{example!firearms-related deaths}
            The following data (\texttt{firearm.deaths} in \texttt{BaM}) are annual firearm-related deaths in the United States
            per 100,000 from 1980 to 1997, by row (source: National Center for Health Statistics, Health and Aging Chartbook,
            August 24, 2001):
            \begin{center}
            \parbox[c]{\linewidth}{
				\begin{normalfont}\begin{tabular}{lcccccccccccc}
				14.9 & 14.8 & 14.3 & 13.3 & 13.3 & 13.3 & 13.9 & 13.6 & 13.9 \\ 
				14.1 & 14.9 & 15.2 & 14.8 & 15.4 & 14.8 & 13.7 & 12.8 & 12.1 \\
			    \end{tabular}\end{normalfont}\vspace{5pt}
            }
            \end{center}
		    Specify a normal model for these data with a diffuse normal prior for $\mu$ and a diffuse inverse gamma prior for 
            $\sigma^2$ (see Section~\ref{both.unknown.section} starting on page~\pageref{both.unknown.section} for the more complex
            multivariate specification that can be simplified).  In order to obtain local robustness of this specification,
            calculate $||D(q)||$ (see page~\pageref{posterior.sensitivity}) for the alternative priors: $q(\mu) =
            \mathcal{N}(14,1)$, $q(\mu) = \mathcal{N}(12,3)$, $q(\sigma^{-2}) = \mathcal{G}(1,1)$,  $q(\sigma^{-2}) =
            \mathcal{G}(2,4)$.
          
            % NEW
   \item    Berger (1990) gives four important features for a class of priors from expert elicitation: (1) simple elicitation and
   interpretation, (2) including as many reasonable forms as possible, (3) no unreasonable forms that are precluded by measurement
   or functional form, and (4) easy calculations.  Find a published example that violates at least one of these principles.

	\item 	Calculate the posterior predictive distribution of $X$ in Exercise~\ref{Model.Quality.Chapter}.\ref{firearms.exercise} 
		    assuming that $\sigma_0^2=0.75$ and using the prior: $p(\mu) = \mathcal{N}(15,1)$.  Perform the same analysis using the 
            prior: $\mathcal{C}(15,0.675)$.  Describe the difference graphically and with quantiles.  
            \index{subjectindex}{posterior predictive distribution}
          
            % NEW
    \item   Returning to the Africa economic growth data from Example~\ref{africa.growth.example}, notice that Botswana appears
            to be an outlier at 8.5\% growth.  Calculate the probability of getting this value or higher from the prior distribution
            given in the example and the posterior predictive distribution calculated.  Make a histogram of the posterior predictive
            distribution and draw a vertical bar at the value for Botswana.

	\item 	Calculate the posterior predictive distribution for the Palm Beach County model (Section~\ref{pbc.example} starting on
            page~\pageref{pbc.example}) using both the conjugate prior and the uninformative prior.
          
            % NEW
    \item   The cross-validation (also called the leave-one-out) posterior predictive distribution is given by:
            \index{subjectindex}{posterior predictive distribution!cross validation}
            \begin{equation*}
                p(x_{\new}|\mathbf{x}_{-i}) = \int_{\Theta}p(x_{\new}|\theta)p(\theta|\mathbf{x}_{-i})d\theta.
            \end{equation*}
            where $\mathbf{x}_{-i}$ denotes the dataset removing the $i$th case (Stern and Cressie 2000).
            \index{authorindex}{Stern, H. S.} \index{authorindex}{Cressie, N.}
            A small value of $p(x_{\new}|\mathbf{x}_{-i})$ means that this new value will occur infrequently in the absence of $\x_i$.
            Further define: 
            \begin{equation*}
                A = \frac{1}{n} \sum_{i=1}^n \log\left( p(x_{\new}|\mathbf{x}_{-i}) \right),
            \end{equation*}
            which is smaller for better fitting models and asymptotically equivalent to the AIC for iid samples (Stone 1977a, 1977b). 
            \index{authorindex}{Stone, M.}  Replicate the plot from Example~\ref{africa.growth.example} using the cross-validation
            posterior predictive distribution.  Observe any differences from the original analysis.  Calculate the $A$ statistic.
            \index{subjectindex}{iid}

	\item	Using the model specification in \eqref{abortion.model.specification}, produce a posterior distribution for appropriate 
            survey data of your choosing.  Manipulate the forms of the priors and indicate the robustness of this specification for 
            your setting.  Summarize the posterior results with tabulated quantiles.
          
            % NEW
    \item   Calculation of the posterior predictive distribution in cases where the integral cannot be calculated or standard
            parametric forms are not used can be challenging.  Fortunately, the common use of MCMC estimation in Bayesian
            modeling makes this task easier.  Suppose $X$ and $Y$ are bivariate normal with mean vector $(0,0)$ and covariance
            matrix $\left[ \begin{smallmatrix} 1 & \rho \\ \rho & 1 \end{smallmatrix} \right]$, with $\rho=0.75$.  Write a Gibbs
            sampler in \R\ that draws from the full conditional distribution:
            \begin{align*}
                X|Y &\sim \mathcal{N}(\rho y,1-\rho^2)= \rho y + \sqrt{1-\rho^2}\mathcal{N}(0,1) \4
                Y|X &\sim \mathcal{N}(\rho x,1-\rho^2)= \rho x + \sqrt{1-\rho^2}\mathcal{N}(0,1).
            \end{align*}
            and show graphically that the marginal distributions are as expected.  Take a bivariate sample of $n=100$ and treat
            this as observed data.  Now estimate $\rho$ as if unknown and produce a sample of posterior predictive values from
            this result.  Do they differ from the Gibbs sampling output?

	\item	Leamer (1983) proposes reporting extreme bounds for coefficient values during the specification search as a way to 
            demonstrate the reliability of effects across model-space.  Thus the researcher would record the maximum and minimum 
            values that every coefficient took on as different model choices were attempted.  Explain how this relates to 
            Bayesian model averaging.
          
            % NEW
    \item   Observed data of size $n$ are known to be classified into $k=1,\ldots,K$ groups and distributed 
            $x_k \sim \mathcal{N}(\theta_k,s^2_k)$ where the $\theta_k$ parameters are distributed $\mathcal{N}(\mu,\sigma^2)$.
            In this hierarchical setting (continued in Chapter~\ref{Hierarchical.Chapter}) posterior conclusions about
            $\theta$ and $\mu$ are known to be sensitive to the prior treatment of the random effects standard deviation 
            $\sigma$ and many approaches have been suggested.  DuMouchel and Normand 
            (2000) suggest a uniform prior for $s_0/(s_0+\sigma)$ where $s_0^2$ is the harmonic mean of the group variances:
            $(\frac{1}{K}\sum s_k^{-2})^{-1}$.  Show that this means that $p(\sigma) = s_0/(s_0+\sigma)^2$ and 
            $p(\sigma^2) = s_0/(2\sigma(s_0+\sigma)^2)$.
            \index{authorindex}{DuMouchel, W.}\index{authorindex}{Normand, S.}
\end{exercises}

%%\thispagestyle{empty}
%\chapter{Generalized Linear Model Review}\label{GLM.Chapter}
\chapter{Generalized Linear Model Review}\label{GLM.Chapter}
%\renewcommand{\thechapter}{\Alph{chapter}}
\setcounter{examplecounter}{1}

\section{Terms}
Consider a one-parameter conditional probability density function or probability mass function 
for the random variable Z of the form: $f(z|\zeta)$.  This family form is classified as an 
exponential family form\index{subjectindex}{exponential family form} if it can be reparameterized as: 
$f(z|\zeta) = \exp\bigl[t(z)u(\zeta)\bigr]r(z)s(\zeta)$, where: $r$ and $t$ are real-valued 
functions of $z$ that do not depend on $\zeta$, and $s$ and $u$ are real-valued functions of 
$\zeta$ that do not depend on $z$, and $r(z) > 0, s(\zeta) > 0 \; \forall z,\zeta$.
The canonical form\index{subjectindex}{canonical form} is the result of one-to-one transformation that reduces 
the complexity of the symbolism and reveals structure.  If $t(z) = z$, then we say that this PDF 
or PMF is in its canonical form for the random variable $Z$.  Otherwise, we can make the simple 
transformation: $y = t(z)$ to force a canonical form.  Similarly, if $u(\zeta) = \zeta$ in this 
expression, then this PDF or PMF is in its canonical form for the parameter $\zeta$.  If not, 
we can again force a canonical form by transforming: $\theta= u(\zeta)$, and call $\theta$ the 
canonical parameter.  This produces the final form: 
$f(y|\theta) = \exp\bigl[y\theta - b(\theta) + c(y)\bigr]$.
Here $b(\theta)$ plays a key role in calculating the moments of the distribution and other 
important quantities.  If the form of $\theta$, the \emph{canonical link}\index{subjectindex}{canonical link} 
between the original form and the $\theta$ parameterized form, is the source of the link 
function\index{subjectindex}{link function} in generalized linear models\index{subjectindex}{GLM} 
(Fahrmeir\index{authorindex}{Fahrmeir, L.}
and Kaufmann\index{authorindex}{Kaufmann, H.} 1985; 
J{\o}rgensen\index{authorindex}{J{\o}rgensen, B.} 1983; 
Wedderburn\index{authorindex}{Wedderburn, R. W. M.} 1976), then this process is equivalent to finding a 
$k$-dimensional global modal point.

Our real interest lies in obtaining the posterior distribution\index{subjectindex}{posterior distribution}
 of the unknown $k$-dimensional $\T$ coefficient vector, given an observed $\X$ matrix of 
data values: $p(\T|\X)$.  This allows us to determine the ``most likely'' values of the
$\T$ vector using the $k$-dimensional mode (maximum likelihood inference, Fisher\index{authorindex}{Fisher, R. A.}
1925b), or alternatively to describe the resulting distribution (as in Bayesian inference).
This posterior is produced by Bayes' Law:\index{subjectindex}{Bayes' Law}
\begin{equation}\label{bayes.law.new}
    p(\T|\X) = p(\X|\T)\frac{p(\T)}{p(\X)}
\end{equation}
where $p(\X|\T)$ is the $n$-dimensional joint probability function for data (the 
\emph{probability of the sample} for a fixed $\T$) under the assumption that the data are 
independent and identically distributed according to $p(X_i|\T) \; \forall \; i=1,\ldots,n$, 
and $p(\T)$, $p(\X)$ are the corresponding unconditional probabilities.\index{subjectindex}{unconditional probability}

The Bayesian approach integrates to obtain ${p(\X)}$ (or ignores it using proportionality)
\index{subjectindex}{proportionality} and specifies an assumed (prior) distribution on $\T$, thus allowing 
fairly direct computation of $p(\T|\X)$ from (\ref{bayes.law.new}).  If we regard $p(\X|\T)$ 
as a function of $\T$ for some given observed data $\X$, then 
$L(\T|\X) = \prod_{i=1}^{n}p(\X|\T)$ is called a likelihood function (DeGroot\index{authorindex}{DeGroot, M. H.}
1986, p.339).  The maximum likelihood principle states that an \index{subjectindex}{admissible}
    admissible\footnote{\emph{Admissible} here means values of $\theta$ are taken from the valid parameter space ($\Theta$):
    values of $\theta$ that are unreasonable according to the form of the sampling distribution of $\theta$ are not considered
    (integrated over).  A Bayesian decision rule is called \emph{admissible} if it is no worse (equal or lower risk) than every
    other decision rule and has at least one better alternative for all possible values of the parameter as discussed in 
    Chapter~\ref{Decision.Chapter} See also: Berger\index{authorindex}{Berger, J. O.} (1985, Section~1.3;
    Ghosh\index{authorindex}{Ghosh, M.} and Meeden\index{authorindex}{Meeden, G.} (1997, Section~2.2).}
$\T$ that maximizes the likelihood function probability (discrete case) or density (continuous case), 
relative to alternative values of $\T$, provides the $\T$ that is most ``likely'' to have 
generated the observed data, $\X$, given the assumed parametric form.  Restated, if $\hat{\T}$ 
is the maximum likelihood estimator for the unknown parameter vector, then it is necessarily true 
that $L(\hat{\T}|\X) \ge L(\T|\X) \; \forall \; \T \in \mathbf{\Theta}$, where $\mathbf{\Theta}$ 
is the admissible set of $\T$.

The likelihood function\index{subjectindex}{likelihood function} differs from the inverse probability\index{subjectindex}{inverse
probability}, $p(\T|\X)$, in that it is necessarily a \emph{relative} function since it is not a normalized probability measure
($\mathfrak{P}()$) bounded by zero and 
    one.\footnote{From a frequentist standpoint, the probabilistic\index{subjectindex}{frequentism}
    uncertainty is a characteristic of the random variable $\X$, not the unknown but fixed
    $\T$.  Barnett\index{authorindex}{Barnett, V.} (1973, p.131) clarifies this distinction: ``Probability remains
    attached to $X$, not $\theta$; it simply reflects inferentially on $\theta$.''}
    \index{subjectindex}{uncertainty}
Thus maximum likelihood estimation substitutes the unbounded notion of likelihood for the bounded
definition of probability (Casella\index{authorindex}{Casella, G.} and 
Berger\index{authorindex}{Berger, R. L.} 2002, 
p. 316; Fisher\index{authorindex}{Fisher, R. A.} 1922, p.327; 
King\index{authorindex}{King, G.} 1989, p.23).  This
is an important theoretical distinction, but of little significance in applied practice.

Typically it is mathematically more convenient to work with the natural log of the likelihood
function.  This does not change any of the resulting parameter estimates because the likelihood
function and the log likelihood function have identical modal points.  Using a probability density
function for a single parameter of interest the basic \emph{log} likelihood function is very simple:
\index{subjectindex}{log likelihood function}
\begin{equation}\label{standard.log.like.fn}
    \ell(\T|\X) = \log(L(\T|\X)),
\end{equation}
where we use $\ell(\T|\X)$ as shorthand to distinguish the log likelihood function from the likelihood
function: $L(\T|\X)$.

The score function is the first derivative of the log likelihood function with respect to the
parameters of interest:\index{subjectindex}{score function}
\begin{equation}\label{ll.score.fn}
    \dot{\ell}(\T|\X)
           = \frac{\partial}{\partial\T}\ell(\T|\X).
\end{equation}
Setting $\dot{\ell}(\T|\X)$ equal to zero and solving gives the maximum likelihood estimate, 
$\hat{\T}$.  This is now the ``most likely'' value of $\T$ from the parameter space $\Theta$
treating the observed data as given: $\hat{\T}$ maximizes the likelihood function at the observed
values.  The \emph{Likelihood Principle}\index{subjectindex}{likelihood principle} 
(Birnbaum\index{authorindex}{Birnbaum, A.} 1962) states that once the data are observed, and therefore treated 
as given, all of the available evidence for estimating $\hat{\T}$ is contained in the (log) 
likelihood function, $\ell(\T|\X)$.  This is a very handy data reduction tool because it tells us 
exactly what treatment of the data is important to us and allows us to ignore an infinite number 
of alternates (Poirer\index{authorindex}{Poirer, D. J.} 1988, p.127).

Setting the score function from the joint PDF or PMF equal to zero and rearranging gives the 
likelihood equation:\index{subjectindex}{likelihood!equation}
\begin{equation}\label{likelihood.equation}
    \sum t(x_i) = n\frac{\partial}{\partial\T}E[\x]
\end{equation}
where $\sum t(x_i)$ is the remaining function of the data, depending on the form of the 
probability density function (PDF) or probability mass function (PMF), and $E[\x]$ is the 
expectation over the kernel of the density function for 
    $\x$.\footnote{The kernel\index{subjectindex}{kernel} of a PDF or PMF is the component
    of the parametric expression that directly depends on the form of the random variable, i.e.,
    what is left when normalizing constants are omitted.  We can often work with kernels of
    distributions for convenience and recover all probabilistic information at the last stage
    of analysis by renormalizing (ensuring summation or integration to one).  The kernel is
    the component of the distribution that assigns \emph{relative} probabilities to levels of
    the random variable (see Gill\index{authorindex}{Gill, J.} 2000, Chapter~2).  For example the kernel 
    of a gamma distribution
    is just the part $x^{\alpha-1}\exp[-x\beta]$, without $\beta^{\alpha}/\Gamma(\alpha)$ (see 
    \ref{GLM.Chapter} for gamma distribution details).}
    \index{subjectindex}{normalizing factor}
The underlying theory is remarkably strong.  Solving (\ref{likelihood.equation}) for the unknown 
coefficient produces an estimator that is unique (a unimodal posterior distribution), consistent 
(converges in probability to the population value),\footnote{In one's 
    enthusiasm for the maximum likelihood estimator it is easy to forget that it is 
    asymptotically unbiased, but not necessarily unbiased in finite sample situations.
    For instance the maximum likelihood estimate for the variance of a normal model, $\hat{\sigma}^2 =
    \frac{1}{n}\sum(x_i - \bar{x})^2$ is biased by $n/(n-1)$.  This difference is rarely of significance
    and clearly the bias disappears in the limit, but it does illustrate that unbiasedness of 
    the maximum likelihood estimate is guaranteed only in asymptotic 
    circumstances.}
    \index{subjectindex}{bias}
    \index{subjectindex}{likelihood!properties of MLE}
and asymptotically efficient (the variance of the estimator achieves the lowest possible value 
as the sample size becomes adequately large: the Cram\'{e}r-Rao lower bound,\index{subjectindex}{Cram\'{e}r-Rao lower bound} 
see Shao\index{authorindex}{Shao, J.} 2005).  This result combined with the 
central limit theorem\index{subjectindex}{central limit theorem} gives the asymptotic normal form for the 
estimator:\index{subjectindex}{likelihood!normality} $\sqrt{n}(\hat{\T}-\T) 
\overset{\mathcal{D}}{\to} \mathcal{N}(\mathbf{0},\varSigma_{\T})$. 
This means that as the sample size gets large, the difference between the estimated value of $\T$ 
and the true value of $\T$ gets progressively close to zero, with a variance governed by 
$\frac{1}{\sqrt{n}}\varSigma_{\T}$, where $\varSigma_{\T}$ is the $k \times k$ variance-covariance 
matrix for $\T$.  Furthermore, $\sum t(x_i)$ is a sufficient statistic for $\T$, meaning that 
all of the relevant information about $\T$ in the data is contained in $\sum t(x_i)$.  For 
example, the normal log likelihood expressed as a joint exponential family form is 
$\ell(\T|\X) = \left(\mu\sum x_i - \frac{n\mu^2}{2}\right)/\sigma^2 - \frac{1}{2\sigma^2}\sum 
x_i^2 -\frac{n}{2}\log(2\pi\sigma^2)$.  So $t(\x) = \sum x_i$,
$\frac{d}{d\mu}\frac{n\mu^2}{2} = n\mu$, and equating gives the maximum likelihood 
estimate of $\mu$ to be the sample average that we know from basic texts: $\frac{1}{n}\sum x_i$.

\subsection{The Linear Regression Model}\label{linear.regression.introduction} 
\index{subjectindex}{example!linear model}
The classic linear model dates back to the early nineteenth century (see Stigler\index{authorindex}{Stigler, S. M.} 
1999, Chapter 17) and is the undeniable workhorse of statistical work in the social and behavioral 
sciences.  The linear model, as elegant as it is, requires a relatively strict set of assumptions.
The Gauss-Markov Theorem\index{subjectindex}{linear model!Gauss-Markov assumptions} states that if one can assume 
that:
\begin{enumerate}
        \item   the relationship between each explanatory variable and the outcome
                variable is approximately linear in structure,
        \item   the residuals are independent with mean zero and constant variance,
        \item   there is no correlation between any regressor and disturbance,
\end{enumerate}
then the solution produced by selecting coefficient values that minimize the sum of the squared 
residuals is unbiased and has the lowest total variance amongst unbiased linear alternatives.
This is sometimes called BLUE for \emph{Best Linear Unbiased Estimate}.

The linear model can be expressed as follows:
\begin{equation}
        \underset{(n \times 1)}{{\mathbf Y}}
                = \underset{(n \times p)(p \times 1)}{\X\B}
                 + \underset{(n \times 1)}{\EP},
	\qquad\quad
        \underset{(n \times 1)}{E[{\mathbf Y}]}
                = \underset{(n \times p)(p \times 1)}{\X\B.}
	\label{ols.basic}
\end{equation}
The right-hand sides of the two equations are very familiar: $\X$ is the model matrix of observed
data or the design matrix of stipulated values (organized by column), $\B$ is the column vector 
of unknown coefficients to be estimated, $\X\B$ is called the ``linear structure vector,'' 
\index{subjectindex}{linear structure vector} and $\EP$ is the column vector of (assumed) independent, normally 
distributed error terms with constant variance: the stochastic component.  In the expectation 
component of \eqref{ols.basic}, $E[{\mathbf Y}] = \T$ is the column vector of means: the 
systematic component.  The vector, ${\mathbf Y}$, is distributed iid\index{subjectindex}{iid} normal with mean $\T$, and 
constant variance ${\sigma^2}$.  This is exactly the multivariate linear model described in basic 
statistics texts, but provided here in matrix form.
Gauss\index{authorindex}{Gauss, C. F.} (1809) showed that a good way to estimate the unknown linear slope 
term(s) is to minimize the sum of the squared errors.\index{subjectindex}{linear model!squared errors}  The 
squaring is a convenience since otherwise positive and negative residuals will 
cancel each other out.  Actually there are many ways to accomplish this such as taking the 
absolute value of the residuals: $L_1$ regression\index{subjectindex}{L-regression}
\index{subjectindex}{quantile regression} 
(Legendre\index{authorindex}{Legendre, A. M.} 1805).  The principle behind L-regression (also called quantile 
regression) is that a selected quantile, $\kappa \in [0,1]$, is selected and the
following quantity is minimized:
\begin{equation}
        \underset{\hat{\B} \in \mathbb{R}}{\text{min}}
            \left[ \sum_{i=1}^{n} \kappa|y_i - x_i\hat{\B}| +
            \sum_{i=1}^{n} (1-\kappa)|y_i - x_i\hat{\B}| \right].
\end{equation}
Obviously if $\kappa =\frac{1}{2}$, this is greatly simplified and in fact becomes a well-known 
alternative to least squares estimates called the least absolute errors (LAE) 
\index{subjectindex}{least absolute errors regression} estimator, the $L_1$ estimator, or just the median 
regression estimator.  In the notation of (\ref{ols.basic}), minimizing squared error is 
equivalent to:\index{subjectindex}{linear model!squared errors} 
\begin{equation}
    \text{min}_\mathbf{b}\sum_{i=1}^{n}\varepsilon_i^2 = 
		\text{min}_\mathbf{b}\sum_{i=1}^{n}(\mathbf{y}_i-\X_i\mathbf{b})^2.
\end{equation}
Or in matrix notation, this is the process of minimizing the quantity $S(\mathbf{b})$ over the range of possible
values of $\mathbf{b}$:
\begin{equation}
     S(\mathbf{b}) = \EP'\EP                          				
          = (\mathbf{y}-\mathbf{Xb})'(\mathbf{y}-\mathbf{Xb})                
          = \mathbf{y}'\mathbf{y} -2\mathbf{b}\X'\mathbf{y} + \mathbf{b}\X'\X\mathbf{b}.
\end{equation}
This is a quadratic form with a positive sign on the squared term, so there is a unique value 
that minimizes $S(\mathbf{b})$.  We can therefore differentiate with respect to $\mathbf{b}$ and solve for zero to get:
\begin{align}
    \frac{\partial}{\partial b}S(\mathbf{b}) &=  -2\X'\mathbf{y} + 2\X'\X\mathbf{b} \equiv 0
        \nonumber \\
    \X'\X\mathbf{b} &=  \X'\mathbf{y}   \quad\text{(the ``normal equation'')}
        \nonumber \\
    \hat{\mathbf{b}} &=             (\X'\X)^{-1}\X'\mathbf{y}.
        \nonumber
\end{align}

In addition, it is also possible to derive an estimator of $\B$ using maximum 
likelihood estimation.  By the Gauss-Markov \emph{along with} the central limit theorem, the 
\index{subjectindex}{central limit theorem}
residuals are normally distributed with mean zero and constant variance.  The 
likelihood equation for the residuals is therefore:\index{subjectindex}{linear model!residuals}
\begin{align}
    L(\EP) &= (2\pi\sigma^2)^{-\frac{n}{2}} \exp\left[ -\frac{1}{2\sigma^2}\EP'\EP \right]
                \nonumber \\
           &= (2\pi\sigma^2)^{-\frac{n}{2}}\exp\left[ -\frac{1}{2\sigma^2}
                (\mathbf{y}-\mathbf{Xb})'(\mathbf{y}-\mathbf{Xb}) \right].
                \nonumber
\end{align}
The function $L(\EP)$ is concave for this equation, although concavity is not\index{subjectindex}{concavity}
a guaranteed feature of the likelihood equation.\footnote{In the absence of concavity over 
    the allowable range of parameter values multiple modes may exist or the maximum may not be 
    unique (Shao\index{authorindex}{Shao, J.} 2005).
    A number of authors have shown the existence and uniqueness of a maximum value not on the
    boundaries of the parameter space (and therefore concavity) for commonly specified likelihood
    functions: Haberman\index{authorindex}{Haberman, S. J.} (1974b), Kaufmann
    \index{authorindex}{Kaufmann, H.} (1988), 
    Lesaffre\index{authorindex}{Lesaffre, E.} and Kaufmann
    \index{authorindex}{Kaufmann, H.} (1992), 
    Makelainen,\index{authorindex}{Makelainen, T.} Schmidt,
    \index{authorindex}{Schmidt, K.} and Styan
    \index{authorindex}{Styan, G. P. H.}
    (1981), Silvapulle\index{authorindex}{Silvapulle, M. J.} (1981), 
    Wedderburn\index{authorindex}{Wedderburn, R. W. M.} 
    (1976), and Fahrmeir\index{authorindex}{Fahrmeir, L.} and 
    Tutz\index{authorindex}{Tutz, G.} (2001, p.43) discuss 
    the properties for generalized linear models.}

The log of $L(\EP)$ is maximized at the same point as the function itself, allowing us to use
the easier form, taking the derivative with respect to $\mathbf{b}$, and solving for zero:
\begin{align}
    \log L(\EP) &= -\frac{n}{2}\log(2\pi)-\frac{n}{2}\log(2\sigma^2) -\frac{1}{2\sigma^2}
                (\mathbf{y}-\mathbf{Xb})'(\mathbf{y}-\mathbf{Xb})
                \nonumber \\
    \frac{\partial}{\partial \mathbf{b}}\log L(\EP) &= \frac{1}{\sigma^2}\X'(\mathbf{y}-\mathbf{Xb}) \equiv 0
                \nonumber \\
    \hat{\mathbf{b}} &= (\X'\X)^{-1}\X'\mathbf{y}.
                \nonumber
\end{align}
This demonstrates that the least squares estimator is identical to the maximum likelihood estimator 
for the parameter vector $\mathbf{b}$ in linear regression.  In general, most statistical procedures 
do not provide such a nice linkage between disparate theories, and it is a reflection of both the 
elegance and the fundamental nature of the linear model that this is true here.

\section{The Generalized Linear Model}\index{subjectindex}{GLM}
To look at a broader application of maximum likelihood estimation, we now turn to the generalized
linear model construct for specifying nonlinear regression specifications.  This will demonstrate the
utility of maximizing the likelihood function relative to some observed data in a more complex setting.

\subsection{Defining the Link Function}\index{subjectindex}{link function}
Consider the standard linear model meeting the Gauss-Markov conditions:
\begin{equation}
        \underset{(n \times 1)}{{\mathbf V}}
                = \underset{(n \times p)(p \times 1)}{\X\B}
                 + \underset{(n \times 1)}{\EP},
	\qquad\quad
        \underset{(n \times 1)}{E({\mathbf V})}
                = \underset{(n \times 1)}{\T}
                = \underset{(n \times p)(p \times 1)}{\X\B},
                \label{glm.basic}
\end{equation}
where $E({\mathbf V}) = \T$ is the vector of means (the systematic component), and ${\mathbf V}$ is distributed iid
\index{subjectindex}{iid} normal with mean $\T$, and constant variance ${\sigma^2}$.  We use $\mathbf{V}$ instead of $\Y$ 
(as opposed to \eqref{ols.basic}), because $\mathbf{V}$ is not the real outcome variable of interest; it is merely a description of
the linear component on the right-hand-side.  Now generalize this slightly with a function based on the mean of 
the outcome variable, which is no longer required to be normally distributed or even continuous:
\begin{equation}
        \underset{(n \times 1)}{g(\M)} =
        \underset{(n \times 1)}{\T} =
        \underset{(n \times p)(p \times 1)}{\X\B.}
\end{equation}
Here $g()$ is required to be an invertible, \emph{smooth} function of the mean vector $\M$ of the 
$y_i$.\footnote{More specifically, $g()$ must be a one-to-one function that is everywhere 
	differentiable over the support of $\M$.}

Information from the explanatory variables is now expressed in the model only through the link 
from the linear structure, $\X\B$, to the linear predictor, $\T = g(\M)$, controlled by
the form of the link function, $g()$.  This link function connects the linear predictor to the 
\emph{mean} of the outcome variable, not directly to the expression of the outcome variable 
itself, so the outcome variable can now take on a variety of non-normal forms.
The link function connects the stochastic component that describes some response variable from a wide
variety of forms to all the standard normal theory supporting the systematic component through the
mean function:
\begin{align}\label{eq:general.link.function}
                & g(\M) = \T = \X\B \nonumber \\
                & g^{-1}(g(\M)) = g^{-1}(\T) = g^{-1}(\X\B)
                        = \M = E(\mathbf{Y}). \nonumber
\end{align}
So the inverse of the link function ensures that $\X\B$ maintains the Gauss-Markov assumptions
for linear models and all the standard theory applies even though the outcome variable no longer meets
the required assumptions.

The generalization of the linear model now has four components derived from the expressions above.
\begin{enumerate}[I.]
\item   {\bf Stochastic Component:} ${\mathbf Y}$ is the random or stochastic component that
        remains distributed iid\index{subjectindex}{iid} according to a specific parametric family distribution
        with mean $\M$.\index{subjectindex}{GLM!stochastic component}
\item   {\bf Systematic Component:} $\;{\T} = \X\B$ is the systematic
        component with an associated Gauss-Markov normal basis.\index{subjectindex}{GLM!systematic component}
\item   {\bf Link Function:} the stochastic component and the systematic component are linked
        by a function of $\T$, which is \emph{exactly the canonical link function},
        summarized in Table \ref{Link.Table} below.
        We can consider $g(\M)$ as ``tricking'' the linear model into
        thinking that it is still acting upon normally distributed outcome variables.
	\index{subjectindex}{GLM!link function}
\item   {\bf Residuals:}  Although the residuals can be expressed in the same manner as in the
        standard linear model, observed outcome variable value minus predicted outcome variable
        value, a more useful quantity is the deviance residual described in detail below.
	\index{subjectindex}{GLM!residuals}
\end{enumerate}
This setup is much more powerful than it initially appears.  The outcome variable described by
the outcome family form is affected by the explanatory variables strictly through the
link function applied to the systematic component, $g^{-1}(\X\B)$, and nothing
else.  

Table~\ref{Link.Table} summarizes the link functions for common distributions.\footnote{In the
	case of the binomial, only the logit link function qualifies as the canonical link function.
	The other two forms are provided because of their substitutabillity and widespread use.  See 
	also the robit model for robust estimation of dichotomous outcomes (Gelman and Hill 2007, pp.124-5).}
Note that $g()$ and $g^{-1}()$ are both included. \index{subjectindex}{GLM!list of link functions}

\begin{table}[h!]
\parbox[c]{\linewidth}{
    \renewcommand{\arraystretch}{1.5}
    \caption{\textsc{Natural Link Functions for Common Specifications}}\label{Link.Table}
    \vspace{4pt}
    \begin{tabular}{lccc}
                            & Canonical Link                &$\quad$ & Inverse Link \\ \cline{2-4}
    Distribution            & ${\theta}=g({\mu})$           &$\quad$ & ${\mu}=g^{-1}({\theta})$ \\
    \hline
    Poisson                 & $\log({\mu})$                 &$\quad$ & $\exp({\theta})$ \\
    Normal                  & ${\mu}$                       &$\quad$ & ${\theta}$ \\
    Gamma                   & $-\frac{1}{{\mu}}$            &$\quad$ & $-\frac{1}{{\theta}}$ \\
    Negative binomial       & $\log(1-{\mu})$         &$\quad$ & $1-\exp({\theta})$ \\
    Binomial logit          & $\log\left(\frac{{\mu}}{1-{\mu}}\right)$
                                                            &$\quad$ &
                                                            $\frac{\exp({\theta})}{1+\exp({\theta})}$ \\
    Binomial probit         & $\Phi^{-1}\left({\mu}\right)$
                                                            &$\quad$ & $\Phi\left({\theta}\right)$ \\
    Binomial cloglog        & $\log\left(-\log(1-{\mu})\right)$
                                                            &$\quad$ & $1-\exp\left(-\exp({\theta})\right)$ \\
    \end{tabular} 
}
\end{table}\bls
\index{subjectindex}{probit model}

A substantial advantage of the generalized linear model is its freedom from the standard
Gauss-Markov assumption that the residuals have mean zero and constant variance.  Yet this
freedom comes with the price of interpreting more complex stochastic structures.  Currently,
the dominant philosophy is to assess this stochastic element by looking at (summed) discrepancies:
a function that describes the difference between observed and expected outcome data for some
specified model: $D = \sum_{i=1}^{n}d(\T,y_i)$.\index{subjectindex}{GLM!discrepancies}  
This definition is intentionally left
vague for the moment to stress that the format of $D$ is widely applicable.
For instance, if the discrepancy in $D$ is measured as the squared arithmetic difference from
a single mean, then this becomes the standard form for the variance.  In terms of generalized
linear models, the squared difference from the mean will prove to be an overly restrictive
definition of discrepancy, and a likelihood-based measure will be shown to be far more useful.

\subsection{Deviance Residuals}\index{subjectindex}{GLM!deviances}
Starting with the log likelihood for a proposed model from the (\ref{standard.log.like.fn}) notation,
add the `$\mathbf{\hat{\;\;}}$' notation as a reminder that it is evaluated at the maximum likelihood values:
\begin{equation}
        \ell(\hat{\T},\psi|\mathbf{y}) = \sum_{i=1}^{n}\frac{y_i\hat{\T}
        - b(\hat{\T})}{a(\psi)} + c(\mathbf{y},\psi), \nonumber
\end{equation}
where $\psi$ is a scale parameter and the exponential family form is given by its ``canonical'' form:
$f(y|\theta) = \exp\left[\frac{y\theta - b(\theta)}{a(\psi)} + c(y,\psi)\right]$.
When a given PDF or PMF does not have a scale parameter, then $a(\psi) = 1$ and this reduces to the
previous form.  Often we need to explicitly treat these nuisance parameters instead of ignoring them or 
assuming they are known, and the most important case of a two-parameter exponential family is this form
here where the second parameter is a scale parameter.  

Also consider the same log likelihood function with the same data and the same link function,
except that it now has $n$ coefficients
for the $n$ data points, i.e., the saturated model log likelihood function with the `$\tilde{\;\;}$'
function to denote the n-length $\T$ vector:\index{subjectindex}{GLM!saturated model}
\begin{equation}
        \ell(\tilde{\T},\psi|\mathbf{y}) = \sum_{i=1}^{n}\frac{y_i\tilde{\T}
        - b(\tilde{\T})}{a(\psi)} + c(\mathbf{y},\psi). \nonumber
\end{equation}
This is the highest possible value for the log likelihood function achievable with the given
data, $\mathbf{y}$.  Yet it is also often analytically unhelpful except as a benchmark.  The 
deviance function is defined as minus twice the log likelihood ratio (that is, the arithmetic 
difference since both terms are already written on the log metric):
\begin{align}\label{deviance.function}\index{subjectindex}{deviance function}
        D(\T,\mathbf{y}) &= -2\left[ \ell(\hat{\T},\psi|\mathbf{y}) -
                                        \ell(\tilde{\T},\psi|\mathbf{y}) \right] \nonumber \\
                                &= -2\sum_{i=1}^{n}\left[ y_i(\hat{\T} - \tilde{\T}) -
                                                         \left( b(\hat{\T}) - b(\tilde{\T} \right)
                                                  \right]a(\psi)^{-1}.
\end{align}
A utility of the deviance function is that it also allows a look at the individual deviance
contributions in an analogous way to linear model residuals.
The single point deviance function is just the deviance function for the $y_i^{\text{\text{th}}}$ 
point (i.e., without the summation):
\begin{equation}
        d(\T,y_i) = -2\left[ y_i(\hat{\T} - \tilde{\T}) -
		    \left( b(\hat{\T}) - b(\tilde{\T} \right) \right]a(\psi)^{-1}.
        \nonumber
\end{equation}
To define the deviance residual at the $y_i$ point, we take the square root:
\begin{equation}
        R_{Deviance} = \frac{(y_i-\mu_i)}{|y_i-\mu_i|}\sqrt{|d(\T,y_i)|}, \nonumber
\end{equation}
where $\frac{(y_i-\mu_i)}{|y_i-\mu_i|}$ is a sign-preserving function.  For example, the 
natural link function for the binomial is the logit function: $\theta=\log(\mu/(1-\mu))$,
and the log likelihood contribution from a single datum, in exponential family form, is 
given by $\ell(p|y_i,n_i) = y_i\log(p/(1-p)) - (-n_i\log(1-p)) + \log\binom{n_i}{y_i}$.
Substituting in the maximum likelihood result for $p$, $\mu_i = n_i\hat{p}$, and the 
saturated model result for $p$, $\y_i = n_i\tilde{p}$ into \eqref{deviance.function} gives: 
\begin{equation}
	D(p,\y) = -2\sum_{i=1}^{n}\left[ y_i\log\left(\frac{y_i}{\mu_i}\right) 
	+(n_i - y_i)\log\left(\frac{n_i-y_i}{n_i-\mu_i}\right) \right].
\end{equation}

Comparisons are made between: the \emph{null model}, a common mean $\mu$\ for all $y$\ meaning $y = g^{-1}(\mu + \epsilon)$\ (data
is modeled as all random variation).  the \emph{saturated} or \emph{full} model, where the data are explained exactly but no data
reduction or underlying trend information is obtained.  This is typically $n$\ parameters for $n$\ datapoints (data is modeled as
all systematic).  the proposed model where we have partitioned the data into systematic structures \emph{and} a random component
according to some theoretical consideration.  The log-likelihood for the full model versus the research model can be  compared in
ratio terms:
\begin{equation}
			2(\ell(y,\phi|y) - \ell(\hat{\mu},\phi|y)).
\end{equation}
Assuming iid data and $a(\phi_i) = \phi/w_i$, this becomes:
\index{subjectindex}{iid}
\begin{equation}
    \sum_i 2w_i(y_i(\tilde{\theta}_i - \hat{\theta}_i) - b(\tilde{\theta}_i) + b(\hat{\theta}_i))/\phi
\end{equation}
Call $D(y,\hat{\mu})$\ the \emph{deviance}, and the above forms the scaled deviance ($D(y,\hat{\mu})/\phi$).
For common models, these deviances are summarized in Table~\ref{Deviance.Table}.

\begin{table}[h!]
\parbox[c]{\linewidth}{ \hspace{-0.45in}
\renewcommand{\arraystretch}{1.5}
\caption{\textsc{Summed Deviances for Common Specifications}}\label{Deviance.Table}
    \vspace{11pt}
    \begin{tabular}{ll}
			    & \\[-8pt]
    GLM			& Unscaled Deviance, $D(y,\hat{\mu})$\						\\[12pt]
    \hline
			    & \\[-8pt]
    Gaussian		& $\sum_i(y_i - \hat{\mu}_i)^2$\							\\[12pt]
    Poisson			& $2\sum_i\left[ y_i\log(y_i/\hat{\mu}_i) + (y_i - \hat{\mu}_i) \right]$\	\\[12pt]
    Binomial		& $2\sum_i\left[ y_i\log(y_i/\hat{\mu}_i) + (m - \hat{\mu}_i)
			    \log(((m-y_i)/(m-\hat{\mu}_i)) \right]$\				        \\[12pt]
			    & where $m$\ is the sample size so $\mu$\ is the count not the proportion.	\\[12pt]
    Gamma			& $2\sum_i\left[ -\log(y_i/\hat{\mu}_i) + (y_i - \hat{\mu}_i)/\hat{\mu}_i \right]$\\[12pt]
    Negative Binomial	& $\sum_i (y_i - \hat{\mu}_i)^2/(\hat{\mu}_i^2y_i)$
    \end{tabular}
\vspace{11pt}
}
\end{table}

In standard practice, the two main tools: the deviances, as described, and Pearson's statistic:
\begin{equation}
                	X^2 = \sum_i \mathbf{R}_{Pearson}^2 =
                    	      \sum_i \frac{ (\y_i - \hat{\mu}_i)^2 }{ var(\hat{\mu}_i) }
\end{equation}
which lead to asymptotic $\chi^2$\ tests with the degrees of freedom equal to the difference in the number of parameters.
Paradigm: We compare two \emph{nested} models where the more parsimonious model is one that puts linear restrictions (usually
$\beta_j = 0$) on the parameters.  Intuition: an unrestricted model versus a restricted model.  The goodness of fit test to the
data is just a nested test where the nesting model is the 	saturated model.  Two caveats: if $\phi \ne 1$, more elaborate testing
required (estimates of $\phi$), and sample sizes need to be large for less-granular responses (huge for dichotomous).

For the deviance comparison we are comparing a large (possibly saturated) model $\Omega$\ to a restricted research model of
interest $\omega$.  The difference in the scaled deviances, $D_\omega - D_\Omega$\ is asymptotically $\chi^2$\ with $df$\ the
number of restrictions.  The restricted model will have larger deviances because we are making theoretical statements away from
just trending through the data.  General test: Model 1 has $p$\ parameters and model 2 has $q>p$\ parameters.  Then
\begin{equation}
			D_p - D_q \;\;\underset{\sim}{{\tiny{asym}}}\;\; \chi^2_{df=q-p}.  
\end{equation}
If this difference is ``small'' then the restrictions make sense.  If this difference is ``large'' then they take us far from what
the data want to say.  Our claim is that the $p-q$\ parameters all have coefficients equal to zero (this is the restriction).
The number of parameters in each model determine the corresponding degrees of freedom: the saturated model has $n$\ parameters, the
research model with $p$\ parameters (counting the intercept), and the null model has $1$\ parameter to account for the mean.	To
test the research model versus null model: the research model gives $D_\Omega$, the null model gives $D_\omega$, and the degrees
of freedom are $\#params(\Omega) - \#params(\omega) = p-1$.  So large values of $D_\omega - D_\Omega$\ support the saturated
model.  In this case we want to be in the tail of the associated chi-square test to distinguish the research model from the simple
null model.  To test the saturated model versus research model: the saturated model gives $D_\Omega$, the research model gives
$D_\omega$, and the degrees of freedom are $\#params(\Omega) - \#params(\omega) = n-p$\ So small values of $D_\omega - D_\Omega$\
support the research model by indicating that it is statistically close to the saturated model.  Note that failing this test does
not discredit any specification as it merely means more information may be available to use.

\begin{examplelist}
    \item   {\bf Poisson Model of Military Coups.}\label{africa.poisson.glm}\index{subjectindex}{example!military coups in Africa}
Sub-Saharan Africa has experienced a disproportionately high proportion of regime changes due to the military takeover of
government for a variety of reasons, including ethnic fragmentation, arbitrary borders, economic problems, outside intervention,
and poorly developed governmental institutions.  These data, selected from a larger set collected by Bratton and Van De Walle (1994),
\index{authorindex}{Bratton, M.} \index{authorindex}{Van De Walle, N.} look at potential causal factors for counts of
military coups (ranging from 0 to 6 events) in 33 sub-Saharan countries over the period from each country's colonial independence
to 1989.  Seven explanatory variables are chosen here to model the count of military coups: {\texttt Military Oligarchy} (the number
of years of this type of rule); {\texttt Political Liberalization} (0 for no observable civil rights for political expression, 1 for
limited, and 2 for extensive); {\texttt Parties} (number of legally registered political parties); {\texttt Percent Legislative Voting};
{\texttt Percent Registered Voting}; {\texttt Size} (in one thousand square kilometer units); and {\texttt Population} (given in millions).

A generalized linear model for these data with the Poisson link function is specified as:
\begin{equation*}
    g^{-1}(\T) = g^{-1}(\X\B)=\exp\left[\X\B\right]=E[\mathbf{Y}]=E[\text{\bf Military Coups}].
\end{equation*}
In this specification, the systematic component is $\X\B$, the stochastic component is $\mathbf{Y}=\text{\bf Military
Coups}$, and the link function is $\T=\log(\M)$.  We can re-express this model by moving the link function to the
left-hand side exposing the linear predictor: $g(\M) = \log(E[\mathbf{Y}]) = \X\B$\ (although this is now a less intuitive
form for understanding the outcome variable).  The \texttt{R} language GLM call for this specification is: 
\begin{R.Code}
data(africa)
africa.out <- glm(MILTCOUP ~ MILITARY + POLLIB + PARTY93 + PCTVOTE 
                + PCTTURN + SIZE*POP + NUMREGIM*NUMELEC,
                family=poisson, data=africa)
summary(africa.out)
\end{R.Code}
This gives the model results in Table~\ref{africa.model.table}.\\[11pt]

\begin{table}[h]
\parbox[c]{\linewidth}{  \hspace{0.22in}
    \tabletitle{\textsc{Poisson Model of Military Coups}}\label{africa.model.table}
    \begin{tabular}{lrrc}
    & \multicolumn{1}{c}{Coefficient} & \multicolumn{1}{c}{Std. Error} & \multicolumn{1}{c}{95\% CI} \\
    \hline                      
                                &         &         &                   \\[-9pt]
    \texttt{Intercept}                   &  2.9209 &  1.3368 & [~0.3008:~5.5410] \\
    \texttt{Military Oligarchy}          &  0.1709 &  0.0509 & [~0.0711:~0.2706] \\
    \texttt{Political Liberalization}    & -0.4654 &  0.3319 & [-1.1160:~0.1851] \\
    \texttt{Parties}                     &  0.0248 &  0.0109 & [~0.0035:~0.0460] \\
    \texttt{Percent Legislative Voting}  &  0.0613 &  0.0218 & [~0.0187:~0.1040] \\
    \texttt{Percent Registered Voting}   & -0.0361 &  0.0137 & [-0.0629:-0.0093] \\
    \texttt{Size}                        & -0.0018 &  0.0007 & [-0.0033:-0.0004] \\
    \texttt{Population}                  & -0.1188 &  0.0397 & [-0.1965:-0.0411] \\
    \texttt{Regimes}                     & -0.8662 &  0.4571 & [-1.7621:~0.0298] \\
    \texttt{Elections}                   & -0.4859 &  0.2118 & [-0.9010:-0.0709] \\
    \texttt{(Size)(Population)}          &  0.0001 &  0.0001 & [~0.0001:~0.0002] \\
    \texttt{(Regimes)(Elections)}        &  0.1810 &  0.0689 & [~0.0459:~0.3161] \\
    \end{tabular} 
}
\vspace{11pt}
\end{table}

Note that the two interaction terms are specified by using the multiplication character.  The iteratively weighted least squares
algorithm converged in only four iterations using Fisher scoring, and the results are provided in the table.  The model appears to
fit the data quite well: an improvement from the null deviance of 62 on 46 degrees of freedom to a residual deviance of 7.5 on 35
degrees of freedom (evidence that the model does not fit would be supplied by a deviance value in the tail of a
$\chi^2_{n-k}$\ distribution).  Nearly all the coefficients have 95\% confidence intervals bounded away from zero and
therefore appear reliable in the model.  Since this is a Poisson link function, we might also want to check for overdispersion:
\begin{R.Code}
sum(residuals(africa.out,type="pearson")^2)
[1] 6.823595
1-pchisq(sum(residuals(africa.out,type="pearson")^2), 
    df=africa.out$df.residual, lower.tail=FALSE)
[1] 0.9984635
\end{R.Code}
This gives no evidence that we have extra-Poisson variance.
\end{examplelist}

\section{Numerical Maximum Likelihood}\index{subjectindex}{likelihood!numerical estimation}
Generally the maximum likelihood estimate for the unknown parameter vector in a generalized 
linear model does not have a closed-form analytical solution.  So this section explains the 
method for applying maximum likelihood estimation to generalized linear models using numerical 
techniques.  The primary iterative root-finding procedure is \emph{iteratively weighted least 
squares} (IWLS),\index{subjectindex}{iteratively weighted least squares (IWLS)} created by 
Nelder\index{authorindex}{Nelder, J. A.} and 
Wedderburn\index{authorindex}{Wedderburn, R. W. M.} (1972).  In order to 
fully understand the numerical aspects of this technique, this section first discusses 
finding coefficient estimates in nonlinear models (i.e., simple root finding), followed by 
weighted regression, and finally the iterating algorithm.  This section can be skipped on 
first reading as it addresses the lower level computational aspects of statistical software only.

\subsection{Newton-Raphson and Root Finding}
The problem of finding the best possible estimate for some coefficient value typically reduces to 
the problem of finding the mode of the likelihood function, given some observed data and a parametric 
specification.  In textbook examples, this is the challenge of finding the point where the first 
derivative of the likelihood function is equal to zero (including the secondary consideration that 
the second derivative be negative to ensure that the point is not a minima).  However, in realistic 
work, the problem is that of finding a high-dimensional mode with likelihood functions that are 
considerably less simple and even have suboptimal modes.

Rather than using analytical solutions we are driven to using numerical techniques, which are 
algorithms that manipulate the data and the specified model to produce a mathematical solution 
for the modal point.  Because the solutions are produced by iterating/searching procedures using 
the data, they are considerably more messy with regard to machine-generated round-off and 
truncation in intermediate steps of the applied algorithm. 

Consider the problem of numerical maximum likelihood estimation as that of finding the top of an 
``ant hill'' in the parameter space.  It is easy then to see that this is equivalent to finding 
the parameter vector value where the derivative of the likelihood function is equal to zero: 
where the tangent line is horizontal.  The most widely used procedure is called Newton-Raphson, 
based on Newton's method for finding the roots of polynomial equations.

\index{subjectindex}{Newton-Raphson algorithm}
\index{subjectindex}{Taylor series expansion} 
Newton's method exploits the properties of a Taylor series expansion around some given point.  The Taylor 
series expansion gives the relationship between the value of a mathematical function at point, $x_0$, and 
the function value at another point, $x_1$, given as: 
\begin{align}
   f(x_1) = f(x_0) + (x_1 - x_0)f'(x_0) &+ \frac{1}{2!}(x_1 - x_0)^2f''(x_0) 	        \nonumber \\
                                        &+ \frac{1}{3!}(x_1 - x_0)^3f'''(x_0) + \ldots, \nonumber
\end{align}
where $f'$ is the first derivative with respect to $x$, $f''$ is the second derivative with 
respect to $x$, and so on.  Note that it is required that $f()$ have continuous derivatives over 
the relevant support of the function here: the allowable parameter space of $x$.  Otherwise the 
algorithm fails.

Infinite precision is achieved with the infinite extending of the series 
into higher order derivatives and higher order polynomials (of course the factorial component in 
the denominator means that these are rapidly decreasing increments).  This process is both 
unobtainable and unnecessary, and only the first two terms are required as a step in an iterative 
process.  

\index{subjectindex}{root finding}
The point of interest is $x_1$ such that $f(x_1) = 0$.  This value is a root of the function, 
$f()$ in that it provides a solution to the polynomial expressed by the function.  It is also 
the point where the function crosses the x-axis in a graph of $x$ versus $f(x)$.  This point 
could be found in one step with an infinite Taylor series: $0 = f(x_0) + (x_1 - x_0)f'(x_0) + 
\frac{1}{2!}(x_1 - x_0)^2f''(x_0) + \ldots + \frac{1}{\infty!}(x_1 - x_0)^\infty f^{(\infty)}(x_0) 
+ \ldots$ .  While this is impossible, it is true that we could use just the first two terms 
to get closer to the desired point: $0\cong f(x_0)+(x_1-x_0)f'(x_0)$.  Now rearrange to produce 
at the $(j+1)^{\text{th}}$ step: $x^{(j+1)} = x^{(j)} - \frac{f(x^{(j)})}{f'(x^{(j)})}$,
so that progressively improved estimates are produced until $f(x^{(j+1)})$ is sufficiently 
close to zero.  It has been shown that this method converges quadratically to a solution 
provided that the selected starting point is reasonably close to the solution, although the 
results can be very bad if this condition is not met.

\begin{examplelist}
	\item	{\bf A Simple Application of Newton-Raphson.}\label{Simple.NR.Example}
\index{subjectindex}{example!Newton-Raphson for quadratic function}
Suppose that we wanted a numerical routine for finding the square root of a number, $\mu$.  This 
is equivalent to finding the root of the simple equation $f(x) = x^2 - \mu = 0$.  The first
derivative is just $\frac{d}{d x}f(x) = 2x$.  If we insert these functions into
$(j+1)^{\text{th}}$ step: $x^{(j+1)} = x^{(j)} - \frac{f(x^{(j)})}{f'(x^{(j)})}$, we get:
\begin{equation}\label{newton.example}\index{subjectindex}{R@\R!Newton-Raphson example}
	 x^{(j+1)} = x^{(j)} - \frac{ (x^{(j)})^2 - \mu }{ 2x^{(j)} } 
		   = \frac{1}{2} ( x^{(j)} + \mu (x^{(j)})^{-1} ).
\end{equation}
A very basic \R\ function for implementing \eqref{newton.example} is:\\[-4pt]
\begin{R.Code} 
newton.raphson.ex <- function(mu,x,iterations)  \{
    for (i in 1:iterations)  
        x <- 0.5*(x + mu/x)
    return(x) 
\}       
\end{R.Code}
This function is run with the command \texttt{newton.raphson.ex(99,2,3)} to find the square root
of 99 starting at 2, but only taking 3 steps.  The result is the (incorrect) value 10.74386.  
However, running the function according to \texttt{newton.raphson.ex(99,2,6)}, that is six 
iterations instead of three, gives the correct value of 9.949874.
\end{examplelist}

\subsubsection{Newton-Raphson for Statistical Problems}\index{subjectindex}{Newton-Raphson algorithm!applied to score function}
The Newton-Raphson algorithm, when applied to mode finding in a statistical setting, substitutes 
$\theta^{(j+1)}$ for $x^{(j+1)}$ and $\theta^{(j)}$ for $x^{(j)}$ (where the $\theta$ values are
iterative estimates of the parameter vector)  and $f()$ is the score function \eqref{ll.score.fn}.
The real goal is to estimate a $k$-dimensional $\hat{\T}$ estimate, given data and a model.  
The applicable multivariate likelihood updating equation is now provided by:
\begin{equation}\label{newton4}
        \T^{(j+1)} = \T^{(j)} -
        \frac{ \partial }{ \partial\T }\ell(\T^{(j)}|\x)
        \left( \frac{ \partial^2 }{ \partial\T \partial\T' }\ell(\T^{(j)}|\x) \right)^{-1}.
\end{equation}
When $\T^{(j)}$ is the maximum likelihood coefficient vector, the quantity in the denominator is a matrix
called the \emph{Hessian}.\index{subjectindex}{Hessian}  For exponential family distributions and 
natural link functions, the observed and expected Hessian are identical 
(Fahrmeir\index{authorindex}{Fahrmeir, L.} and Tutz\index{authorindex}{Tutz, G.} 2001, p.42; 
Lehmann\index{authorindex}{Lehmann, E. L.} and Casella,\index{authorindex}{Casella, G.} 1998, pp.124-8), although 
Efron\index{authorindex}{Efron, B.} and 
Hinkley\index{authorindex}{Hinkley, D. V.} (1978) give situations where the observed information is preferable.  
So it is common to replace this calculation with forms that are equivalent for the exponential 
family, such as the Fisher Information:\index{subjectindex}{Fisher Information}
\begin{equation}
      	\mathbf{A}_F = -E\left( \frac{ \partial^2 }{ \partial\T\partial\T' } \ell(\T^{(j)}|\x) \right)
                \nonumber
\end{equation}
(Fisher\index{authorindex}{Fisher, R. A.} 1925), or the square of the score function:
\begin{equation}
	\mathbf{A}_B = E\left[ \frac{ \partial }{ \partial\T }\ell(\T^{(j)}|\x)'
                          \frac{ \partial }{ \partial\T }\ell(\T^{(j)}|\x) \right].
       		\nonumber
\end{equation}
which is sometimes called the \emph{BHHH} method (Berndt,\index{authorindex}{Berndt, E.} 
Hall,\index{authorindex}{Hall, B.} 
Hall,\index{authorindex}{Hall, R.} and 
Hausman\index{authorindex}{Hausman, J.} 1974).

At each step of these Newton-Raphson steps, a system of equations determined by the multivariate
normal equations must be solved:
\begin{equation}\label{normal.1}
               \underbrace{ \mathbf{A} }_{ \text{angle} }
                \underbrace{ \; (\T^{(j+1)} - \T^{(j)}) \; }_{ \text{direction: $\delta\bb$} }
                = \underbrace{ \; \frac{\partial}{\partial\T}
                        \ell(\T^{(j)}|\x) \; }_{ \text{size of direction: $\mathbf{u}$} },
\end{equation}
which builds a linear structure in the parameter vector, and leads to estimates from
the system of linear equations: ${\delta\bb} = {\mathbf{A}^{-1}}{\mathbf{u}}$. 
It is computationally convenient to solve on each iteration by least squares, so that the problem of 
mode finding reduces to a repeated weighted least squares application in which the inverse of the 
diagonal values of the second derivative matrix in the denominator are the appropriate weights (this
is a diagonal matrix by the iid assumption).  
\index{subjectindex}{iid}

\subsubsection{Weighted Least Squares}\index{subjectindex}{weighted (generalized) least squares} A standard 
compensating technique for nonconstant error variance (so-called heteroscedasticity)\index{subjectindex}{heteroscedasticity} 
is to insert a diagonal matrix of weights, $\mathbf{\Omega}$, into the calculation of $\hat{\B}$ such 
that this heteroscedasticity is removed by design.  The $\mathbf{\Omega}$ matrix is created by 
taking the error variance of the $i^{\text{th}}$ case (estimated or known), $\nu_i$, and assigning
it to the $i^{\text{th}}$ diagonal $\mathbf{\Omega}_{ii} = \frac{1}{\nu_i}$, leaving the 
off-diagonal elements as zero.  So large error variances are reduced by premultiplying the model 
terms by this reciprocal.

We can premultiply each term in the standard linear model setup, by the square root of the 
$\mathbf{\Omega}$ matrix (that is, by the standard deviation).  This ``square root'' is actually 
produced from a Cholesky factorization:\index{subjectindex}{Cholesky factorization} if $\mathbf{A}$ is a 
positive definite symmetric ($\mathbf{A}' = \mathbf{A}$) matrix, then there must exist a matrix
$\mathbf{G}$ such that: $\mathbf{A} = \mathbf{GG}'$.  A matrix, $\mathbf{A}$, is positive definite
\index{subjectindex}{positive definite}
if for any nonzero $p\times 1$ vector $\textbf{x}$, $\textbf{x}'\textbf{A}\textbf{x}>0$.
In our case, this decomposition is greatly simplified because the $\mathbf{\Omega}$ matrix has only
diagonal values (all off-diagonal values equal to zero).  Therefore the Cholesky factorization
is produced simply from the square root of these diagonal values.  Premultiplying gives:
\begin{equation}\label{ols.weighted}
        \mathbf{\Omega}^{\frac{1}{2}}\mathbf{Y} = \mathbf{\Omega}^{\frac{1}{2}}\X\B
                + \mathbf{\Omega}^{\frac{1}{2}}\EP.
\end{equation}
Instead of minimizing squared errors in the usual manner, we now minimize 
$(\mathbf{Y} - \X\B)'\mathbf{\Omega}(\mathbf{Y} - \X\B)$, and the subsequent weighted least squares 
estimator is found by $\hat{\B} = (\X'\mathbf{\Omega}\X)^{-1}\X'\mathbf{\Omega}\mathbf{Y}$.  
The weighted least squares estimator gives the best linear unbiased estimate (BLUE) of the 
coefficient estimator in the presence of heteroscedasticity.
\index{subjectindex}{heteroscedasticity}
\index{subjectindex}{bias!from heteroscedasticity}  

\subsubsection{Iterative Weighted Least Squares}\index{subjectindex}{iteratively weighted least squares (IWLS)}
It is more common that the individual variances used to make the reciprocal diagonal values for 
$\mathbf{\Omega}$ are unknown, and cannot be easily estimated, but are known to be a function of the 
mean of the outcome variable: $\nu_i = f(E[Y_i])$.  So if the expected value of the outcome variable, 
$E[Y_i] = \mu$ and the form of the relation function, $f()$ are known, then this is a very 
straightforward estimation process.  Unfortunately, although it is common for the variance structure 
to be dependent on the mean function, it is relatively rare to know the exact form of the dependence.

Nelder\index{authorindex}{Nelder, J. A.} and 
Wedderburn\index{authorindex}{Wedderburn, R. W. M.} (1972) provide a solution 
to this problem that iteratively re-estimates the weights, improving the estimate on each cycle 
using the mean function.  Because $\M = g^{-1}(\X\B)$, then the coefficient estimate, $\hat{\B}$, 
provides a mean estimate and vice versa.  So the algorithm iteratively estimates these values 
using progressively better weights.  This proceeds as follows.  First assign starting values to 
the weights, generally equal to one: $\frac{1}{\nu_1^{(1)}} = 1$, and specify the diagonal 
matrix $\mathbf{\Omega}$, guarding against division by zero.  Then iterate the following steps:
\begin{bayeslist}\index{subjectindex}{iteratively weighted least squares (IWLS)!steps}
\item   Define the current (or starting) point of the linear predictor by:
        \begin{equation}
                \underset{(n \times 1)}{\hat{\E}_0} =
                \underset{(n \times p)(p \times 1)}{\X\B_0}
                \nonumber
        \end{equation}
        with fitted value $\hat{\M}_0$ from $g^{-1}(\hat{\E}_0)$.

\item   Form the ``adjusted dependent variable'' according to:
        \begin{equation}
                \underset{(n \times 1)}{z_0} =
                \underset{(n \times 1)}{\hat{\E}_0} +
                \underset{diag(n \times n)}{\left( \frac{\partial\eta}{\partial\mu}\bigg|_{\hat{\M}_0} \right)}
                \underset{(n \times 1)}{(\y - \hat{\M}_0)}
                \nonumber
        \end{equation}
        which is a linearized form of the link function applied to the data.  As an example of this
        derivative function, the Poisson form looks like $\eta = \log(\mu) \; \Longrightarrow \; 
	\frac{\partial\eta}{\partial\mu} = \frac{1}{\mu}$.

\item   Form the \emph{quadratic weight matrix}, which is the variance of $z$:
        \begin{equation}
                \underset{(n \times n)}{w_0^{-1}} =
                           \left( \frac{\partial\eta}{\partial\mu}\bigg|_{\hat{\M}_0} \right)^2
                           v(\mu)|_{\hat{\M}_0}
                           \nonumber
        \end{equation}
        where $v(\mu)$ is the variance function: $\frac{\partial}{\partial\theta}b'(\theta) = b''(\theta)$.

\item   So the general iteration scheme is:
        \begin{enumerate}
                \item   Construct $z$, $w$.  Regress $z$ on the covariates with weights to get a new
                        interim estimate:
                        \begin{equation}
                                \underset{(p \times 1)}{\hat{\B}_1} =
                                (\underset{(p \times n)}{\X'}
                                \underset{(n \times n)}{w_0}
                                \underset{(n \times p)}{\X})^{-1}
                                \underset{(p \times n)}{\X'}
                                \underset{(n \times n)}{w_0}
                                \underset{(n \times 1)}{z_0}
                                \nonumber
                        \end{equation}

                \item   Use the coefficient vector estimate to update the linear predictor:
                        \begin{equation}
                                \hat{\E}_1 = \X'\hat{\B}_1              \nonumber
                        \end{equation}

                \item   Iterate:
                        $z_1, w_1 \; \Rightarrow \; \hat{\B}_2, \hat{\E}_2$, 
                        $z_2, w_2 \; \Rightarrow \; \hat{\B}_3, \hat{\E}_3$,
                        $z_3, w_3 \; \Rightarrow \; \hat{\B}_4, \hat{\E}_4,\ldots\;$.

        \end{enumerate}
\end{bayeslist}
These steps are repeated until convergence (i.e., $\X\hat{\B}_{j} - \X\hat{\B}_{j+1}$ is close 
to zero).  Under very general conditions, satisfied by the exponential family of distributions,
the IWLS procedure finds the mode of the likelihood function, thus producing the maximum 
likelihood estimate of the unknown coefficient vector, $\hat{\B}$, and the variance matrix, 
$\hat{\sigma}^2(\X'\mathbf{\Omega}\X)^{-1}$ (Gill\index{authorindex}{Gill, J.} 2000, 
Green\index{authorindex}{Green, P. J.}
1984, Del Pino\index{authorindex}{Del Pino, G.} 1989).

The purpose of this section is to illustrate the means by which generalized linear models are 
estimated in practice.  This process is the core methodological practice in non-Bayesian 
statistical analysis in the social and behavioral sciences, even though many practitioners 
believe they are using some particularistic model (logit, Poisson, duration, etc.).  It should 
not be inferred from this discussion that this process always works without computational 
difficulties.  For extensive discussions of maximum likelihood computational issues and what 
can go wrong, see Altman\index{authorindex}{Altman, M.} and 
McDonald\index{authorindex}{McDonald, M. P.} (2001), 
McCullough\index{authorindex}{McCullough, B. D.} (1998, 1999), and 
Gill\index{authorindex}{Gill, P.}, 
Murray\index{authorindex}{Murray, W.}, and 
Wright\index{authorindex}{Wright, M. H.} (1981, Chapter~8).

\section{Quasi-Likelihood}\index{subjectindex}{quasi-likelihood!estimation}
Wedderburn\index{authorindex}{Wedderburn, R. W. M.} (1974) introduced the concept of ``quasi-likelihood'' 
estimation to address circumstances when either the parametric form of the likelihood is 
known to be misspecified, or only the first two moments\footnote{Moments are characterizations 
	of distributions based on expectations.  The $n^{th}$ moment of a random variable $X$ 
	is given by: $\mu_n=E[X^n]$.  So the mean of $X$ is the first moment: $E[X] = \mu_1$, 
	and the variance of $X$ is the second moment minus the square of the first moment: 
	$\Var[X] = \mu_2 - (\mu_1)^2$. See Stuart\index{authorindex}{Stuart, A.} and 
        Ord\index{authorindex}{Ord, J. K.} 
	(1994, Chapter 3) for a lengthy discussion of moments and related quantities.}
are definable.  Albert\index{authorindex}{Albert, J. H.} (1988) and 
Albert\index{authorindex}{Albert, J. H.} and 
Pepple\index{authorindex}{Pepple, P. A.} (1989) detail the utility of applying quasi-likelihood 
models to hierarchical Bayesian estimation problems where the posterior calculation is 
difficult.  This is before the revolution brought on by the widespread use of MCMC techniques, 
but quasi-likelihood remains useful as a way of relaxing distributional assumptions in model 
specifications.  General applications of quasi-likelihood specifications in the social 
sciences include: White's\index{authorindex}{White, H.} (1982) look at econometric model misspecification, 
Goldstein\index{authorindex}{Goldstein, H.} and 
Rasbash's\index{authorindex}{Rasbash, J.} (1996) application to public 
policy,  the use of quasi-likelihood in analyzing multinational corporate decision-making by 
Hannan\index{authorindex}{Hannan, M. T.} \etal (1995), 
Western's\index{authorindex}{Western, B.} (1995) application to 
studying union decline, Sampson\index{authorindex}{Sampson, R. J.} and 
Raudenbush's\index{authorindex}{Raudenbush, S. W.}
(1999) study of behavior in public spaces, and Mebane's\index{authorindex}{Mebane, W. R., Jr.} (1994) 
evaluation of the linkage between taxation policy and subsequent elections.

Suppose that we know something about the parametric form of the distribution generating the 
data, but not in complete detail.  Obviously this precludes the standard maximum likelihood 
estimation of unknown parameters since we cannot specify a full likelihood equation.  
Wedderburn's\index{authorindex}{Wedderburn, R. W. M.} idea was to develop an estimation procedure that only
requires specification for the mean function of the data and a stipulated relationship between
this mean function and the variance function.  This is useful in a Bayesian context when we have
prior information readily at hand but only a vague idea of the form of the likelihood.

Re-express the exponential family form as a joint distribution function of observed data:
\begin{equation}
    f(\mathbf{y}|\theta)=\exp\left[\sum_{i=1}^n y_i\theta - nb(\theta)+\sum_{i=1}^n c(y_i)\right],
\end{equation}
and with the more realistic assumption of a multiparameter model with $k$ parameters:
$f(y|\T) = \exp\left[ \sum_{i=1}^{n}\sum_{j=1}^{k}y\theta_j-nb(\theta_j)
    + \sum_{i=1}^{n}c(y) \right]$.  Adopting the canonical form with a scale parameter 
gives: $f(y|\theta) = \exp\left[\frac{y\theta - b(\theta)}{a(\psi)} + c(y,\psi)\right]$.

We can also define a variance \emph{function} for a given exponential family expression, which is
used in generalized linear models to indicate the dependence of the variance of $Y$ on location 
and scale parameters:  $v(\mu) = \frac{\partial^2}{\partial\theta^2}b(\theta)$,
meaning that $\Var[Y] = a(\psi)v(\mu)$ indexed by $\theta$.  Note that the dependence on
$b(\theta)$ explicitly states that the variance function is conditional on the mean function,
whereas there was no such stipulation with the $a(\psi)$ form.
It is conventional to leave the variance function in terms of the canonical parameter, $\theta$, rather
than return it to the parameterization in the original probability function as was done for the variance
of $Y$.  Table~\ref{Varfun.Table} summarizes some common variance functions.
\index{subjectindex}{GLM!normalizing constants}\index{subjectindex}{GLM!variance functions}

\begin{table}[h!]
\parbox[c]{\linewidth}{
    \blstable
    \caption{\textsc{Normalizing Constants and Variance Functions}}\label{Varfun.Table}
    \vspace{8pt}
    \begin{tabular}{lccc}
		    & & & \\[-6pt]
    Distribution      & $b(\theta)$
                    & $v(\mu)=\frac{\partial^2}{\partial\theta^2}b(\theta)$ \\[4pt]
    \hline
    Poisson           & $\exp(\theta)$
                    & $\exp(\theta)$ \\
    Binomial          & $n\log\left(1+\exp(\theta)\right)$
                    & $n\exp(\theta)(1+\exp(\theta))^{-2}$\\
    Normal            & $\frac{\theta^2}{2}$
                    & $1$ \\
    Gamma             & $-\log(-\theta)$
                    & $\frac{1}{\theta^2}$ \\
    Negative binomial & $r\log(1-\exp(\theta))$
                    & $r\exp(\theta)(1-\exp(\theta))^{-2}$ \\
    \end{tabular} 
}
\end{table}
\bls
\suppressfloats

The log likelihood function in this context is now written as:
\begin{equation}
    \ell(\T)=\sum_{i=1}^{n}\sum_{j=1}^{k}\frac{y\theta_j-nb(\theta_j)}{a(\psi)} +\sum_{i=1}^{n}c(y,\psi), 
\end{equation}
where $E[\mathbf{y}]=\frac{\partial b(\theta_j)}{\partial\T}$.
The expected value of the first derivative of the joint distribution is equal to zero since this
is the slope of the tangent line at the mode:
\begin{equation}
    E\left[\frac{\partial\ell(\T)}{\partial\T}\right]
    = E\left[\sum_{i=1}^{n}\frac{y}{a(\psi)}
    - \frac{\partial nb(\theta_j)}{\partial\T}\frac{1}{a(\psi)}\right]
    = 0,    \nonumber
\end{equation}
and
\begin{equation}
    \Var\left[\frac{\partial\ell(\T)}{\partial\T}\right]
    = -E\left[\frac{\partial^2\ell(\T)}{\partial\T^2}\right]
    = -E\left[-\frac{\partial^2 nb(\theta_j)}{\partial\T^2}\frac{1}{a(\psi)}\right]
    = \frac{\partial^2 nb(\theta_j)}{\partial\T^2}\frac{1}{a(\psi)}.
    \nonumber
\end{equation}
So again $\Var[Y] = a(\psi)v(\mu)$, where $v(\mu) = \frac{\partial^2 nb(\theta_j)}{\partial\T^2}$
is the variance function.

Instead of taking the first derivative of log likelihood with respect to the parameter vector,
$\T$, suppose we take this derivative with respect to the mean function in a
generalized linear model, $\M$, with the analogous property that:
$E\left[ \frac{\partial\ell(\T)}{\partial\M} \right] = 0$.  Thus
\begin{equation}
    \Var\left[\frac{\partial\ell(\T)}{\partial\M}\right]
        = -E\left[\frac{\partial^2\ell(\T)}{\partial\M^2}\right]
    = \frac{1}{a(\psi)v(\mu)}.
    \nonumber
\end{equation}
Therefore what we have here is a linkage between the mean function and the variance function that
does not depend on the form of the likelihood function.  If we combine this with a form of
$\frac{\partial\ell(\T)}{\partial\M}$ that satisfies the condition above that
the expected value of its first derivative is zero, and the variance property above holds, then
we have a replacement for the unknown specific form of the likelihood function that still
provides the desired properties of maximum likelihood estimation as described.  Thus we imitate
these three criteria of the score function with a function that contains significantly less 
parametric information: only the mean and variance.

\index{subjectindex}{quasi-likelihood!substitution function}
A substitution function that satisfies all of these conditions is:
$\mathbf{q} = \frac{\mathbf{y}-\M}{a(\psi)v(\mu)}$ (McCullagh\index{authorindex}{McCullagh, P.} and 
Nelder\index{authorindex}{Nelder, J. A.} 1989, 325; 
McCulloch\index{authorindex}{McCulloch, C. E.} and 
Searle\index{authorindex}{Searle, S. R.} 2001, 152; 
Shao\index{authorindex}{Shao, J.} 2005).
The contribution to the log likelihood function from the $i^{th}$ point is defined by:
\begin{equation}
    Q_i = \int_{y_i}^{\mu_i}\frac{y_i - t}{a(\psi)v(\mu)}dt,
    \nonumber
\end{equation}
so finding the maximum likelihood estimator for this setup, $\hat{\T}$ is equivalent
to solving:
\begin{equation}
    \frac{\partial}{\partial\theta}\sum_{i=1}^{n}Q_i
    = \sum_{i=1}^{n}\frac{y_i-\mu_i}{a(\psi)v(\mu)}\frac{\partial\mu_i}{\partial\theta}
    = \sum_{i=1}^{n}\frac{y_i-\mu_i}{a(\psi)v(\mu)}\frac{\mathbf{x}_i}{g(\mu)}
    = \mathbf{0},   \nonumber
\end{equation}
where $g(\mu)$ is the canonical link function for a generalized linear model specification.  In other
words we can use the usual maximum likelihood engine for inference with complete asymptotic properties
such as consistency and normality (McCullagh\index{authorindex}{McCullagh, P.} 1983), by only specifying the 
relationship between the mean and variance functions as well as the
link function (which actually comes directly from the form of the outcome variable data).

As an example, suppose we assume that the mean and variance function are related by stipulating
that $a(\psi) = \sigma^2 = 1$, and $b(\theta)=\frac{\theta^2}{2}$, so
$v(\mu) = \frac{d^2 b(\theta)}{d\theta^2} = 1$.
Then it follows that:
\begin{equation}
        Q_i = \int_{y_i}^{\mu_i}\frac{y_i - t}{a(\psi)v(\mu)}dt = -\frac{(y_i - \mu_i)^2}{2}.
        \nonumber
\end{equation}
The quasi-likelihood solution for $\hat{\T}$ comes from solving the quasi-likelihood
equation:
\begin{equation}
     \frac{d}{d\theta}\sum_{i=1}^{n}Q_i
        = \frac{d}{d\theta}\sum_{i=1}^{n}\frac{y_i-\theta}{2}
        = -\sum_{i=1}^{n}y_i + n\theta
        = \mathbf{0}.   \nonumber
\end{equation}
In other words, $\hat{\T} = \bar{y}$, because this example was set up with the same assumptions
as a normal maximum likelihood problem but without specifying a normal likelihood function.

Quasi-likelihood models drop the requirement that the true underlying density of the outcome 
variable belongs to a particular exponential family form.  Instead, all that is required is 
the identification of the first and second moments and an expression for their relationship 
up to a proportionality constant.  It is assumed that the observations are independent and 
that mean function describes the mean effect of interest.  Even given this generalization of 
the likelihood assumptions, it can be shown that quasi-likelihood estimators are consistent 
asymptotically equal to the true estimand (Fahrmeir\index{authorindex}{Fahrmeir, L.} and 
Tutz\index{authorindex}{Tutz, G.} 2001, pp.55-60, 
Firth\index{authorindex}{Firth, D.} 1987; 
McCullagh\index{authorindex}{McCullagh, P.}
1983).  However, a quasi-likelihood estimator is often less efficient than a corresponding 
maximum likelihood estimator (McCullagh\index{authorindex}{McCullagh, P.} and 
Nelder\index{authorindex}{Nelder, J. A.} 1989, pp.347-348; Shao\index{authorindex}{Shao, J.} 2005).

Despite this drawback with regard to variance, there are often times when it is convenient or 
necessary to specify a quasi-likelihood model.  A number of authors have extended the 
quasi-likelihood framework to: \emph{extended quasi-likelihood} models to compare different 
variance functions for the same data (Nelder\index{authorindex}{Nelder, J. A.} and 
Pregibon\index{authorindex}{Pregibon, D.}
1987), \emph{pseudo-likelihood} models, which build upon extended quasi-likelihood
models by substituting a $\chi^2$ component instead of a deviant component in dispersion analysis
(Breslow\index{authorindex}{Breslow, N.} 1990; 
Carroll\index{authorindex}{Carroll, R. J.} and 
Ruppert\index{authorindex}{Ruppert, D.}
1982; Davidian\index{authorindex}{Davidian, M.} and 
Carroll\index{authorindex}{Carroll, R. J.} 1987), and models where the 
dispersion parameter is dependent on specified covariates (Smyth\index{authorindex}{Smyth, G. K.} 1989).  
Nelder\index{authorindex}{Nelder, J. A.} and 
Lee\index{authorindex}{Lee, Y.} (1992) provide an informative overview of 
these variations.  It is also the case that quasi-likelihood models are not more difficult to 
compute (Nelder\index{authorindex}{Nelder, J. A.} 1985), and the \R\ package has preprogrammed functions
that make the process routine (see below).\index{subjectindex}{R@\R!quasi-Poisson}
\index{subjectindex}{distribution!chi-square@$\chi^2$}

\begin{examplelist}
	\item	{\bf A Quasi-Likelihood Model of Military Coups.}\label{Africa.Example.Continued}
            \index{subjectindex}{quasi-likelihood GLM} \index{subjectindex}{example!military coups in Africa}
            We can develop a quasi-likelihood model for counts in the Africa Coups data as an alternative to stipulating a regular
            Poisson GLM as done in the previous example.  First stipulate that: 
            \begin{equation}
                \mu_i = v(\mu_i), 
            \end{equation}
            so that
            \begin{equation}
                Q_i = \int_{y_i}^{\mu_i}\frac{y_i - t}{a(\psi)v(\mu)}dt 
                      = (y_i\log\mu_i -\mu_i -y_i\log +y_i)/a(\psi).
            \end{equation}
            Therefore we get the form: 
            \begin{equation}
                \frac{d}{d\mu}\sum_{i=1}^{n}Q_i = \frac{d}{d\mu}\sum_{i=1}^{n} (y_i\log\mu_i -\mu_i), 
            \end{equation}
            which is the same as the regular GLM model for a Poisson link function except that the scale parameter is no longer 
            fixed to be equal to one and is instead estimated by: 
            \begin{equation}
                \hat{a(\psi)} = (n-k)^{-1} \sum(y_i-\mu_i)^2/v(\mu_i).  
            \end{equation}
            This means that the coefficient estimates will be exactly as they were before but the corresponding errors will differ
            here because the scale parameter is now free to vary.

            Constructing this model is very easy in \R: 
            \begin{R.Code} 
africa.quasi.out <- glm(formula = MILTCOUP ~ MILITARY + POLLIB 
        + PARTY93 + PCTVOTE + PCTTURN + SIZE*POP + NUMREGIM*NUMELEC, 
        data=africa, family=quasipoisson(link="log")) 
summary(africa.quasi.out)
        \end{R.Code}
        \index{subjectindex}{R@\R!quasi-likelihood specification}
        Running this we notice that the coefficient estimates are identical to the regular Poisson GLM specification, as are the
        deviance summaries.  The estimated scale parameter is estimated to be $0.325$, justifying use of the more flexible form. 
        The only differences found, as expected, are the resulting standard errors:\\[3pt]

	    \parbox[c]{\linewidth}{
        \blstable \begin{tabular}{rrrr}
	        Intercept    & MILITARY &  POLLIB  & PARTY93 \\
	 	    0.76200      &  0.02901 &  0.18920 & 0.00619 \\
		    PCTVOTE      &  PCTTURN &  SIZE    & POP  \\
		    0.01240      &  0.00780 & 0.00041  & 0.02260  \\
		    NUMREGIM     & NUMELEC  & SIZE:POP & NUMREGIM:NUMELEC \\
		    0.26057      & 0.12070  &  0.00002 & 0.03929 \\
	    \end{tabular}\bls
        }
\end{examplelist}		    

\section{Exercises}
\begin{exercises}
    \item 	Suppose $X_1,\ldots,X_n$ are iid exponential: $f(x|\theta) = \theta e^{-\theta x}, \quad \theta >0$.  Find the 
            maximum likelihood estimate of $\theta$ by constructing the joint distribution, express the log likelihood function, 
            take the first derivative with respect to $\theta$, set this function equal to zero, and solve for $\hat{\theta}$ 
            the maximum likelihood value.  \index{subjectindex}{distribution!exponential}\index{subjectindex}{iid}

	\item 	If the variance of the residuals in the linear model is not constant, then the regression model is heteroscedastic.
            \index{subjectindex}{heteroscedasticity}  The \emph{general linear model}\index{subjectindex}{general linear model} 
            can be used when the form of the heteroscedasticity is known.  Assuming the residuals are uncorrelated, the new 
            $n \times n$ variance matrix is given by:
		    \begin{equation}
			    \sigma\mathbf{\Omega} =
			        \left[ \begin{array}{rrrrr}
				    \sigma_1^2 & 0          & \ldots    & \ldots      &   0 \\
				    0          & \sigma_2^2 & 0         & \ldots      &   0 \\
				    \vdots     & \ldots     & \ddots    & \ldots      &   \vdots \\
				    \vdots     & \ldots     & \ldots    & \ddots      &   \vdots \\
				    0          & 0          & 0         & \ldots      &   \sigma_n^2 \\
			        \end{array} \right]. \nonumber
		    \end{equation}
		    Using this matrix, calculate the new maximum likelihood estimate for the unknown parameter vector $\mathbf{b}$.

         	% There are several ways of showing this.  Here is a reasonably general and therefore reasonably
          	% theoretical.  Start with the log likelihood expression from an exponential family form of the
          	% type:
          	% \begin{equation}
                %	 \ell(x,\phi,w,\theta) = \sum_{i=1}^{n} \left[ (y_i\theta_i - b(\theta_i))w_i/\phi_i
                %                     	 - \frac{1}{2}c_1(-w_i/\phi_i) - c_2(y_i) \right]  \nonumber
          	% \end{equation}
          	% in the (slightly different notation of Barndorff-Nielsen and Blaesild (1983) where $\phi_i = diag(\Omega)$
          	% from above.  This means that the new weights can be expressed as $w^*_i = w_i/\phi_i$.  Define
          	% the new diagonal weight matrix by: $\mathbf{W} = diag(w^*_i g'(\mu_i)^{-2}v(\mu_i))$, and the
          	% ``working'' vector by: $\mathbf{r}_i = g'(\mu_i)(y_i - \mu_i) + g(\mu_i)$.  Here $g(\mu_i)$ is the
          	% link function.  The payoff for this setup is that the coefficient vector can be calculated
          	% (iteratively) by:
          	% \begin{equation}
                %	 \B = (\mathbf{X}'\mathbf{WX})^{-1}\mathbf{X}'\mathbf{Wr}       \nonumber
          	% \end{equation}


    	\item 	Below are two sets of data each with least square regression lines calculated ($\hat{y} = 6 + 0x$).  Answer 
		    the following questions by looking at the plots.\\
	    	\begin{center}
		    \vspace{-30pt}
		    \begin{figure}[h]
			    \centerline{ $\qquad\qquad$ \epsfig{file=./Images/appendix_a.figure01.ps, width=2.25in, height=4.25in,clip=,angle=270} }
		    \end{figure}
		    \end{center}
		    \vspace{-.4in}
	        \index{subjectindex}{linear model!Gauss-Markov assumptions}
	    	\begin{enumerate}
			    \item   Does the construction of the least squares line in panel 1 violate any of
				    the Gauss-Markov assumptions?
			    \item   Does the construction of the least squares line in panel 2 violate any of
				    the Gauss-Markov assumptions?
			    \item   Does the identified point (identically located in both panels) have
				    a substantively different interpretation?
		    \end{enumerate}

    \item 	Calculate the maximum likelihood estimate of the intensity parameter of the Poisson distribution, 
            $f(y|\mu) = \frac{e^{-\mu} \mu^y}{y!}, \quad \mu > 0$, for the data: $[7,4,3,4,7,6,9,11,21,3]$.
            \index{subjectindex}{distribution!Poisson}

	\item  	Consider the bivariate normal PDF:
		    \begin{align}
			    &f(x_1,x_2) = \left(2\pi\sigma_1\sigma_2\sqrt{1-\rho^2}\right)^{-1}\;\times
				    \nonumber \\
			        &\text{\normalfont{exp}}\left[-\frac{1}{2(1-\rho^2)} \left(
				        \frac{(x_1-\mu_1)^2}{\sigma_1^2} -
				        \frac{2\rho(x_1-\mu_1)(x_2-\mu_2)}{\sigma_1 \sigma_2} +
				        \frac{(x_2-\mu_2)^2}{\sigma_2^2}
				    \right)\right].    \nonumber
		    \end{align}
		    for $-\infty < \mu_1,\mu_2 < \infty$, $\sigma_1,\sigma_2 > 0$, and $\rho \in [-1:1]$.\index{subjectindex}{distribution!bivariate normal}

		For $\mu_1 = 3, \mu_2 = 2, \sigma_1 = 0.5, \sigma_2 = 1.5, \rho=0.75$, calculate a grid search using \begin{normalfont}\R\end{normalfont} 
        for the mode of this bivariate distribution on $\mathbb{R}^2$.  A grid search bins the parameter space into equal space intervals on each 
        axis and then systematically evaluates each resulting subspace.  First set up a two dimensional coordinate system stored in a matrix 
        covering 99\% of the support of this bivariate density, then do a systematic analysis of the density to show the mode without using 
        ``for'' loops.  Hint: see the \begin{normalfont}\R\end{normalfont} help menu for the \begin{normalfont}\texttt{outer}\end{normalfont}
		function.  Use the \begin{normalfont}\texttt{contour}\end{normalfont} function to make a figure depicting bivariate contour lines at 0.05, 
        0.1, 0.2, and 0.3 levels.

          	% \begin{R.Code}
          	% dmultnorm <- function(x,y,mu1,mu2,sigma.mat)  {
              	% rho <- sigma.mat[1,2]/prod(sqrt(diag(sigma.mat)))
              	% nlizer <- 1/(2*pi*prod(sqrt(diag(sigma.mat)))*sqrt(1-rho^2))
              	% e.term1 <- (x - mu1)/sqrt(sigma.mat[1,1])
              	% e.term2 <- (y - mu2)/sqrt(sigma.mat[2,2])
              	% like <- exp( -1/(2*(1-rho^2)) *
                %              	 (e.term1^2 + e.term2^2 - 2*rho*e.term1*e.term2) )
              	% nlizer*like
          	% }
		%	 
          	% x.ruler <- seq(1.8,4.2,length=30); y.ruler <- seq(-1.2,5,length=30)
          	% xy.cov.mat <- matrix(c(0.5^2,0.75*0.5*1.5,0.75*0.5*1.5,1.5^2),2,2)
          	% xy.grid <- outer(x.ruler,y.ruler,dmultnorm,3,2,xy.cov.mat)
          	% contours <- c(0.05,0.1,0.2,0.3)
          	% postscript("Spring.2002/contour.figure.ps")
          	% contour(x.ruler,y.ruler,xy.grid,levels=contours,xlab="X",ylab="Y", cex=2)
          	% dev.off()
          	% \end{R.Code}
		% 
          	% \begin{figure}[h]
          	% \begin{center}
          	% \parbox[l]{0.9\linewidth}{ \hspace{0.7in}
                %	 \epsfig{file=../Class.GLM/contour.figure.ps,height=4.65in,width=3.05in,clip=,angle=270}  }
          	% \end{center}
          	% \end{figure}

    	\item 	For the simplified Pareto PDF:
                \begin{equation}
                    f(x|\theta) = \theta x^{-\theta-1}, \qquad 0 \le x,\theta < \infty,
                    \nonumber
                \end{equation}
                find the maximum likelihood estimate for an iid sample: $X_1,\ldots,X_n$.  \index{subjectindex}{distribution!Pareto}

                % \begin{align*}
                %        L(a|\x) &= a^n \prod_{i=1}^n x_i^{-(a+1)}                       \9
                %     \ell(a|\x) &= n\log(a) - (a+1) \sum_{i=1}^n \log(x_i)              \9
                %        \frac{\partial}{\partial a} \ell(a|\x)
                %                &= \frac{n}{a} - \sum_{i=1}^n \log(x_i) \equiv 0        \9
                %        \hat{a} &= n[ \log(x_i) ]^{-1}                                  
                % \end{align*}

	\item 	Derive the exponential family form and $b(\theta)$ for the Inverse Gaussian distribution: 
          	\begin{equation*}
                	f(x|\mu,\lambda) = \left( \frac{\lambda}{2\pi x^3} \right)^{1/2}
                                   	\exp \left[ -\frac{\lambda}{2\mu^2 x}(x-\mu)^2 \right],
                                   	\qquad x>0, \mu > 0.
          	\end{equation*}
          	Assume $\lambda=1$.  
	  	    %\begin{align}
          	%      f(x|\mu,\lambda) &= \exp\left[ \frac{1}{2}\log(1) - \frac{1}{2}\log(2\pi x^3)
          	%                              - \frac{1}{2\mu^2 x} (x - \mu)^2 \right]
          	%                       = \exp\left[ -\frac{1}{2}\log(2\pi) -\frac{3}{2}\log(x)
          	%                              -\frac{x}{2\mu^2} + \frac{1}{\mu} - \frac{1}{2x} \right]
          	%                              \nonumber\\
          	%                       &= \exp\left[ \underbrace{-\frac{1}{2}\mu^{-2}x}_{\theta y}
          	%                                      +\underbrace{\mu^{-1}}_{-b(\theta)}
          	%                                      -\underbrace{\frac{1}{2}\log(2\pi)-\frac{3}{2}\log x
          	%                                              -\frac{1}{2}x^{-1}}_{c(y)} \right]
          	%                              \nonumber
          	%\end{align}
          	%Since $\theta = -\frac{1}{2}\mu^{-2}$, then $\mu^2 = (-2\theta)^{-1}$, $\mu = (-2\theta)^{-1/2}$.
          	%So: $b(\theta) = -1/\mu\big|_{\mu = 1/\sqrt{-2\theta}} = -1/(-2\theta)^{-1/2}
                %        	= -\sqrt{-2\theta}$, with $\theta<0$.
	
    \item 	Using exponential family assumptions and notation, show that the score function has the following three important properties:
            \begin{bayeslist}
                \item   $E_Y[\frac{\partial}{\partial\mu_i}\ell(\T(\mu_i)|y_i,\phi)] = 0$,
                \item   $\Var_Y\left[ \frac{\partial}{\partial\mu_i}\ell(\T(\mu_i)|y_i,\phi) \right]= \frac{1}{\phi v(\mu_i)}$, 
                \item   $-E_Y\left[ \frac{\partial^2}{\partial\mu_i^2}\ell(\T(\mu_i)|y_i,\phi) \right] = \frac{1}{\phi v(\mu_i)}$.  
            \end{bayeslist}
            \index{subjectindex}{exponential family form!score function properties}
            Now prove that the quasi-likelihood substitution function, $q_i = \frac{y_i-\mu_i}{\phi v(\mu_i)}$, also satisfies these three
            properties for likelihood estimation.\index{subjectindex}{quasi-likelihood!substitution function properties}

                %\begin{align}
                %\text{For the score function:}\quad
                %E[\frac{\partial}{\partial\mu_i}\ell(\T(\mu_i)|y_i,\phi)] 
                %        &= E_Y\left[ \frac{\partial}{\partial\mu_i}\left(\frac{\y_i\T - \T(\mu_i)}{\phi}\right) \right]
                %        = E_Y\left[ \frac{y_i - \frac{\partial}{\partial\T}\T(\mu_i)}{\phi} \right]
                %        = E_Y\left[ \frac{y_i - E[y_i]}{\phi} \right]
                %        = 0             
                %           \nonumber\\  
                %\Var_Y\left[ \frac{\partial}{\partial\mu_i}\ell(\T(\mu_i)|y_i,\phi) \right] 
                %        &= \Var_Y\left[ \frac{\partial}{\partial\T(\mu_i)}\ell(\T(\mu_i)|y_i,\phi)
                %           \frac{\partial\T(\mu_i)}{\partial\mu_i} \right]
                %        = \Var_Y\left[ \frac{\partial}{\partial\T(\mu_i)}\ell(\T(\mu_i)|y_i,\phi) \right]
                %           \left( \frac{\partial\T(\mu_i)}{\partial\mu_i} \right)^2
                %           \nonumber\\ 
                %        &= \left[ -E\left( \frac{\partial^2}{\partial\T(\mu_i)^2}\ell(\T(\mu_i)|y_i,\phi) \right) \right]
                %           \left( \frac{\partial\T(\mu_i)}{\partial\mu_i} \right)^2 
                %        = \frac{1}{\phi}v(\mu_i)\frac{1}{v(\mu_i)^2} = \frac{1}{\phi v(\mu_i)}
                %           \nonumber\\ 
                %E_Y\left[ \frac{\partial}{\partial\mu_i} \ell(\T|y_i,\phi) \right]
                %        &= \Var_Y \left[\frac{\partial}{\partial\mu_i}\ell(\T(\mu_i)|y_i,\phi) \right]
                %        = \frac{1}{\phi v(\mu_i)}
                %           \nonumber
                %\end{align}
                %\begin{align}
                %\text{For the $q_i$ function:}\quad
                %E[q_i] &= \left[ \frac{y_i-\mu_i}{\phi v(\mu_i)} \right] = \frac{1}{\phi v(\mu_i)}(E[y_i] - \mu) = 0
                %                \nonumber\\
                %\Var[q_i] &= E[q_i^2] - E[q_i]^2 = E\left[\left(\frac{y_i-\mu_i}{\phi v(\mu_i)}\right)^2\right]
                %          = \frac{1}{\phi^2 v(\mu_i)^2} \Var[y_i] = \frac{\phi v(\mu_i)}{\phi^2 v(\mu_i)^2}
                %          = \frac{1}{\phi v(\mu_i)}
                %        \nonumber\\
                %-E\left[\frac{\partial}{\partial\mu_i}q_i\right]
                %         &= -\left[ \frac{\partial}{\partial\mu_i} \frac{y_i-\mu_i}{\phi v(\mu_i)} \right]
                %          = \frac{-1}{\phi v(\mu_i)}(0-1) = \frac{1}{\phi v(\mu_i)}
                %                \nonumber
                %\end{align}

	\item   Show that the Weibull distribution, 
            $w(x|\gamma,\beta) = \frac{\gamma}{\beta} x^{\gamma-1} \exp\left(-\left(\frac{x}{\beta}\right)^{\gamma}\right)$
        	is an exponential family form when $\gamma=2$, labeling each part of the final answer.

    \item 	A well-known alternative to Newton-Raphson that does not require the calculation of derivatives is the secant method, 
            where at the $(j+1)^{\text{th}}$ step we calculate: 
            $x^{(j+1)} = x^{(j)} - f(x^{(j)}) \frac{ x^{(j)} - x^{(j-1)} }{ f(x^{(j)}) - f(x^{(j-1)})}$. 
            Write an algorithm (\R\ or pseudo-code) to apply the secant method to the problem in the root of the simple equation 
            $f(x) = x^2 - \mu = 0$ (i.e. a function that finds the square root of $\mu$). 

                %\begin{minipage}[t]{.60\linewidth}
                %\begin{small}
                %\begin{R.Code}
                %secant.ex <- function(m,x,iterations)  {
                %  for (i in 1:iterations)  {
                %    x.new <- x[2]-(x[2]^2-m)*(x[2]-x[1])/((x[2]^2-m)-(x[1]^2-m))
                %    x[1] <- x[2]; x[2] <- x.new
                %  }
                %    return(x[2])
                %}
                %\end{R.Code}
                %\end{small}
                %\end{minipage}
                %\begin{minipage}[t]{.40\linewidth}
                %\begin{small}
                %\begin{R.Code}
                %        > secant.ex(99,c(1,2),8)
                %        [1] 9.949874
                %        > sqrt(99)
                %        [1] 9.949874
                %\end{R.Code}
                %\end{small}
                %\end{minipage}

    \item 	For normally distributed data a robust estimate of the standard deviation can be calculated by 
            (Devore\index{authorindex}{Devore, J.} 1999):
                \begin{equation}
                    \tilde{\sigma} = \frac{1}{0.6745n}\sum_{i=1}^{n}\left|X_i-\bar{X}\right|.
                    \nonumber
                \end{equation}
		    \index{subjectindex}{normal robust standard error}
            Write an \begin{normalfont}\R\end{normalfont} function to calculate $\tilde{\sigma}$ for the FARM variable
            in Exercise~A.\ref{dichot.exercise} below.  Compare this value to the standard deviation.  Is there evidence 
            of influential outliers in this variable?

    \item 	Cigarette smoking among elderly males has decreased substantially whereas cigarette smoking among elderly females 
            has remained fairly constant.  Given the following data, calculate the maximum likelihood estimate of the mean and 
            variance for each group under a normal assumption. Test for a significant difference.\\

            \parbox[c]{\linewidth}{ \hspace{-0.2in}
                \begin{normalfont} \begin{small} 
		        \begin{tabular}{lp{.35cm}p{.35cm}p{.35cm}p{.35cm}p{.35cm}p{.35cm}p{.35cm}p{.35cm}
					    p{.35cm}p{.35cm}p{.35cm}p{.35cm}}
				    \multicolumn{13}{c}{Proportion of Smokers, 65 and Over} \\
			    	    Year   & 1965& 1974& 1979& 1983& 1985& 1990& 1992& 1993& 1994& 1995& 1997& 1998  \\
				    Male   & 28.5&~~9.6& 24.9& 12.0& 20.9& 13.2& 22.0& 13.1 & 19.6& 13.5& 14.6& 11.5 \\
				    Female & 16.6& 12.4& 13.5& 10.5& 13.2& 11.1& 14.9& 11.5 & 12.8& 11.5& 10.4& 11.2 \\
		        \end{tabular} \end{small} \end{normalfont}
                \vspace{11pt}
            }

		    {\bf Source:} Centers for Disease Control and Prevention, National Center for Health Statistics.  National Health 
            Interview Survey, respective years.

	\item 	Derive the generalized hat matrix from $\hat{\B}$ produced from the final ($n^{th}$) IWLS step.
      		Specifically, give the quantities of interest from the $n^{th}$ and $(n-1)^{th}$ step,
      		and show how the hat matrix is produced.  Secondly, give the form of Cook's D appropriate to
      		a GLM and show that it contains hat matrix values.

      		% At the $n^{th}$ (last) step of IWLS:
      		% \begin{equation}
        	%	\B_n = (\X w_{n-1} \X)^{-1}\X'w_n z_{n-1},
             	%	\qquad
        	%	\text{where:} z_{n-1} = \hat{\E}
                %		+ \left( \frac{\partial\eta}{\partial\mu}\biggm|_{\hat{\M}_{n-1}} \right) (\y - \M),
             	%	\qquad
                %      		w_{n-1}^{-1} = \left( \frac{\partial\eta}{\partial\mu}\biggm|_{\hat{\M}_{n-1}} \right)^{2}
                %        		v(\M)\biggm|_{\hat{\M}_{n-1}}
             	%	\nonumber
      		% \end{equation}
      		% Now set: $\X^{*} = w_{n-1}^{1/2}\X$, $H^{*} = \left( w_{n-1}^{1/2} \X \right)
                %                                    		\left( \X w_{n-1}^{1/2} \X \right)^{1/2}
                %                                    		\left( w_{n-1}^{1/2} \X \right)'$.
	 	    %
      		% For part two, start with Cook's D for a linear model:
      		% \begin{equation}
        	%	D_i = (\hat{\B}_{(i)} - \hat{\B})(\X'\X)(\hat{\B}_{(i)} - \hat{\B})/ps^2,       \nonumber
      		% \end{equation}
      		% and modify by inserting the IWLS weighting:
      		% \begin{equation}
        	%	D_i(GLM) = (\hat{\B}_{(i)} - \hat{\B})(\X'w_{n-1}\X)(\hat{\B}_{(i)} - \hat{\B})/p\phi,  \nonumber
      		% \end{equation}
      		% where $\phi$ is the standard GLM dispersion parameter.
      		% The relationship to the hat matrix is found in inserting the equivalence:
      		% \begin{equation}
        	%	\hat{\B}_{(i)} - \hat{\B} = \frac{-(\X'\X)^{-1}\X_i r_i'}{p(1-h_{ii})}, \qquad
                %                    		r_i' = \frac{y_i = \hat{\mu_i}}{\sqrt{1-h_{ii}}}.
                %		\nonumber
      		% \end{equation}
		
	\item	An exponential family form has \emph{quadratic variance}\index{subjectindex}{quadratic variance} if
		    the variance term can be expressed as $v(\mu) = \phi''(\theta) = \eta_0 + \eta_1\mu + \eta_2\mu^2$,
		    where $\mu$ is the mean function.  Show that there exists an exponential family form conjugate 
		    distribution and that it also has quadratic variance.

    \item 	\label{dichot.exercise} The likelihood function for dichotomous choice regression is given by:
		    \index{subjectindex}{GLM!dichotomous outcome variable}
            \begin{equation}
                    L(\mathbf{b},\mathbf{y}) = \prod_{i=1}^{n}
                    \left[F(\mathbf{x}_i\mathbf{b})\right]^{\mathbf{y}_i}
                    \left[1-F(\mathbf{x}_i\mathbf{b})\right]^{1-\mathbf{y}_i}.
                    \nonumber
            \end{equation}
            There are several common choices for the $F()$ function:\\
		    \begin{center}
                \begin{tabular}{ll}
			    &  \\[-8pt]
                    Logit:  & $\Lambda(\mathbf{x}_i\mathbf{b}) = 
				        \frac{1}{1+\text{\normalfont{exp}}[\mathbf{x}_i\mathbf{b}]}$ 		\\[15pt]
                    Probit: & $\Phi(\mathbf{x}_i\mathbf{b}) = \int_{-\infty}^{\mathbf{x}_i\mathbf{b}}
                                    \frac{1}{\sqrt{2\pi}}\text{\normalfont{exp}}[-t^2/2]dt $ 		\\[15pt]
                    Cloglog: & $CLL(\mathbf{x}_i\mathbf{b}) 
			            = 1-\text{\normalfont{exp}}\left(-\text{\normalfont{exp}}(\mathbf{x}_i\mathbf{b})\right)$.
                      											\\[5pt]
			        &  \\[-12pt]
                \end{tabular}
		    \end{center}
	
            \noindent Using the depression era economic and electoral data, calculate a dichotomous regression model for using 
            each of these link functions according to the following model specification:
                \begin{equation}
                    FDR \sim f[(POST.DEP - PRE.DEP) + FARM]    \nonumber
                \end{equation}
                where: $FDR$ indicates whether or not Roosevelt carried that state in the 1932 presidential elections, $PRE.DEP$ is the 
                mean per-state income before the onset of the Great Depression (1929) in dollars, $POST.DEP$ is the mean per-state 
                income after the onset of the great depression (1932) in dollars, and $FARM$ is the total farm wage and salary 
                disbursements in thousands of dollars per state in 1932.

                \parbox[c]{\linewidth}{
                \begin{normalfont}
                \begin{tiny}\begin{tabular}{lrrrr}\label{FDR.table}
                State               & FDR     & PRE.DEP   & POST.DEP  &    FARM     \\
                \hline
                Alabama             &    1    &    323    &    162    &    4067 \\
                Arizona             &    1    &    600    &    321    &    6100 \\
                Arkansas            &    1    &    310    &    157    &    8134 \\
                California          &    1    &    991    &    580    &    83371    \\
                Colorado            &    1    &    634    &    354    &    10167    \\
                Connecticut         &    0    &   1024    &    620    &    10167    \\
                Delaware            &    0    &   1032    &    590    &    3050 \\
                District of Columbia&    1    &   1269    &    1054   &    0    \\
                Florida             &    1    &    518    &    319    &    14234    \\
                Georgia             &    1    &    347    &    200    &    10167    \\
                Idaho               &    1    &    507    &    274    &    7117 \\
                Illinois            &    1    &    948    &    486    &    27451    \\
                Indiana             &    1    &    607    &    310    &    11184    \\
                Iowa                &    1    &    581    &    297    &    28468    \\
                Kansas              &    1    &    532    &    266    &    22368    \\
                Kentucky            &    1    &    393    &    211    &    8134 \\
                Louisiana           &    1    &    414    &    241    &    10167    \\
                Maine               &    0    &    601    &    377    &    6100 \\
                Maryland            &    1    &    768    &    512    &    14234    \\
                Massachusetts       &    1    &    906    &    613    &    14234    \\
                Michigan            &    1    &    790    &    394    &    14234    \\
                Minnesota           &    1    &    599    &    363    &    25418    \\
                Mississippi         &    1    &    286    &    127    &    4067 \\
                Missouri            &    1    &    621    &    365    &    15251    \\
                Montana             &    1    &    592    &    339    &    10167    \\
                Nebraska            &    1    &    596    &    307    &    17284    \\
                Nevada              &    1    &    868    &    550    &    4067 \\
                New Hampshire       &    0    &    686    &    427    &    3050 \\
                New Jersey          &    1    &    918    &    587    &    18301    \\
                New Mexico          &    1    &    410    &    208    &    5084 \\
                New York            &    1    &   1152    &   1676    &    38635    \\
                North Carolina      &    1    &    332    &    187    &    8134 \\
                North Dakota        &    1    &    382    &    176    &    14234    \\
                Ohio                &    1    &    771    &    400    &    18301    \\
                Oklahoma            &    1    &    455    &    216    &    9150 \\
                Oregon              &    1    &    668    &    379    &    11184    \\
                Pennsylvania        &    0    &    772    &    449    &    25418    \\
                Rhode Island        &    1    &    874    &    575    &    2033 \\
                South Carolina      &    1    &    271    &    159    &    7117 \\
                South Dakota        &    1    &    426    &    189    &    8134 \\
                Tennessee           &    1    &    378    &    198    &    6100 \\
                Texas               &    1    &    479    &    266    &    33552    \\
                Utah                &    1    &    551    &    305    &    4067 \\
                Vermont             &    0    &    634    &    365    &    5084 \\
                Virginia            &    1    &    434    &    284    &    15251    \\
                Washington          &    1    &    741    &    402    &    14234    \\
                West Virginia       &    1    &    460    &    257    &    6100 \\
                Wisconsin           &    1    &    673    &    362    &    21351    \\
                Wyoming             &    1    &    675    &    374    &    5084 \\
                \end{tabular}\end{tiny}\bls
                \end{normalfont}
                }

                (also provided in the \R\ package \texttt{BaM}). % NEED TO ADD TO BaM [already there]
                Do you find substantially different results with the three link functions?
                Explain.  Which one would you use to report results?  Why?

            	% \begin{small}
            	% \begin{R.Code}
            	% Call:
            	% glm(formula = FDR ~ I(POST2.DEP - PRE.DEP) + FARM, family = binomial(link = "cloglog"))
		        % 
            	% Deviance Residuals: 
                %     Min       1Q   Median       3Q      Max  
            	% -0.9544  -0.5103  -0.3278  -0.2049   2.5667
		        % 
            	% Coefficients:
                %                    	  Estimate Std. Error z value Pr(>|z|)
            	% (Intercept)            -4.663e+00  1.650e+00  -2.825  0.00472 
            	% I(POST2.DEP - PRE.DEP) -1.372e-02  5.401e-03  -2.540  0.01107 
            	% FARM                   -1.198e-04  6.311e-05  -1.898  0.05775 
            	% ---
            	% (Dispersion parameter for binomial family taken to be 1)
		        % 
                %    	 Null deviance: 36.434  on 48  degrees of freedom
                %	 Residual deviance: 28.493  on 46  degrees of freedom
                %	 AIC: 34.493
		        % 
            	% fdr.out <- glm(FDR~I(POST2.DEP-PRE.DEP)+FARM, family=binomial(link="probit"))
            	% summary.glm(fdr.out)
		        % 
            	% Deviance Residuals:
                %     Min       1Q   Median       3Q      Max
            	% -0.9304  -0.5412  -0.3447  -0.1672   2.5301
		        % 
            	% Coefficients:
                %                     	  Estimate Std. Error z value Pr(>|z|)
            	% (Intercept)            -2.752e+00  9.701e-01  -2.837  0.00455 
            	% I(POST2.DEP - PRE.DEP) -8.295e-03  3.662e-03  -2.265  0.02351 
            	% FARM                   -6.566e-05  3.812e-05  -1.723  0.08496 
            	% ---
            	% (Dispersion parameter for binomial family taken to be 1)
		        % 
                %    	 Null deviance: 36.434  on 48  degrees of freedom
                %	 Residual deviance: 29.093  on 46  degrees of freedom
                %	 AIC: 35.093
		        % 
            	% fdr.out <- glm(FDR~I(POST2.DEP-PRE.DEP)+FARM, family=binomial(link="logit"))
            	% summary.glm(fdr.out)
		        %	 
            	% Deviance Residuals:
               	%     Min       1Q   Median       3Q      Max
            	% -0.9504  -0.5198  -0.3261  -0.1964   2.5775
		        % 
            	% Coefficients:
                %                    	  Estimate Std. Error z value Pr(>|z|)
            	% (Intercept)            -4.822e+00  1.920e+00  -2.512   0.0120 
            	% I(POST2.DEP - PRE.DEP) -1.508e-02  7.030e-03  -2.145   0.0320 
            	% FARM                   -1.311e-04  7.549e-05  -1.737   0.0824 
            	% ---
            	% (Dispersion parameter for binomial family taken to be 1)
		        % 
                %   	 Null deviance: 36.434  on 48  degrees of freedom
                %	 Residual deviance: 28.900  on 46  degrees of freedom
                %	 AIC: 34.9
            	% \end{R.Code}
            	% \end{small}
		        % 
               	% All three link functions lead to the same coefficient reliability conclusions
               	% at the standard (but arbitrary) thresholds, except that the intercept for
               	% the cloglog model is better (even though the intercept is clearly not
               	% \emph{substantively} important here).  The residual deviances are all very
               	% close in sizewith the cloglog model being just a little bit better.  The
               	% Akaike information criterion (two times the negative log likelihood plus two
               	% times the number of parameters: smaller values imply a better fit) also
               	% favors the cloglog model but only sightly so.  More tellingly, all three
               	% models demonstrate an asymmetric distribution of the deviance residuals,
              	% indicating the need for further investigation.

            % NEW

            \index{subjectindex}{iid}
    \item   Suppose you have an iid sample of size $n$ from the exponential distribution:
            \begin{equation*}
            f(y|\theta) = \frac{1}{\theta}\exp\left[ -\frac{y}{\theta} \right], \quad y \ge 0,\; \theta > 0.
            \end{equation*}
            Derive the MLE for $\theta^2$.  For the data $\y=[5,3,2,1,4]$, calculate the MLE.
            \begin{comment}
            \begin{align*}
            L(\theta|\y) &= \prod_{i=1}^n \theta^{-1}\exp\left[ -\frac{y_i}{\theta} \right] \\
            &= \theta^{-n} \exp\left[ \sum_{i=1}^n -\frac{y}{\theta} \right] \\
            \ell(\theta|\y) &= -n \log(\theta) - \frac{1}{\theta}\sum_{i=1}^n y_i \\
            \ddy\ell(\theta|\y) &= -\frac{n}{\theta} + \frac{1}{\theta^2}\sum_{i=1}^n y_i \equiv 0\\
            & \text{therefore}\; \hat{\theta} = \frac{1}{n} \sum_{i=1}^n y_i
            \end{align*}
            \end{comment}

            % NEW
    \item   Tobit regression (censored regression) deals with an interval-measured outcome variable that is censored 
	        such that all values that would have naturally been observed as negative are reported as zero,
	        generalizeable to other values (Tobin 1958, Amemiya (1985, Chapter 10), Chib (1992).  There can be left 
            censoring and right censoring at any arbitrary value, single and double censoring, mixed truncating and 
            censoring.  If $\z$ is a latent outcome variable in this context with the assumed relation:
            \index{authorindex}{Tobin, J.}\index{authorindex}{Amemiya, T.}\index{authorindex}{Chib, S.}
            \begin{equation*}
                \z = \x\B + \E \qquad\qquad \text{and} \qquad\qquad
                z_i \sim \mathcal{N}(\x\B,\sigma^2),
            \end{equation*}
            then for \emph{left censoring at zero}, the observed outcome variable is produced according to:
            \begin{equation*}   
                y_i = \begin{cases}
                        z_i \;\text{if}\; z_i > 0               \\
                        0   \;\text{if}\; z_i \le 0. 
                      \end{cases}
                \end{equation*}
            The resulting likelihood function is:
            \begin{equation*}
                	L(\B,\sigma^2|\y,\x) = \prod_{y_i=0}\left[ 1 - \text{\parbox{0.12in}{\begin{Large}${\Phi}$\end{Large}}}
                                                                                \left(\frac{x_i\B}{\sigma}\right) \right]
                                                        \prod_{y_i>0}(\sigma^{-1})\exp\left[ -\frac{1}{2\sigma^2}(y_i - x_i\B)^2 \right],
            \end{equation*} 
            where $\sigma^2$ is called the \emph{scale}.  
			Explain the structure of the likelihood function: what parts accommodate observed values and what parts accommodate unobserved 
            values? Replicate Tobin's original analysis in \R\ using the \texttt{survreg} function in the
		    \texttt{survival} package.  The data are obtainable by:
			\begin{verbatim}
tobin <- read.table(
         "http://artsci.wustl.edu/~jgill/data/tobin.dat",header=TRUE)
			\end{verbatim}
            or in the \texttt{BaM} package.
                % tobin.tob <- survreg(Surv(durable,durable>0,type='left') ~ age + quant, dist='gaussian',data=tobin)
                % summary(tobin.tob)
                %               Value Std. Error      z        p
                % (Intercept) 15.1449    16.0795  0.942 3.46e-01
                % age         -0.1291     0.2186 -0.590 5.55e-01
                % quant       -0.0455     0.0583 -0.782 4.34e-01
                % Log(scale)   1.7179     0.3103  5.536 3.10e-08
		        %
                %Scale= 5.57
                %
                % Gaussian distribution  
                % Loglik(model)= -28.9   Loglik(intercept only)= -29.5
                %         Chisq= 1.1 on 2 degrees of freedom, p= 0.58
                % Number of Newton-Raphson Iterations: 3
                % n= 20

            % NEW
    \item   An effective way of compensating for heteroscedasticity in probit models is to identify explanatory variables that 
            introduce widely varying response patterns and specifically associate them in the model with the dispersion term.
            With this method, based on Harvey (1976),\index{authorindex}{Harvey, A. C.} the functional form is changed from:
            $P(Y=1) = \Phi(\X\B)$ to $P(Y=1) = \text{\parbox{0.12in}{\begin{Large}${\Phi}$\end{Large}}}
            \left(\frac{\X\B}{e^{\Z\A}}\right)$, where the exponent function in the denominator is just a convenience that
            prevents division by zero.  This approach distinguishes between the standard treatment of the estimated coefficients
            $\B$ with corresponding matrix of explanatory observations $\X$, and the set of dispersion determining estimated
            coefficients $\A$ with a corresponding matrix of explanatory observations $\Z$.  By this means the dispersion term is
            reparameterized to be a function of a set of coefficients and observed values that are suspected of causing the
            differences in error standard deviations: $\sigma_i^2 = e^{\Z_i\A}$.  Derive the log likelihood of the heteroscedastic
            probit and give a formal likelihood ratio test for the existence of heteroscedasticity.

            %\begin{equation}\label{Harvey.Probit}
            %    \ell(\A,\B|\y,\X,\Z) = 
            %        \sum_{i=1}^{n}\biggl[ y_i\log\left( \text{\parbox{0.12in}{\begin{Large}${\Phi}$\end{Large}}}
            %        \left(\frac{\X\B}{e^{\Z\A}}\right)\right) + 
            %        (1-y_i)\log\left(1- \text{\parbox{0.12in}{\begin{Large}${\Phi}$\end{Large}}}
            %        \left(\frac{\X\B}{e^{\Z\A}}\right)\right) \biggr]
            %\end{equation}
            %where the inclusion of a separate parameterized dispersion component in \eqref{Harvey.Probit}
            %addresses the heteroscedasticity problem.  A formal test for heteroscedasticity is
            %a likelihood ratio test using the value of the log likelihood in the restricted model
            %(the restriction is that $\A = \mathbf{0}$) and the unrestricted
            %\eqref{Harvey.Probit} evaluated at the maximum likelihood estimates of the parameters:
            %$\B$ and $\A$.  The test is simply:
            %$LR = -2\left[\ell_{\text{restricted}} - \ell_{\text{unrestricted}}\right]
            %= 2\left[\ell(\X\B,\Z\A) - \ell(\X\B)\right]$,
            %where LR is (as usual) distributed $\chi^2$ but with degrees of freedom equal to the
            %number of columns of the $\Z$ matrix (the number of dispersion estimation parameters
            %in the Harvey specification).

            % NEW
    \item   Sometimes when estimation is problematic, \emph{Restricted Maximum Likelihood} (REML) estimation is helpful 
            (Bartlett 1937).  \index{subjectindex}{restricted maximum likelihood (REML)} \index{authorindex}{Bartlett, M. S.} REML
            uses a likelihood function calculated from a transformed set of data so that some parameters have no effect on the
            estimation on the others.  For a One-Way ANOVA, $y|\G \sim \mathcal{N}(\X\B + \Z\G, \sigma^2\I)$, $\G \sim
            \mathcal{N}(0,\sigma^2\D)$, such that $\Var[y] = \Var[\Z\G] + \Var[\EP] = \sigma^2\Z\D\Z' + \sigma^2\I$.  The
            unconditional distribution is now: $y \sim \mathcal{N}(\X\B,\sigma^2(\I + \Z\D\Z')$ where $\D$ needs to be estimated.
            If  $\V = \I + \Z\D\Z'$, then the likelihood for the data is: $\ell(\B,\sigma,\D|\y) = -\frac{n}{2}\log(2\pi) -
            \half\log |\sigma^2V| - \frac{1}{2\sigma^2}(\y-\X\B)'\V^{-1}(\y-\X\B)$.  The REML steps for this model are: (1) find a
            linear transformation $k$\ such that $k'\X = 0$, so $k'\y \sim \mathcal{N}(0,k'\sigma k)$, (2) run MLE on this model
            to get $\hat{\D}$, which no longer has any fixed effects, (3) then estimate the fixed effects with ML in the normal
            way.  Code this procedure in \R\ and run the function on the depression era economic and electoral data in
            Exercise~\ref{dichot.exercise}.

            % NEW
    \item   Maltzman and Wahlbeck (1996) \index{authorindex}{Maltzman, F.} \index{authorindex}{Wahlbeck, P. J.}
            look at the reasons that Supreme Court justices switch their votes.  Consider the following distances between a justice 
            and the author of the opposing opinion for four cases:\\[-5pt]

            \parbox[c]{\linewidth}{
                \begin{tabular}{lrrrrrrr}
                case     &   1 &  1  &   1   &   1   &  1   &  1 &   1 \\
                distance & -22 & 22  & -22   & -22   & 22   & 12 & -22 \\
                \hline
                case     &   1 &     2 &     2 &    2 &     2 &   2 &     2 \\
                distance & -22 & -42.1 & -42.1 & 14.3 & -32.1 & -44 & -13.9 \\
                \hline
                case     &    2 &     2 &    3 &   3 &    3 &   3 &   3 \\
                distance &-42.1 & -42.1 & -2.2 & 2.2 & -2.2 & 2.2 & 2.2 \\
                \hline
                case     &    3 &  3 &   3 &     4 &    4 &     4 &     4 \\
                distance & -2.2 & 2.2 & 2.2 & -57.6 & 16.6 & -39.8 & -57.6 \\
                \hline
                case     &    4 &     4 &     4 &     4 &     4  &&\\
                distance & 20.4 & -39.4 & -57.6 & -57.6 & -19.6  &&\\
                \end{tabular}\bls
                \vspace{7pt}
            }

            Run a One-Way ANOVA model in \R\ for these data according to:
            % NEED TO ADD TO BaM [added 4/16/15]
            % fluid <- read.table("Article.Interactions/fluid1.data",header=TRUE)
            % scd <- fluid[1:33,c(2,21)]
            % save(scd,file="Book.Bayes.III/BaM2/data.supreme.court.rda")
            \begin{verbatim}
library(lme4)
scd.out <- lmer(dist2 ~ 1+(1|case), scd)
summary(scd.out)
            \end{verbatim}
            \vspace{-22pt}
            and interpret the REML results.

\end{exercises}

\thispagestyle{empty}
\chapter{Utilitarian Markov Chain Monte Carlo}\label{MCMC.Utilitarian.Chapter}
\setcounter{examplecounter}{1}

\section{Objectives}
This chapter has several rather practical purposes related to applied MCMC work: to introduce formal convergence diagnostic 
techniques, to provide tools to improve mixing and coverage, and to note a number of challenges that are routinely encountered.  
This is a stark contrast to the last chapter, which was concerned with theoretical properties of Markov chains and Markov chain 
Monte Carlo.  Since applied work is generally done computationally through the convenient programs \bugs\ (in any of the versions) 
and \jags, or by writing source code in \R, \texttt{C}, or even \texttt{Fortran}, practical considerations are important to 
getting reliable inferences from chain values.  Most of the concern centers on assessing convergence, but the speed of the 
sampler, and its ability to thoroughly explore the sample space are also important issues to be concerned with.  This chapter 
also describes the two very similar \R\ packages for analyzing MCMC output and evaluating convergence: \boa\ and \coda.  
These are merely convenient functional routines, and users will often want to go beyond their capabilities, particularly in 
graphics.  However, the purpose here is mainly to understand the key workings of these tools rather than to function as a 
detailed description of the syntax of software.  See Albert (2009) or Ntzoufras (2009) for recent book-length works with very 
detailed \R\ and \bugs\ code description.

As estimation with MCMC tools becomes more common, mastery of computational mechanical challenges grows in importance.  It is
fundamentally import to remind ourselves that MCMC estimation is not a cookbook procedure and that it is necessary to pay
attention to convergence issues.  There are two critical and practical challenges in assessing MCMC reliability: 
\index{subjectindex}{Markov chain!assessing convergence}
\begin{bayeslist}
	\item   For any Markov chain at some given time $t$, there is no absolute assurance that this chain is currently in 
		its stationary (target) distribution.
	\item	There is no way to guarantee that a Markov chain will explore all of the areas of the target distribution 
		in a finite run time.
\end{bayeslist}
The bulk of the tools in this chapter are a means of adding to our confidence that a specific application has addressed these 
two concerns.  Gelman\index{authorindex}{Gelman, A.} (1996) adds three related standard problems to also consider: an 
inappropriately specified model, errors in programming the Markov chain (the achieved stationary distribution of the chain may 
not be the desired target distribution), and \emph{slow} convergence (distinct from mixing through the posterior).  The latter 
can be particularly troublesome even for experienced researchers in this area as it is often difficult to anticipate problems 
like the chain getting stuck in low-density, high-dimension regions for long periods of time (see the litany of worries 
in Guan, Flei\ss ner, and Joyce  [2006]).  Essentially these concerns boil down to three worries: setting up the parameters 
of the process correctly, ensuring good mixing through the sample space, and making a reliable decision about convergence at 
some point in time.  These are the primary concerns of this chapter.  
\index{authorindex}{Guan, Y.} \index{authorindex}{Fleisner, R.@Flei\ss ner, R.} \index{authorindex}{Joyce, P.}

This preliminary discussion seems to imply that the use of MCMC algorithms is fraught with danger and despair (the \winbugs\
webpage and manual contain a warning message that would impress tobacco regulators).  However, we know from 
Chapter~\ref{MCMC.Theory.Chapter} that all ergodic Markov chains are guaranteed to eventually converge (see also 
Chan\index{authorindex}{Chan, K. S.} [1993] and Polson\index{authorindex}{Polson, N. G.} [1996]), and it is often quite easy 
to determine if a given chain has \emph{not} converged.\index{subjectindex}{ergodic!convergence property}  Consequently, 
convergence diagnostics have moved from fairly \emph{ad hoc} methods (Gelfand\index{authorindex}{Gelfand, A. E.} and 
Smith\index{authorindex}{Smith, A. F. M.} 1990) to the reasonably sophisticated statistical tests outlined in this chapter.
In addition, progressively faster processors mean that the time-dependent issues are guaranteed to diminish in the future. 
Furthermore, in nearly all occasions with typical generalized linear models and non-exotic priors, the Markov chain
converges quickly with obvious evidence. Nonetheless, users of MCMC for Bayesian estimation should use all reasonably caution with
respect to controllable features.

\section{Practical Considerations and Admonitions}
This section covers a few practical considerations in implementing MCMC algorithms.  There are several critical design 
questions that must be answered before actually setting up the chain and running it, whether one is using a packaged 
resource or writing original code.  These include such decisions as: the determination of where to start the chain, 
judging how long to ``burn-in'' the chain before recording values for inference, determining whether to ``thin''  
\index{subjectindex}{thinning chains} the chain values by discarding at intervals, and setting various software implementation 
parameters.\index{subjectindex}{burn-in period of a chain}

\subsection{Starting Points}\index{subjectindex}{starting points}
Starting points as an integrated part of the simulation specification is an under-studied aspect, except perhaps in the case of 
one particular convergence diagnostic (Gelman\index{authorindex}{Gelman, A.} and Rubin\index{authorindex}{Rubin, D. B.} 1992a).
Generally it is best to try several starting points in the state space and observe whether they lead to noticeably different 
descriptions of the posteriors.  This is surely a sign of non-convergence of the Markov chain, although certainly not a 
systematic test.  Unfortunately the reverse does not hold: it is not true that if one starts several Markov chains in different 
places in the state space and they congregate for a time in the same region that this is the region that characterizes
the stationary distribution.  It could be that all of the chains are seduced by the same local maxima, and will for a time 
mix around in its local region.

\emph{Overdispersing} the starting points\index{subjectindex}{starting points!overdispersed} relative to the expected modal 
point is likely to provide a useful assessment (Gelman\index{authorindex}{Gelman, A.} and Rubin\index{authorindex}{Rubin, D. B.} 
1992a, 1992b).  We will look in detail at Gelman\index{authorindex}{Gelman, A.} and Rubin's\index{authorindex}{Rubin, D. B.} 
formal diagnostic in Section~\ref{gelman.rubin.diag}, but for now it is warranted to talk about strategies for determining 
such overdispersed points.  If one can determine the mode with reasonable certainty, either by using the EM algorithm, a grid 
search, or some other technique (possibly analytically), then it is relatively simple to spread starting points around it at 
some distance.  If this is not possible, or perhaps excessively complicated in high dimensions, then it is often 
straightforward to carefully spread starting points widely throughout the sample space.  

Sometimes starting points of \emph{theoretical interest} are available.  It might be the case that a starting point can be 
assigned to values associated with other studies, subject-matter expertise, previous work with the same data, or even the 
modal point of the associated likelihood function.  Often such points are close to the high density region for the posterior
being explored. These strategies, of course, do not involve overdispersion and the use of the Gelman and Rubin convergence
diagnostic.

Some researchers randomly distribute starting points through the state space with the idea that if little is known before the
estimation process, these will at least be reasonably overdispersed points.  Overdispersed starting points relative to the
central region of the distribution mean that if the chains coalesce into the same region anyway, it is some evidence that
they have converged to the stationary distribution.  

\subsection{Thinning the Chain}\index{subjectindex}{thinning chains}
It is sometimes the case that with very long simulations, storage of the observed chain values on the computer becomes an issue,  
although this need has diminished substantially over time.  The need for large storage files results from running the chain for 
extended periods, running multiple parallel chains, or monitoring a large number of parameters.  Social science models, 
however, rarely call for hundreds or thousands of model parameters as one might see in statistical genetics, though it may be
necessary in future research as the bounds between disciplines recede.  More often long runs are needed in the presence of: 
high autocorrelation in the iterations, slow convergence of the chain to its limiting distribution combined with many parallel 
simultaneous runs of the chain, possibly combined with relatively high dimensionality of the model.  These are addressed in
this chapter.  The problem introduced is that disk storage may be strained by these demands.  Furthermore, many of the 
convergence diagnostics described in this chapter slow down considerably with very large matrix sizes.  Some researchers even 
report software failures due to excessively large chains (Rathbun and Black 2006).\index{authorindex}{Rathbun, S. L.} 
\index{authorindex}{Black, B.} However, as computer storage capabilities continue to increase rapidly in size (including cloud
storage), while decreasing in cost, these problems will decline in importance.

\index{subjectindex}{thinning chains} \index{subjectindex}{thinning chains!definition} 
The idea of thinning the chain is to run the chain normally but record only every $k$th value of the chain, $k$ some small
positive integer, thus reducing the storage demands while still preserving the integrity of the Markovian process.  Importantly,
note that thinning does \emph{not} in any way improve the quality of the estimate (suggested presumably but erroneously as a way
to increase the independence of the final evaluated values [Geyer\index{authorindex}{Geyer, C. J.} 1992]), speed up the chain, or
help in convergence and mixing. Instead, it is purely a device for dealing with possibly limited computer resources.  In fact, the
quality of the subsequent estimation always suffers because the resulting variance estimate will be higher for a given run-time
(MacEachern and Berliner 1994), albeit to varying degrees depending on the length of the chain and the nature of the model.  Also
since the variance estimate is wrong it confounds the estimate of the serial correlation of the chain for the purpose of
understanding the mixing properties.  However, sometimes this variance issue is a trivial concern and thinning remains a
convenience.  \index{authorindex}{MacEachern, S. N.} \index{authorindex}{Berliner, L. M.} 

Given the trade-offs between storage and accuracy as well as diagnostic ability, what value of $k$ is appropriate in a given 
application? The greater the amount of thinning, the more potentially important information is lost, suggesting caution.  
Conversely, prior to assumed convergence, thinning is irrelevant \emph{inferentially} and may therefore be useful, provided it 
gives sufficient information to the diagnostics used.  Many researchers pick every fourth, fifth, or tenth iteration to save, but 
for completely arbitrary reasons.  Occasionally applications will thin to every 30th, 50th, or even 100th iteration, but this 
tends to decrease chain efficiency more substantially.\index{subjectindex}{thinning chains}

\subsection{The Burn-In Period}\index{subjectindex}{burn-in period of a chain}\label{burn-in.section}
First, one must decide the length of the burn-in period, the beginning set of runs that are discarded under the assumption that 
they represent pre-convergence values and are therefore not equivalent draws of the desired limiting distribution.  The slower 
the chain is to converge, the more careful one should be about the burn-in period.  Usually this involves cautiously extending
the length of the chain, but this chapter also presents some customized tools for speeding up this process.  Unfortunately, even 
starting the chain right in the area of highest density does not guarantee that the burn-in period is unimportant as it will 
still take the Markov chain some time to ``forget'' its starting region,\index{subjectindex}{Markov chain!memory} take some time 
to settle into the stationary distribution, and then need some further time to fully explore the target distribution.

There is no systematic, universal, guaranteed way to calculate the length of the burn-in period, and considerable work on 
convergence diagnostics has been done to make specific recommendations and identify helpful tests.  Raftery
\index{authorindex}{Raftery, A. E.} and Lewis\index{authorindex}{Lewis, S. M.} (1992, 1996) suggest a running diagnostic for 
the length of the burn-in period, which starts with the analysis of an initial run.  The idea is to solve for the number of 
iterations required to estimate some quantile of interest within an acceptable range of accuracy, at a specified probability level.  
The procedure is based on conventional normal distribution theory and implemented in the \R\ package \texttt{mcgibbsit}, as 
well as in the standard diagnostic packages \boa\ and \coda.  Unfortunately, this procedure does not always work, but it often does 
provide good approximate guidance (Robert and Cellier 1998, Brooks and Roberts 1997).  See Section~\ref{raftery.lewis.diag} for an 
extended description.  \index{authorindex}{Robert, C. P.}\index{authorindex}{Cellier, D.}
\index{subjectindex}{mcgibbsit@\texttt{mcgibbsit}} \index{subjectindex}{burn-in period of a chain}
\index{authorindex}{Brooks, S. P.} \index{authorindex}{Roberts, G. O.}

\section{Assessing Convergence of Markov Chains}\label{convergence.diagnostic.section} \index{subjectindex}{Markov chain!mixing}
\index{subjectindex}{Markov chain Monte Carlo (MCMC)!convergence}\index{subjectindex}{Markov chain!stationary distribution}
As shown in Chapter~\ref{MCMC.Chapter}, the empirical results from a given MCMC analysis are not deemed reliable until the 
chain has reached its stationary distribution and has time to sufficiently mix throughout.  Until $\mathfrak{M}(\T_t,t \ge 0)$ 
converges at time $t^*$ (i.e. $\|f(\T_t^*) - \pi(\T)\|$ is negligible), it is not possible to rely upon the effect of any 
variant of the central limit theorem.  Therefore the single greatest risk in applied MCMC work is that the user will misjudge
the required length of the burn-in period and assert convergence before the Markov chain has actually reached the stationary 
distribution. \index{subjectindex}{burn-in period of a chain}

Unfortunately, some convergence problems come directly from the model specification and it may not be obvious when the MCMC 
process fails. Natarajan and McCulloch (1995)\index{authorindex}{Natarajan, R.} \index{authorindex}{McCulloch, C. E.} found 
that it is possible to have a proper form for every full conditional distribution in a Gibbs sampler and still specify a 
joint posterior that is improper.  Hobert and Casella (1998) in a seminal paper \index{authorindex}{Hobert, J. P.}
\index{authorindex}{Casella, G.} demonstrate that resulting improper posteriors are useless for purposes of inference.  
Obviously many of these issues emanate from the (sometimes desired) specification of improper priors, which are typically 
not a problem in simple, stylized models but can present difficult algorithmic challenges in fully-developed social science 
specifications.  One common alternative (Chapter~\ref{Prior.Chapter}) is to use highly diffuse but proper priors.  This is 
usually an effective alternative but can sometimes lead to slow convergence of the Markov chain, and this should be checked.
Furthermore, if the model is truly non-identified in the classic sense (Manski 1995), then the MCMC estimation process is 
going to fail to provide useful results, even if it appears to converge (which is highly unlikely).
\index{authorindex}{Manski, C. F.}

\index{subjectindex}{perfect sampling}\index{subjectindex}{coupling from the past}%
There are basically three approaches to determining convergence for Markov chains: assessing the theoretical and mathematical 
properties of particular Markov chains, diagnosing summary statistics from in-progress models, and avoiding the issue 
altogether with perfect sampling, which uses the idea of ``coupling from the past'' to produce a sample from the exact 
stationary distribution (Propp\index{authorindex}{Propp, J. G.} and Wilson\index{authorindex}{Wilson, D. B.} 1996).  The 
emphasis in this chapter is on the second approach, whereas perfect sampling is described in
Chapter~\ref{MCMC.Extensions.Chapter}, Section~\ref{Perfect.Sampling.Section}.

The first approach is to study the mathematical properties of individual chain transition kernels, perhaps placing restrictions 
on data or analysis, and then determining in advance the total variation distance to the target distribution with some specified 
tolerance.  MCMC algorithms on discrete state space converge at a rate related to how close the second largest eigenvalue of the 
transition matrix for discrete state spaces is to one.\index{subjectindex}{Markov chain Monte Carlo (MCMC)!convergence!eigenvalue}\
 This is an elegant theoretical result that is often difficult to apply to actual sampling output.  
\index{subjectindex}{Markov chain Monte Carlo (MCMC)!convergence!mathematical properties}

The purely theoretical approach has several disadvantages: it often becomes inordinately complex mathematically, the bounds can be 
``weak''\index{subjectindex}{weak bounds on convergence time} meaning sufficiently wide as to not be useful in practice (i.e., lacking 
reasonable guidance about when to stop sampling), some of the approaches are restricted to unrealistically simple models, and generally 
the calculations are model-specific.  Nonetheless, a great deal of important work has been done here: a Gibbs sampler on variance
components models for large data size and number of parameters (Rosenthal\index{authorindex}{Rosenthal, J. S.} 1995b), transition
kernels that satisfy certain minorization conditions on the state space (Rosenthal\index{authorindex}{Rosenthal, J. S.} 1995a),
log-concave\index{subjectindex}{concavity} functions over a discretized sample space (Frieze,\index{authorindex}{Frieze, A.}
Kannan,\index{authorindex}{Kannan, R.} and Polson\index{authorindex}{Polson, N. G.} 1994), required properties of the eigenvalues
(Frigessi\index{authorindex}{Frigessi, A.} \etal 1993, Ingrassia\index{authorindex}{Ingrassia, S.} 1994), data augmentation on a
finite sample space (Rosenthal\index{authorindex}{Rosenthal, J. S.} 1993), the special case of Gaussian forms
(Amit\index{authorindex}{Amit, Y.} 1991, 1996), the conditions and rate of convergence for standard algorithms
(Roberts\index{authorindex}{Roberts, G. O.} and Polson\index{authorindex}{Polson, N. G.} 1994; 
Roberts\index{authorindex}{Roberts, G. O.} and Smith\index{authorindex}{Smith, A. F. M.} 1994); and more generally, the models
described theoretically in Meyn\index{authorindex}{Meyn, S. P.} and Tweedie\index{authorindex}{Tweedie, R. L.} (1993, 1994).
Madras and Sezer (2010) \index{authorindex}{Madras, N.} \index{authorindex}{Sezer, D.} use Steinsaltz's drift functions
for obtaining bounds on the rate of convergence on a general state space, Fort, Moulines, and Priouret (2011)
\index{authorindex}{Fort, G.} \index{authorindex}{Moulines, E.} \index{authorindex}{Priouret, P.} get convergence bounds
for new adaptive and interacting Markov chains, and Roberts and Rosenthal (2011) \index{authorindex}{Roberts, G. O.}
\index{authorindex}{Rosenthal, J. S.} extend their work on independence samplers from geometrically ergodic Markov chains to
non-geometrically ergodic Markov chains.

The second convergence assessment method involves monitoring the performance of the chain as part of the estimation process 
and making an often subjective determination about when to stop the chain.
\index{subjectindex}{Markov chain Monte Carlo (MCMC)!convergence!monitoring}  Also related to this process are efforts to 
``accelerate'' \index{subjectindex}{Markov chain Monte Carlo (MCMC)!convergence!acceleration} convergence through various 
particularistic properties.  There are quite a few of these techniques and several of the most popular and straightforward 
will be discussed here.  For additional discussion of these tools, see the review essays by Brooks
\index{authorindex}{Brooks, S. P.} (1998a), Brooks\index{authorindex}{Brooks, S. P.} and Roberts\index{authorindex}{Roberts, G. O.} 
(1999), Cowles\index{authorindex}{Cowles, M. K.} and Carlin\index{authorindex}{Carlin, B. P.} (1996), 
Mengersen,\index{authorindex}{Mengersen, K. L.} Robert,\index{authorindex}{Robert, C. P.} and Guihenneuc-Jouyaux
\index{authorindex}{Guihenneuc-Jouyaux, C.} (1999), as well as Gelfand and Sahu (1994).  
\index{authorindex}{Gelfand, A. E.} \index{authorindex}{Sahu, S. K.}

The general process is to run the chain for some conservatively large number of iterations, and then dispose of a conservatively
large proportion of the early values (say half), and then to run all of the standard empirical diagnostics. If there is any
indication from these diagnostics that the Markov chain is not in its stationary distribution then run the chain for an additional
conservatively long period and retest. This process is iterated until the researcher is convinced that there are no long concerns
from the diagnostics.  There is no ``magic'' number of iterations that makes the process conservative, but as computers get faster
something on the order of $10^4$ has become something on the order of $10^5$ or higher. Of course more complex models require more
iterations so such numbers should not be considered as a ``rule-of-thumb'' (that phrase usually presages bad advice in
statistics). Finally, after one is satisfied with the length of the chain, and have moved on to writing up the results, continue
the iterating chain in the background or on another machine for a very long time as an ``insurance run'' on the order of $10^6$
and later summarize the last $10^4$ as additional verification. Almost certainly the numerical values will be the same (within
simulation error), but this provides additional faith in the results.

It is essential to remember that the convergence diagnostics described below, as well as others in the literature, are actually 
indicators of \emph{nonconvergence}.\index{subjectindex}{nonconvergence}  That is, failing to find evidence of nonconvergence 
with these procedures is just that; it is not direct evidence of convergence.  The careful practitioner should treat encouraging 
results from one test with continued skepticism and be willing to use multiple diagnostics on any single Markov chain, any one 
of which can provide sufficient evidence of failure.  It is also important to remember that this is the process of evaluating
the integrity of the MCMC process; it does not evaluate the overall quality of the specified statistical model.

The following subsections outline the use of popular MCMC diagnostics including those provided by \boa\ and \coda, and some are also 
integrated directly into the \winbugs\ \index{subjectindex}{WinBUGS@\winbugs!diagnostics} environment.  The \R\ package \texttt{superdiag} 
(Tsai and Gill 2012) \index{authorindex}{Gill, J.}\index{Tsai, Tsung-Han} calls all of the conventional convergence diagnostics used in 
typical MCMC output assessment in one convenient \R\ statement.  Each of these diagnostics have limitations, and therefore it is 
recommended that cautious users evaluate Markov chain output with each of these.  Here these convergence diagnostics are illustrated with 
two real-data examples where one produces clean and obvious convergence assessments while the other remains problematical.  In
this way readers can see both positive and negative outcomes for comparison with their own results.

\begin{examplelist}
	\item{\bf Tobit Model for Death Penalty Support.}\label{death.penalty.example}
    \index{subjectindex}{Bayesian Tobit model|(}\index{subjectindex}{death penalty}
    \index{subjectindex}{Furman v. Georgia 1972@\emph{Furman v. Georgia} 1972}
    Norrander (2000)\index{authorindex}{Norrander, B.} uses Tobit models (Tobin 1958)\index{authorindex}{Tobin, J.} to look at
    social and political influences on U.S. state decisions to impose the death penalty since the Supreme Court ruled the practice
    constitutional in \emph{Furman v. Georgia} 1972.  
    The research question is whether the ideological, racial, and religious makeup, political culture, and urbanization are causal
    effects for state-level death sentences from 1993 to 1995.  Norrander posits a causal model whereby public opinion centrally,
    influenced by past policies and demographic factors,  determines death penalty rates by legitimating the practice over time.
    The Tobit model to account for censoring is appropriate here because 15 states did not have capital punishment provisions on
    the books in the studied period causing the actual effect of public opinion on death penalty rates to be substantively
    missing.  If these states had the legal ability to impose death penalty sentences, then it would be possible to observe
    whether there exists a relationship between the explanatory variables and the count.  For instance, the death penalty in
    murder cases in Hawaii is recorded as zero, but is unlikely to actually be zero if observable.  In addition, these data are
    also truncated at zero since states cannot impose a negative number of death penalty sentences.  

	Define now terms consistent with the discussion in Amemiya\index{authorindex}{Amemiya, T.} (1985, Chapter~10).  If $\z$ is a 
	latent outcome variable in this context with the assumptions $\z = \x\B + \E$ and $z_i \sim \mathcal{N}(\x\B,\sigma^2)$, 
	then the observed outcome variable is produced according to: $y_i = z_i$ if $z_i > 0$, and $y_i = 0$, if $z_i \le 0$.  The 
	likelihood function is then:
	\begin{equation}
		L(\B,\sigma^2|\y,\X) = \prod_{y_i=0}\left[ 1 - \text{\parbox{0.12in}{\begin{Large}${\Phi}$\end{Large}}}
						   	\left(\frac{x_i\B}{\sigma}\right) \right]
			       	\prod_{y_i>0}(\sigma^{-1})\exp\left[ -\frac{1}{2\sigma^2}(y_i - x_i\B)^2 \right].
	\end{equation}  
    \index{authorindex}{Chib, S.} \index{authorindex}{Greenberg, E.} \index{authorindex}{Albert, J. H.}
	Chib (1992) introduces a blocked Gibbs sampling estimation process for this model using data augmentation, Albert and  
	Chib (1993) extend this generally to discrete choice outcomes, and Chib and Greenberg (1998) focus on multivariate probit.  
	This is a quite natural approach since augmentation (Chapter~\ref{MCMC.Chapter}, Section~\ref{data.augmentation.section}) can 
    be done with the computationally convenient latent variable $\z$.  Furthermore, a flexible parameterization for the priors is 
    given by Gawande (1998):\index{authorindex}{Gawande, K.}
	\begin{equation}\label{R2.m24}
		\B|\sigma^2 \sim \mathcal{N}(\B_0,\I\sigma^2 B_0^{-1}) 	\qquad\qquad
		\sigma^2 \sim \mathcal{IG}\left( \frac{\gamma_0}{2},\frac{\gamma_1}{2} \right)
	\end{equation}
	with vector hyperparameter $\B_0$, scalar hyperparameters $B_0$, $\gamma_0>2$, $\gamma_1>0$, and appropriately sized identity 
	matrix $\I$.  Substantial prior flexibility can be achieved with varied levels of these parameters, although values far from
	those implied by the data will make the algorithm run very slowly.  The full conditional distributions for Gibbs sampling are 
	given for the $\B$ block, $\sigma^2$, and the individual $z_i|y_i=0$ as:
	\begin{align}\label{Gawande.Full.Conditionals}
		& \B|\sigma^2,\z,\y,\X	\sim \mathcal{N}\biggl( (B_0+\X'\X)^{-1})(\B_0 B_0+\X'\z), \nonumber\\
		& \qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad (\sigma^{-2}B_0 + \sigma^{-2} \X'\X)^{-1}) \biggr)
					\nonumber\9
		& \sigma^2|\B,\z,\y,\X	\sim \mathcal{IG}\left( \frac{\gamma_0+n}{2},
				                         	\frac{\gamma_1 + (\z-\X\B)'(\z-\X\B)}{2} \right)
					\nonumber\9
		& z_i|y_i=0,\B,\sigma,\X	\sim \mathcal{TN}(\X\B,\sigma^2)I_{(-\infty,0)},
	\end{align}
	where $\mathcal{TN}()$ denotes the truncated normal and the indicator function $I_{(-\infty,0)}$ provides the bounds of 
	truncation.  The results from this model are sensitive to values of $B_0$, and this is why it is common to see very diffuse 
	priors in this specification.  
	\begin{table}[h]
    \parbox[c]{\linewidth}{ \hspace{0.2in}
	    \tabletitle{\textsc{Posterior, Tobit Model}}\label{tobit.table1}
	    \vspace{-3pt}
            \begin{tabular}{lrrrc}
                            &  \multicolumn{1}{r}{Mean}   & \multicolumn{1}{r}{Std. Error}
                            &  \multicolumn{1}{r}{Median} & \multicolumn{1}{c}{95\% HPD Interval}    \\
	    \hline
		    \texttt{Intercept}         & -14.5451 & 3.6721 & -14.5019 & [-21.7661 : ~-7.3500] \\
		    \texttt{Past Rates}        & 171.1460 & 8.0482 & 171.1385 & [155.2004 : 186.6079] \\
		    \texttt{Political Culture} &   0.3461 & 0.1452 &   0.3438 & [~~0.0596 : ~~0.6216] \\
		    \texttt{Current Opinion}   &   3.9738 & 1.0667 &   3.9632 & [~~1.8575 : ~~6.0223] \\
		    \texttt{Ideology}          &   3.1423 & 1.1107 &   3.1436 & [~~0.9728 : ~~5.3146] \\
		    \texttt{Murder Rate}       &   0.0088 & 0.0802 &   0.0095 & [~-0.1567 : ~~0.1590] \\
	    \end{tabular}
    }
	\end{table}
	
	\index{authorindex}{Norrander, B.}
    To extend Norrander's model, we add two additional explanatory variables: state average ideology and the state-level murder
    rate.  A Gibbs sampler code in \R\ is applied using the full conditional distributions given above for $\B$, $\sigma^2$, and
    the $z_i$ using estimates from the MLE as starting points (see the {\bf Computational Addendum}).  The prior parameters for
    the inverse gamma specification are stipulated to provide a relatively diffuse form: $\gamma_0=300$, $\gamma_1=100$,
    $B_0=0.02$, and $\B = \mathbf{0}$.  Making the gamma form less diffuse than the one specified here leads to a poor mixing
    chain as generated values in the truncation range become rare.  The chain is run for $50,000$ iterations with the first
    $40,000$ values discarded as burn-in.  The marginal results summarized in Table~\ref{tobit.table1} do not generally contradict
    the original multiple analyses in Norrander (2000). \index{authorindex}{Norrander, B.} We will see through the course of this
    chapter whether such results should be trusted.\\[11pt]
    \index{subjectindex}{Bayesian Tobit model|)} \index{subjectindex}{burn-in period of a chain}
	
    \begin{table}[h]
    \parbox[c]{\linewidth}{\hspace{0.2in}
    \tabletitle{\textsc{Proportional Changes in Eastern European Militaries, 1948-1983}}\label{military.table}
	    \begin{scriptsize} \begin{tabular}{rrrrrrrrrr}
	    \multicolumn{10}{c}{$\underline{\text{Proportional Change in Military Personnel}}$}\\
     	    & Yugo. & Alb. & Bulg. & Czec. & GDR & Hung. & Poland & Rum. & USSR		\\
	    \hline
	    {\tiny 1949} & ~0.000 & ~0.083 & ~0.166 & ~0.000 & ~1.000 & ~0.571 & ~0.250 & ~0.006 & ~0.241   \\
	    {\tiny 1950} & ~0.000 & -0.077 & ~0.142 & ~0.000 & ~0.833 & ~0.909 & ~0.286 & ~0.305 & ~0.194   \\
	    {\tiny 1951} & ~0.498 & -0.083 & -0.043 & ~0.000 & ~0.864 & ~0.476 & ~0.220 & ~0.234 & ~0.163   \\
	    {\tiny 1952} & ~0.000 & ~0.109 & -0.050 & ~0.000 & ~0.244 & ~0.097 & ~0.125 & ~0.004 & ~0.160   \\
	    {\tiny 1953} & ~0.000 & -0.131 & -0.016 & ~0.000 & ~0.255 & ~0.065 & ~0.000 & ~0.004 & ~0.000   \\
	    {\tiny 1954} & ~0.000 & -0.226 & ~0.139 & ~0.000 & ~0.266 & ~0.066 & -0.333 & ~0.004 & ~0.000   \\
	    {\tiny 1955} & ~0.008 & -0.049 & ~0.000 & ~0.000 & ~0.296 & ~0.057 & ~0.000 & ~0.000 & ~0.000   \\
	    {\tiny 1956} & ~0.000 & -0.051 & -0.103 & ~0.000 & ~0.038 & ~0.054 & ~0.000 & -0.143 & -0.121   \\
	    {\tiny 1957} & ~0.000 & -0.108 & ~0.005 & ~0.000 & ~0.037 & -0.977 & ~0.000 & -0.167 & -0.118   \\
	    {\tiny 1958} & ~0.000 & -0.061 & -0.130 & ~0.000 & -0.124 & ~5.000 & ~0.000 & ~0.057 & -0.133   \\
	    {\tiny 1959} & ~0.000 & -0.129 & -0.144 & ~0.212 & -0.131 & ~0.833 & -0.333 & ~0.054 & -0.077   \\
	    {\tiny 1960} & ~0.000 & ~0.000 & -0.175 & ~0.000 & ~0.291 & ~0.455 & ~0.000 & ~0.051 & ~0.000   \\
	    {\tiny 1961} & ~0.032 & ~0.037 & ~0.017 & -0.182 & -0.099 & ~0.438 & ~0.275 & -0.022 & -0.167   \\
	    {\tiny 1962} & -0.102 & ~0.071 & ~0.125 & -0.254 & -0.150 & ~0.043 & ~0.008 & ~0.369 & ~0.200   \\
	    {\tiny 1963} & -0.114 & ~0.167 & ~0.096 & ~0.000 & ~0.365 & ~0.125 & ~0.000 & -0.056 & -0.083   \\
	    {\tiny 1964} & -0.089 & ~0.086 & ~0.115 & ~0.270 & ~0.034 & ~0.030 & ~0.058 & -0.017 & -0.039   \\
	    {\tiny 1965} & -0.108 & ~0.000 & ~0.012 & ~0.000 & ~0.017 & ~0.036 & ~0.018 & -0.085 & -0.076   \\
	    {\tiny 1966} & ~0.146 & ~0.368 & ~0.024 & ~0.085 & ~0.574 & ~0.000 & ~0.173 & -0.027 & ~0.159   \\
	    {\tiny 1967} & -0.085 & -0.019 & ~0.000 & ~0.039 & ~0.026 & -0.049 & -0.031 & -0.112 & ~0.022   \\
	    {\tiny 1968} & -0.077 & ~0.000 & ~0.012 & ~0.000 & -0.005 & ~0.000 & ~0.013 & ~0.000 & ~0.000   \\
	    {\tiny 1969} & -0.008 & -0.020 & -0.012 & ~0.000 & -0.036 & -0.058 & -0.031 & -0.135 & ~0.023   \\
	    {\tiny 1970} & ~0.084 & ~0.080 & -0.029 & -0.234 & ~0.069 & ~0.062 & -0.071 & -0.062 & -0.004   \\
	    {\tiny 1971} & -0.105 & -0.222 & -0.096 & -0.064 & -0.356 & -0.270 & -0.059 & -0.116 & -0.044   \\
	    {\tiny 1972} & ~0.000 & -0.167 & ~0.000 & ~0.000 & ~0.000 & ~0.000 & ~0.000 & ~0.125 & ~0.000   \\
	    {\tiny 1973} & ~0.043 & ~0.086 & ~0.000 & ~0.000 & ~0.000 & ~0.000 & ~0.037 & -0.056 & ~0.015   \\
	    {\tiny 1974} & -0.042 & ~0.000 & ~0.000 & ~0.053 & ~0.154 & ~0.000 & ~0.071 & ~0.000 & ~0.029   \\
	    {\tiny 1975} & ~0.000 & ~0.000 & ~0.000 & ~0.000 & -0.067 & ~0.100 & ~0.000 & ~0.000 & ~0.014   \\
	    {\tiny 1976} & ~0.087 & ~0.237 & ~0.133 & -0.100 & ~0.143 & -0.091 & ~0.000 & ~0.059 & ~0.020   \\
	    {\tiny 1977} & ~0.040 & -0.043 & -0.118 & ~0.000 & ~0.000 & ~0.000 & ~0.033 & ~0.000 & ~0.008   \\
	    {\tiny 1978} & ~0.038 & -0.089 & ~0.000 & ~0.056 & ~0.000 & ~0.100 & ~0.000 & ~0.000 & -0.011   \\
	    {\tiny 1979} & -0.037 & ~0.049 & ~0.000 & ~0.000 & ~0.000 & -0.091 & ~0.032 & ~0.000 & ~0.005   \\
	    {\tiny 1980} & ~0.000 & -0.047 & ~0.000 & ~0.053 & ~0.013 & -0.070 & ~0.000 & ~0.028 & -0.025   \\
	    {\tiny 1981} & -0.027 & ~0.049 & -0.007 & -0.030 & ~0.031 & ~0.086 & ~0.000 & ~0.000 & ~0.028   \\
	    {\tiny 1982} & -0.008 & ~0.000 & -0.007 & ~0.015 & -0.006 & ~0.050 & ~0.000 & -0.022 & ~0.011   \\
	    {\tiny 1983} & -0.044 & -0.070 & ~0.095 & ~0.041 & ~0.006 & -0.009 & ~0.063 & ~0.050 & -0.043   \\
	    \hline\\
	    \end{tabular}\end{scriptsize}
        \vspace{11pt}
    }
    \end{table}
	
	\item{\bf A Normal-Hierarchical Model of Cold War Military Personnel}
	\label{Military.Example}\index{subjectindex}{example!Warsaw Pact military expenditures}
	These data describe changes in military personnel for seven Warsaw Pact countries plus two (Yugoslavia and Albania) 
	during the period from 1948 to 1983, an interval that covers the height of the Cold War.  These are collected by Faber
	\index{authorindex}{Faber, J.} (1989, ICPSR-9273) for 78 countries total, from which these 9 are taken, and include 
	covariates for military, social, and economic conditions.
	
	The model is a normal-gamma hierarchy with several levels and somewhat vague normal and inverse gamma hyperprior 
	specifications.  While there is a good argument to treat these data as a time-series, we will not do that here in 
	order to produce illustrative results from a diagnostic perspective.  The inverse gamma specification is articulated 
	from the gamma statements through the \bugs\ \index{subjectindex}{bugs@\bugs!precisions} convention of specifying 
	precisions instead of variances in normal specifications.  The motivation for the hierarchy is that the context 
	level recognizes national differences from economic and cultural factors, while the primary level recognizes the 
	political influence that comes from Warsaw Pact membership as well as the obvious pressure exerted by the USSR.  

	The (possibly mis-specified) model is a hierarchy of normals according to:\\
	\hspace{-2in} 
	\parbox[l]{\linewidth}{ 
	\begin{align}\label{military.specification}
	    \,\,\,Y_{ij} &\sim \mathcal{N}(\alpha_i + \beta_{1i}(X_j) + \beta_{2i}(X_j^2) + \beta_{3i}\cos(X_j),\tau_c) & \qquad\qquad & \quad \nonumber
	\end{align}
	} \vspace{-22pt}
	\begin{align}
	    \alpha_i   &\sim \mathcal{N}(\alpha_{\mu},\alpha_{\tau})   & \alpha_{\mu}  \sim \mathcal{N}(1,0.1) && \alpha_{\tau}  \sim \mathcal{G}(1,0.1) \nonumber\\
	    \beta_{1i} &\sim \mathcal{N}(\beta_{1,\mu},\beta_{1,\tau}) & \beta_{1,\mu} \sim \mathcal{N}(0,0.1) && \beta_{1,\tau} \sim \mathcal{G}(1,0.1) \nonumber\\
	    \beta_{2i} &\sim \mathcal{N}(\beta_{2,\mu},\beta_{2,\tau}) & \beta_{2,\mu} \sim \mathcal{N}(0,0.1) && \beta_{2,\tau} \sim \mathcal{G}(1,0.1) \nonumber\\
	    \beta_{3i} &\sim \mathcal{N}(\beta_{3,\mu},\beta_{3,\tau}) & \beta_{3,\mu} \sim \mathcal{N}(0,0.1) && \beta_{3,\tau} \sim \mathcal{G}(1,0.1) \nonumber\\
	    \tau_c     &\sim \mathcal{G}(1,0.1), &&
	\end{align}
	where the $Y_{ij}$ are proportional changes in military personnel for country $i$ at time period $j$ and $X_j$ is the 
	index of the year (rows in the table).  In addition to the linear treatment of the years, there are squared and
	cosine terms to account for systematic fluctuations over time.  While the posterior distribution of the linear 
	coefficient, $\beta_1$ is of primary interest, these secondary coefficients may reveal additional structures in
	the data.  The $\tau$ terms and normal variances here conform to the \bugs\ requirement, 
	mentioned above, of specifying precisions instead of variances.  It should be clear from the hyperparameters that the 
	prior structure is given proper distributions in this setup.  Interestingly, the \winbugs\ examples routinely assign 
	very small parameter values for the gamma distribution priors ($\alpha=0.001, \beta=0.001$) on precisions, which puts 
	prior density on gigantic values for the variance that exceed the typical posterior ranges for reasonable models.
	This practice should be replaced with other forms such as a gamma with $\alpha=1$ and small $\beta$ (Gill 2010) 
	\index{authorindex}{Gill, J.} or a (folded) half-$t$ (Gelman 2006).\index{authorindex}{Gelman, A.}
	
	The \jags\ code for this example is given in the {\bf Computational Addendum} for this chapter, and the data 
	are given here (starting at 1949 since the variable has been changed from absolute numbers to annual proportional 
	change).  After compilation in \jags, 150,000 iterations of the Markov chain are run and discarded.
	\index{subjectindex}{burn-in period of a chain}  Monitoring is turned on for all nodes and then an additional 
	350,000 iterations are run.  This Markov chain turns out to be relatively slow mixing and there are several lurking
	problems.  The posterior results are summarized in Table~\ref{Military.Summary.Table}.  Clearly these results are 
	mixed evidence of the value of the specification given in \eqref{military.specification} since the standard errors
	indicate rather diffuse posterior forms.  Interestingly, the quadratic term is the most reliable predictor, although
	it weakens in the absence of the linear and trigonometric contributions.

	\begin{table}[h]
	\blstable
	\tabletitle{\textsc{Posterior, Military Personnel Model}}\label{Military.Summary.Table}
	\begin{tabular}{lrrrc}
                	&  \multicolumn{1}{r}{Mean}   & \multicolumn{1}{r}{Std. Error}
			&  \multicolumn{1}{r}{Median} & \multicolumn{1}{c}{95\% HPD Interval}  	\\
	\hline
		$\alpha_{\mu}$   &   0.7404 &  3.3585 &   0.9814 & [-5.8422 : ~~7.3229] \\ 
  		$\alpha_{\tau}$  &  10.0155 &  9.9068 &   7.0303 & [-9.4019 : ~29.4328] \\ 
  		$\beta_{1,\mu}$  &   0.0965 &  0.4740 &   0.0983 & [-0.8326 : ~~1.0256] \\ 
  		$\beta_{1,\tau}$ &  13.3070 & 10.8181 &  10.3000 & [-7.8965 : ~34.5105] \\ 
  		$\beta_{2,\mu}$  &   2.0737 &  0.1031 &   2.0643 & [~1.8717 : ~~2.2758] \\ 
  		$\beta_{2,\tau}$ &  24.6509 &  6.9466 &  23.9347 & [11.0357 : ~38.2662] \\ 
  		$\beta_{3,\mu}$  &  -0.0011 &  0.0130 &  -0.0011 & [-0.0266 : ~~0.0243] \\ 
  		$\beta_{3,\tau}$ & 180.0586 & 42.5261 & 176.7230 & [96.7075 : 263.4097] \\ 
  		$\tau_c$         &   0.0308 &  0.0038 &   0.0307 & [~0.0233 : ~~0.0383] \\ 
	\end{tabular} 
	\end{table} 
\end{examplelist}
	
\subsection{Autocorrelation} \index{subjectindex}{Markov chain!autocorrelation}\label{autocorrelation.section}
High correlation \emph{between} the parameters of a chain tends to produce slow convergence, whereas high 
correlation within a single parameter (autocorrelation) chain leads to slow mixing and possibly individual 
\emph{nonconvergence} to the limiting distribution because the chain will tend to explore less space in finite 
time.\index{subjectindex}{within-chain versus cross-chain correlation}  This is a problem since, of course, 
all chains are run in finite time.  Furthermore, these are obviously interrelated problems for most 
specifications.

In addition to the empirical mean and standard deviation of the chain values (the primary inferential quantities of interest), the
output from \coda\ gives the so-called ``na\"ive'' standard error \emph{of the mean} (\texttt{NaiveSE}, which is the square root
of $\sqrt{\text{sample variance}}/\sqrt{n}$, and the standard time-series adjusted standard error \emph{of the mean}
(\texttt{TimeseriesSE}), which is $\sqrt{\text{spectral density var}}/\sqrt{n}$ $= \text{asymptotic SE}$.   The na\"ive standard
error gives an overly-optimistic view (smaller) of the posterior dispersion since it ignores serial correlation that exists by
definition with Markov chains.  The time-series standard error is produced from an estimate of the spectral density at zero using
binned chain values.

One method works around the autocorrelation problem by looking at the means of batches of the parameter. If the 
batch size is large enough, the batch means should be approximately uncorrelated and the normal formula for 
computing the standard error should work.  More specifically, a quantity of interest from the simulated values, 
$h(\theta)$, is calculated empirically from a series $i=1,2,\ldots, n$:
\begin{equation}\index{subjectindex}{Monte Carlo!mean}\index{subjectindex}{spectral density} 
    	\hat{h}(\theta) = \frac{1}{n}\sum_{i=1}^{n}h(\theta_i) 
\end{equation}
with the associated Monte Carlo variance:
\begin{equation}\index{subjectindex}{Monte Carlo!variance}
	\Var[h(\theta)]	= \frac{1}{n}\left( \frac{1}{n-1}\sum_{i=1}^{n} (h(\theta_i) - \hat{h}(\theta))^2 \right),
\end{equation}
as described in Chapter~\ref{MC.Chapter}.  The problem with applying this to MCMC samples is that it ignores the 
serial nature of their generation.  Instead consider specifically incorporating autocorrelation in the variance 
calculation.  Define first for a lag $t$, the autocovariance:
\begin{equation}
	\rho_t = \Cov\left[ h(\T_i),h(\T_{i-t}) \right].
\end{equation}
Chen \etal (2000, 87) show that the expected variance from MCMC samples, knowing $\rho_t$ and $\sigma^2$ (the true 
variance of $h(\theta)$) is:
\begin{equation}
	E[\sigma^2] = \sigma^2 \left[ 1 - \frac{2}{n-1} \sum_{t=1}^{n-1} \left(1 - \frac{t}{n}\right)\rho_t \right],
\end{equation}
which shows the variance difference produced by serially dependent iterates.

In practice the lagged autocovariance is measured by: \index{subjectindex}{lagged autocovariance}
\begin{equation}\label{lagged.autocovariance}
	\hat{\rho}_{nt} = \frac{1}{n}\sum_{t+1}^{n}
		\left[ h(\theta_i) - \hat{h}(\theta) \right] \left[ h(\theta_{i-t}) - \hat{h}(\theta) \right] 
\end{equation}
(Priestley 1981, p.323).\index{authorindex}{Priestley, M. B.}

In analyzing Markov chain autocorrelation, it is helpful to identify specific lags here in order to calculate the long-run trends
in correlation, and in particular whether they decrease with increasing lags.  Diagnostically, though, it is rarely necessary to
look beyond 30 to 50 lags.  Recall that for a series of length $n$, the lag $k$ autocorrelation is the sum of $n-k$ correlations
according to: $\rho_k = \sum_{i=1}^{n-k} (x_i-\bar{x}) (x_{i+k}-\bar{x})/\sum_{i=1}^{n} (x_i-\bar{x})^2$.  

Fortunately, both \boa\index{subjectindex}{boa@\boa!autocorrelations} and \coda\index{subjectindex}{coda@\coda!autocorrelations} 
give straightforward diagnostic summaries and graphical displays of autocorrelation with chains and cross-correlation matrices.  
The four default lag values of the correlations for the Tobit model of death penalty attitudes is given in Table~\ref{Tobit.Corr}.
Notice that the within-chain correlations decline sharply with increasing lag, indicating no problem autocorrelation in
any of these dimensions of the Markov chain.  However, there are some large cross-correlations that might cause the chain to
mix poorly due to constrained high-density space between two parameters.  The graphical diagnostics will reveal if this is really
a problem in mixing.

\begin{comment}
\begin{center}
\begin{table}[h]
\blstable
\tabletitle{\textsc{Correlations and Autocorrelations, Tobit Model}}\label{Tobit.Corr}
\vspace{6pt}
\begin{center}
\begin{scriptsize}
\begin{tabular}{lrrrrrrrrrr}
	        & \multicolumn{4}{c}{$\underline{\text{Within-Chain Correlations}}$} 
	        & \multicolumn{5}{c}{$\underline{\text{Cross-Chain Correlations}}$} \\
                & \multicolumn{1}{c}{}  &  \multicolumn{1}{c}{} 
	        & \multicolumn{1}{c}{} &  \multicolumn{1}{c}{}
                & \multicolumn{1}{c}{}  
		& \multicolumn{1}{c}{\texttt Past} 
		& \multicolumn{1}{c}{\texttt Political} 
		& \multicolumn{1}{c}{\texttt Current} 
		& \multicolumn{1}{c}{} 
		& \multicolumn{1}{c}{\texttt Murder} \\
                & \multicolumn{1}{c}{Lag 1}  &  \multicolumn{1}{c}{Lag 5} 
	        & \multicolumn{1}{c}{Lag 10} &  \multicolumn{1}{c}{Lag 50}
                & \multicolumn{1}{c}{\texttt Intercept}  
		& \multicolumn{1}{c}{\texttt Rates} 
		& \multicolumn{1}{c}{\texttt Culture} 
		& \multicolumn{1}{c}{\texttt Opinion} 
		& \multicolumn{1}{c}{\texttt Ideology} 
		& \multicolumn{1}{c}{\texttt Rates} \\
\hline \hline
\texttt{Intercept}         & 0.205 &  0.003 &  0.011 & -0.0016 &  1.000 & -0.120 &  0.527 & -0.086 & -0.975 & -0.241 \\
\texttt{Past Rates}        & 0.030 & -0.006 &- 0.005 & -0.0156 & -0.120 &  1.000 & -0.167 & -0.102 &  0.146 & -0.123 \\
\texttt{Political Culture} & 0.218 & -0.003 &- 0.007 & -0.0065 &  0.527 & -0.167 &  1.000 &  0.022 & -0.587 & -0.649 \\
\texttt{Current Opinion}   & 0.295 &  0.007 &  0.001 & -0.0183 & -0.086 & -0.102 &  0.022 &  1.000 & -0.089 &  0.139 \\
\texttt{Ideology}          & 0.201 & -0.001 &  0.012 &  0.0049 & -0.975 &  0.146 & -0.587 & -0.089 &  1.000 &  0.182 \\
\texttt{Murder Rate}       & 0.212 &  0.014 & -0.004 &  0.0001 & -0.241 & -0.123 & -0.649 &  0.139 &  0.182 &  1.000 \\
\end{tabular} \end{scriptsize} \end{center} \end{table} \end{center}
\end{comment}

\begin{table}[h]
\parbox[c]{\linewidth}{
    \blstable
    \tabletitle{\textsc{Correlations and Autocorrelations, Tobit Model}}\label{Tobit.Corr}
    \vspace{6pt}
    \begin{tabular}{lrrrrrr}
                & \multicolumn{1}{c}{}  
		& \multicolumn{1}{c}{\texttt Past} 
		& \multicolumn{1}{c}{\texttt Political} 
		& \multicolumn{1}{c}{\texttt Current} 
		& \multicolumn{1}{c}{} 
		& \multicolumn{1}{c}{\texttt Murder} \\[-3pt]
                & \multicolumn{1}{c}{\texttt Intercept}  
		& \multicolumn{1}{c}{\texttt Rates} 
		& \multicolumn{1}{c}{Culture} 
		& \multicolumn{1}{c}{Opinion} 
		& \multicolumn{1}{c}{Ideology} 
		& \multicolumn{1}{c}{Rate} \\
    \hline 
	        & \multicolumn{6}{c}{$\underline{\text{Within-Chain Correlations}}$} \\
        Lag 1		  &  0.205  &  0.030  &  0.218  &  0.295  &  0.201  &  0.212 \\
        Lag 5		  &  0.003  & -0.006  & -0.003  &  0.007  &  0.001  &  0.014 \\
        Lag 10		  &  0.011  & -0.005  & -0.007  &  0.002  &  0.023  & -0.004 \\
        Lag 50		  & -0.0016 & -0.0156 & -0.0065 & -0.0183 &  0.0049 &  0.0001 \\
    \hline
	            & \multicolumn{6}{c}{$\underline{\text{Cross-Chain Correlations}}$} \\
        \texttt{Intercept}         &  1.000 & -0.120 &  0.527 & -0.086 & -0.975 & -0.241 \\
        \texttt{Past Rates}        &        &  1.000 & -0.167 & -0.102 &  0.146 & -0.123 \\
        \texttt{Political Culture} &        &        &  1.000 &  0.022 & -0.587 & -0.649 \\
        \texttt{Current Opinion}   &        &        &        &  1.000 & -0.089 &  0.139 \\
        \texttt{Ideology}          &        &        &        &        &  1.000 &  0.182 \\
        \texttt{Murder Rate}       &        &        &        &        &        &  1.000 \\
    \end{tabular} 
}
\end{table} 

The model correlation structure of the path of the chain for the Eastern European military personnel example is summarized in
Table~\ref{Military.Corr} along with cross-correlations.  Unfortunately, the picture is not as optimistic as in the last example.
The autocorrelations for $\alpha_{\mu}$, $\beta_{1,\mu}$, $\beta_{2,\mu}$, and $\tau_c$ are unambiguously indicative of poor
mixing.  \index{subjectindex}{Markov chain!autocorrelation}\index{subjectindex}{convergence diagnostic!traceplot}  It is possible
to look at longer lags for these variables, but the lag of 50 is sufficient evidence here from the marginals to show that the
overall chain is not mixing well.  The other variables show Markov chain autocorrelations that are very typical of fair to good
mixing.  Interestingly, although some within-chain correlations are quite high, there is little evidence that correlation between
chains is slowing down the mixing.  Only $\beta_{2,\mu}$ and $\tau_c$ appear to be highly (negatively) correlated.  One of the
main weapons for dealing with high autocorrelations is reparameterization as described in Section~\ref{Reparameterization.Section}.

\begin{comment}
\begin{center}
\begin{table}[h]
\blstable
\tabletitle{\textsc{Correlations and Autocorrelations, Military Personnel Model}}\label{Military.Corr}
\vspace{6pt}
\begin{center}
\begin{scriptsize}
\begin{tabular}{lrrrrrrrrrrrrr}
	        & \multicolumn{4}{c}{$\underline{\text{Within-Chain Correlations}}$} 
	        & \multicolumn{9}{c}{$\underline{\text{Cross-Chain Correlations}}$} \\
                & \multicolumn{1}{c}{Lag 1}  &  \multicolumn{1}{c}{Lag 5} 
	        & \multicolumn{1}{c}{Lag 10} &  \multicolumn{1}{c}{Lag 50}
                & \multicolumn{1}{c}{$\alpha_{\mu}$}  & \multicolumn{1}{c}{$\alpha_{\tau}$} 
		& \multicolumn{1}{c}{$\beta_{1,\mu}$} & \multicolumn{1}{c}{$\beta_{1,\tau}$}  
		& \multicolumn{1}{c}{$\beta_{2,\mu}$} & \multicolumn{1}{c}{$\beta_{2,\tau}$}  
		& \multicolumn{1}{c}{$\beta_{3,\mu}$} & \multicolumn{1}{c}{$\beta_{3,\tau}$}  
		& \multicolumn{1}{c}{$\tau_c$} \\
\hline \hline
$\alpha_{\mu}$   & 0.998  & 0.998  & 0.991  & 0.962  & 1.000 & 0.020 &-0.034 & 0.003 & 0.201 &-0.023 &-0.003 & 0.000 &-0.184 \\
$\alpha_{\tau}$  & 0.896  & 0.622  & 0.441  & 0.075  &       & 1.000 & 0.003 & 0.006 & 0.017 & 0.004 & 0.006 &-0.002 &-0.012 \\
$\beta_{1,\mu}$  & 0.981  & 0.911  & 0.836  & 0.444  &       &       & 1.000 &-0.008 & 0.024 &-0.007 & 0.004 &-0.002 &-0.016 \\
$\beta_{1,\tau}$ & 0.871  & 0.537  & 0.323  & 0.009  &       &       &       & 1.000 &-0.017 & 0.007 & 0.003 & 0.002 & 0.024 \\
$\beta_{2,\mu}$  & 0.883  & 0.883  & 0.882  & 0.881  &       &       &       &       & 1.000 &-0.100 &-0.006 & 0.003 &-0.682 \\
$\beta_{2,\tau}$ & 0.304  & 0.284  & 0.284  & 0.283  &       &       &       &       &       & 1.000 & 0.003 & 0.001 & 0.097 \\
$\beta_{3,\mu}$  &-0.001  &-0.001  &-0.001  & 0.002  &       &       &       &       &       &       & 1.000 & 0.005 & 0.002 \\
$\beta_{3,\tau}$ & 0.023  & 0.003  &-0.004  &-0.002  &       &       &       &       &       &       &       & 1.000 &-0.002 \\
$\tau_c$         & 0.587  & 0.534  & 0.533  & 0.531  &       &       &       &       &       &       &       &       & 1.000 \\
\end{tabular} \end{scriptsize} \end{center} \end{table} \end{center}
\end{comment}

\begin{center}
\begin{table}[h]
\blstable
\tabletitle{\textsc{Correlations and Autocorrelations, Military Personnel Model}}\label{Military.Corr}
\vspace{6pt}
\begin{center}
\begin{tabular}{lrrrrrrrrr}
                & \multicolumn{1}{c}{$\alpha_{\mu}$}  & \multicolumn{1}{c}{$\alpha_{\tau}$} 
		& \multicolumn{1}{c}{$\beta_{1,\mu}$} & \multicolumn{1}{c}{$\beta_{1,\tau}$}  
		& \multicolumn{1}{c}{$\beta_{2,\mu}$} & \multicolumn{1}{c}{$\beta_{2,\tau}$}  
		& \multicolumn{1}{c}{$\beta_{3,\mu}$} & \multicolumn{1}{c}{$\beta_{3,\tau}$}  
		& \multicolumn{1}{c}{$\tau_c$} \\
\hline 
	        & \multicolumn{9}{c}{$\underline{\text{Within-Chain Correlations}}$} \\
Lag 1            & 0.998 & 0.896 & 0.981 & 0.871 & 0.883 & 0.304 &-0.001 & 0.023 & 0.587 \\
Lag 5            & 0.998 & 0.622 & 0.911 & 0.537 & 0.883 & 0.284 &-0.001 & 0.003 & 0.534 \\
Lag 10           & 0.991 & 0.441 & 0.836 & 0.323 & 0.882 & 0.284 &-0.001 &-0.004 & 0.533 \\
Lag 50           & 0.962 & 0.075 & 0.444 & 0.009 & 0.881 & 0.283 & 0.002 &-0.002 & 0.531 \\
\hline
	         & \multicolumn{9}{c}{$\underline{\text{Cross-Chain Correlations}}$} \\
$\alpha_{\mu}$   & 1.000 & 0.020 &-0.034 & 0.003 & 0.201 &-0.023 &-0.003 & 0.000 &-0.184 \\
$\alpha_{\tau}$  &       & 1.000 & 0.003 & 0.006 & 0.017 & 0.004 & 0.006 &-0.002 &-0.012 \\
$\beta_{1,\mu}$  &       &       & 1.000 &-0.008 & 0.024 &-0.007 & 0.004 &-0.002 &-0.016 \\
$\beta_{1,\tau}$ &       &       &       & 1.000 &-0.017 & 0.007 & 0.003 & 0.002 & 0.024 \\
$\beta_{2,\mu}$  &       &       &       &       & 1.000 &-0.100 &-0.006 & 0.003 &-0.682 \\
$\beta_{2,\tau}$ &       &       &       &       &       & 1.000 & 0.003 & 0.001 & 0.097 \\
$\beta_{3,\mu}$  &       &       &       &       &       &       & 1.000 & 0.005 & 0.002 \\
$\beta_{3,\tau}$ &       &       &       &       &       &       &       & 1.000 &-0.002 \\
$\tau_c$         &       &       &       &       &       &       &       &       & 1.000 \\
\end{tabular} \end{center} \end{table} \end{center}

\index{subjectindex}{convergence diagnostic!traceplot|(}
\index{subjectindex}{convergence diagnostic!histogram|(}
\subsection{Graphical Diagnostics}\index{subjectindex}{convergence diagnostic!graphical}
Graphical diagnostics can be very useful in evaluating mixing and convergence of the chain.  While they are not
a formal test, as we will do in Section~\ref{diagnostic.tests}, they often show stochastic properties of importance.
Figures~\ref{Tobit.Traceplots} and \ref{Military.Traceplots} simultaneously provide two common visual diagnostics: 
traceplots of the path of the Gibbs sampler runs (with the burn-in period omitted to make the scale more readable), 
and histograms for the chains over this same period on the same vertical scale.  Traceplots merely follow the path
of the Markov chain over time on the x-axis, giving the consecutive mixing through the support of the posterior
space on the y-axis.  Because ``time'' is accounted for moving left to right, we can see the properties of the
mixing of the chain.

\begin{figure}[h!]
\parbox[c]{\linewidth}{ 
	\centerline{ \epsfig{file=Images/mcmc_utilitarian.figure07.ps,height=5.2in,width=5.6in,clip=,angle=0} } }
	\vspace{-11pt}
	\caption{\textsc{Traceplot and Histogram Graphics, Tobit Model}}\label{Tobit.Traceplots}
	\vspace{-11pt}
\end{figure}

In every case, the traceplots for the death penalty analysis show ideal properties: free travel up and down through the space and a
flat trend across the window.  This ``fuzzy caterpillar'' pattern is the desired outcome from looking at traceplots because it
implies (not proves!) that the Markov chain is in its stationary distribution and is exploring it fully.  The histograms show a
strong central limit theorem \index{subjectindex}{central limit theorem} effect from the ergodicity of the Markov chain.  This
complete graphical effect from Figure~\ref{Tobit.Traceplots} is the desired result.

The graphics contained in Figure~\ref{Military.Traceplots} for the Military Personnel model are not as promising.
\index{subjectindex}{burn-in period of a chain}The first traceplot shows a pattern called ``snaking'' for 
$\alpha_{mu}$, indicating poor mixing since the chain does not move freely through the sample space.  However, 
the traceplots for $\beta_{1,\mu}$, $\beta_{3,\mu}$, and $\beta_{3,\tau}$ show a non-trending (flat trajectory) 
pattern where the chain is also making liberally wide moves through the sample space.  Two variance components,
$\alpha_{\tau}$ and $\beta_{1,\tau}$ are somewhat long-tailed as evidenced by both the traceplots and the histograms.
This suggests potential, but not certain, issues with the model specification.  It is obvious that the problematic
dimensions are those for $\beta_{2,\mu}$, and $\tau_c$, where strong evidence of trending exists.  This indicates
that the chains are likely not to be in their stationary distribution since the time period is quite long (150,000
iterations).

\begin{figure}[h!]
\parbox[c]{0.8\linewidth}{ \hspace{0.5in}
	\centerline{ \epsfig{file=Images/mcmc_utilitarian.figure05.ps,height=5.5in,width=5.8in,clip=,angle=270} } }
	\vspace{11pt}
	\caption{\textsc{Traceplot and Histogram Graphics, Military Personnel Model}}\label{Military.Traceplots}
\end{figure}

One general problem with traceplots is that if the chain remains attracted to a nonoptimal mode
\index{subjectindex}{nonoptimal posterior regions} for a long period of time, there is no visual indication that 
this area is not the \index{subjectindex}{convergence diagnostic!traceplot} desired high-density region.  The 
standard solutions are to extend the run for a very long time since eventually it will leave (e.g., Geweke 
\index{authorindex}{Geweke, J.} 1992), and to start the Markov chain from multiple widely dispersed starting points 
(e.g., Gelman\index{authorindex}{Gelman, A.} and Rubin\index{authorindex}{Rubin, D. B.} 1992a).  Also, due to a 
feature in \winbugs\ \index{subjectindex}{WinBUGS@\winbugs!traceplot} that keeps a dynamic running traceplot going 
in a separate window, it is easy to use this as a default convergence assessor.  Yet \winbugs\ automatically 
resizes the $Y$-axis as the chain runs for this graphic, giving an inattentive viewer the illusion of stability.  
So the ``History'' option is superior for real traceplot diagnostic purposes in the \winbugs\ environment.  

One important warning about traceplots is warranted here.  It is common practice to run multiple parallel chains from different
and dispersed starting points as a means of assessing convergence (Section~\ref{gelman.rubin.diag}).  However, graphing traceplots
for the full span of these chains can be very misleading because the starting, or early, values are quite far from the region
where the chain will eventually settle.  So the scale of the graph hides features of the chain inside a densely plotted and narrow
band of line segments.  As an example, Figure~\ref{Different.Traceplots} shows a single dimension of the same Markov chain where
the first panel covers iterations 1 through 50,000 and the second panel shows the last 500 iterations only.  The longer view
implies a great deal of stability and good mixing as well because the scale of the y-axis covers the large distance between the
starting point at 20 and the stable region centered around 0.272.  The plot of the last 500 iterations shows that there is
considerable amount of snaking going on within the dense view on the left showing that the mixing is not very efficient.  
Unfortunately the longer view with distant starting points and small, dense stable regions can be found in published analyses and
are sometimes used by researchers as a confirmation of stability and mixing.

\begin{figure}[h!]
\parbox[c]{0.8\linewidth}{ \hspace{0.65in}
	\centerline{ \epsfig{file=Images/mcmc_utilitarian.figure08.ps,height=3.5in,width=5.8in,clip=,angle=0} } }
	\vspace{1pt}
	\caption{\textsc{Traceplots of the Same Chain with Different Spans}}\label{Different.Traceplots}
\end{figure}

\index{subjectindex}{convergence diagnostic!traceplot|)}
\index{subjectindex}{convergence diagnostic!histogram|)}

\index{subjectindex}{convergence diagnostic!running mean|(}
\index{subjectindex}{convergence diagnostic!density plot|(}
One very useful diagnostic is the running mean graph, which, as its name implies, gives the mean across previous values running
along the figure from left to right.  Thus a stable chain fluctuates in its early period due to a small denominator and then ends
up as a flat line.  This is not iron-clad evidence of convergence because a chain may just be lingering for a long period of time
at some sub-optimal attraction, but it is strong evidence given sufficiently lengthy evaluation periods.
Figure~\ref{Tobit.Mean.Fig} gives the running means for the last $10,000$ values for the death penalty support model, and supports
the claim of convergence for each parameter in the boxes on the left.  Notice that each chain settles down to a stable horizontal
trend.  The right side gives kernel density estimates of the chain values and can be considered a smoothed version of the
histograms in Figure~\ref{Tobit.Traceplots}.  Underneath each density plot is a 95\% HPD interval for the last half of the
analyzed chain values for this plot.  The values of 0.95 for the HPD interval and 0.5 for the window width can be changed in the
function \texttt{mean.kernel.mcmc} to other user-selected values.  This function graphical and the function for traceplots with
histograms are both included in the \texttt{BaM} package in \R\ that accompanies this book.

The idea behind an interval summary of a subset of the values used to make the density plot is that trending in the Markov chain
should cause this interval to be not centered in the plot (skewed distributions will still have equal tails).  These intervals in
Figure~\ref{Tobit.Mean.Fig} show no sign of problems.

\begin{figure}[h!]
\parbox[l]{\linewidth}{ \hspace{0.05in}
	\centerline{ \epsfig{file=Images/mcmc_utilitarian.figure03.ps, height=5.5in,width=5.2in,clip=,angle=0} } }
	\vspace{-4pt}
	\caption{\textsc{Running Mean Diagnostic, Tobit Model}}\label{Tobit.Mean.Fig}
\end{figure}

\index{subjectindex}{burn-in period of a chain}
The left-hand side of Figure~\ref{Military.Mean.Fig} gives the same  running mean diagnostic seen before with $150,000$ 
simulated values after the burn-in period for the Military Personnel model.  The graph is designed to contrast the two 
views: long-term stability and distributional summary.  In particular $\beta_{2,\mu}$, $\beta_{,2\tau}$, and $\tau_c$ 
look untrustworthy in the first column of the figure since they appear not to have settled into a stable running mean.  
This is interesting since the Markov chain for $\beta_{2,\tau}$ appeared to be reasonably stable in the latter part of 
the traceplot.  \index{subjectindex}{HPD region} Robert\index{authorindex}{Robert, C. P.} and Mengersen
\index{authorindex}{Mengersen, K. L.} (1999) use a similar diagnostic to show poor convergence properties in a normal 
mixture model.  \index{subjectindex}{mixture model!normals}  

\begin{figure}[h!]
	\centerline{ \hspace{0.05in} 
        \epsfig{file=Images/mcmc_utilitarian.figure02.ps, height=5.5in,width=5.7in,clip=,angle=0} } 
	    \vspace{6pt}
	    \caption{\textsc{Running Mean Diagnostic, Military Personnel Model}}\label{Military.Mean.Fig}
\end{figure}
\index{subjectindex}{convergence diagnostic!running mean|)}
\index{subjectindex}{convergence diagnostic!density plot|)}

There are a number of other graphical techniques that often give useful diagnostic information.  Gelman,\index{authorindex}{Gelman, A.} 
Gilks,\index{authorindex}{Gilks, W. R.} and Roberts\index{authorindex}{Roberts, G. O.} (1996) give the theoretical result that 
Metropolis-Hastings algorithms with a low acceptance rate will be less stable and poor at mixing through the distribution due to this 
inefficiency.\index{subjectindex}{Markov chain!mixing}  Therefore a serial plot of the acceptance rate over time can be helpful in 
showing the performance of the chain.\index{subjectindex}{convergence diagnostic!Metropolis-Hastings acceptance rate}  

Other graphical methods exist. Robert\index{authorindex}{Robert, C. P.} (1997) introduces \emph{allocation maps} 
\index{subjectindex}{convergence diagnostic!allocation maps} as a way of assessing convergence in mixture models.  The idea is to 
represent each component with a different level of gray.  Sudden and unambiguous shifts in color across increasing iterations is 
evidence of nonconvergence.  Cui\index{authorindex}{Cui, L.} \etal (1992) and Tanner\index{authorindex}{Tanner, M. A.}  (1996, p.154) 
use a simple version of this idea on the $\theta$ parameter of a ``witch's hat'' distribution\index{subjectindex}{witch's hat distribution} 
(see the discussions in Chapter~\ref{MCMC.Extensions.Chapter}) to show a sudden shift in chain behavior.  

\subsection{Standard Empirical Diagnostics}\index{subjectindex}{converge diagnostic!empirical|(}\label{diagnostic.tests}
This section first reviews in detail the four empirical diagnostics used most commonly in practice, giving results for our two running 
models of interest.  These diagnostics, Geweke, Gelman-Rubin, Raftery-Lewis, and Heidelberger-Welch (all named after their creators), 
dominate because they are provided in easy-to-use menu form in the \R\ packages \boa\ and \coda.  Subsequent discussion, though, will 
present some interesting and useful alternatives that require user-coding.

\subsubsection{The Geweke Time-Series Diagnostic}\index{subjectindex}{convergence diagnostic!Geweke|(}
\index{subjectindex}{burn-in period of a chain} \index{subjectindex}{difference of means test} 
Geweke\index{authorindex}{Geweke, J.} (1992) develops a test based on comparing some proportion of the early era of the chain after the 
burn-in period with some nonoverlapping proportion of the late era of the chain.  He proposes a difference of means test using an asymptotic 
approximation for the standard error of the difference (although other summary statistics could be compared if desired).  Since the 
test statistic is asymptotically standard normal, values that are atypical of a standard normal distribution provide evidence that the 
two selected portions of the chain differ reasonably (in the first moment), and one then concludes that the chain has not converged.

Preselect two nonoverlapping window proportions, one early in the chain and one later in the chain: $\T_1$ of length $n_1$ and $\T_2$ 
of length $n_2$ along with some function of interest $g()$.  Then Geweke's\index{authorindex}{Geweke, J.} diagnostic is given by:
\begin{equation}
	G = \frac{\bar{g}(\T_1) - \bar{g}(\T_2)}
		 {\sqrt{ \frac{s_1(0)}{n_1} + \frac{s_2(0)}{n_2} }}.
\end{equation}
Typically, the function choice is the mean of the chain in each window: $\bar{g}(\T_1)=\sum_{i=1}^{n_1}g(\theta_i)/n_1$, 
$\bar{g}(\T_2)=\sum_{i=1}^{n_2}g(\theta_i)/n_2$, and $s_1(0)$ and $s_2(0)$ are the symmetric
spectral density\index{subjectindex}{spectral density} functions (see: Chatfield\index{authorindex}{Chatfield, C.} 2003, 
Section~6.2, or Beckett\index{authorindex}{Beckett, L.} and Diaconis\index{authorindex}{Diaconis, P.} 1994), defined for 
the time-series confined within these windows, provided the assumption that there are no 
discontinuities at the frequency 0.  The spectral density function in a given window can be 
normalized to give a Fourier transform\index{subjectindex}{Fourier transform} of the autocovariance function 
and essentially provides a way of thinking about the uncorrelated contribution from the 
individual values to the total variance (see Granger\index{authorindex}{Granger, G. W. J.} and 
Hatanaka\index{authorindex}{Hatanaka, M.} [1964] or Priestley\index{authorindex}{Priestley, M. B.} [1981] for the missing 
technicalities).\index{subjectindex}{time-series!chain values}

\index{subjectindex}{convergence diagnostic!Geweke!window ratios} 
Geweke\index{authorindex}{Geweke, J.} suggests using the ratios $n_1/n=0.1$ and $n_2/n=0.5$, occupying the $0.0$ to $0.1$ and
$0.5$ to $1.0$ quantiles of the series, respectively, and if these proportions are held fixed as the chain grows in length, then
the central limit theorem \index{subjectindex}{central limit theorem!Geweke diagnostic} applies and $G$ converges in distribution
to standard normal  under the null hypothesis of convergence of the chain.  The utility is therefore that we now have a
nonarbitrary method for asserting that the means of the functions of the values differ statistically.  It is customary to appeal
to basic normal theory and worry about values of $G$ that are greater in absolute value than 2. 

This is an overtly time-series manner of proceeding and reflects Geweke's\index{authorindex}{Geweke, J.} 
belief that more can be learned by one very long chain since it will end up exploring areas 
where humans might not think to send it.\index{subjectindex}{convergence diagnostic!Geweke!long run}  
Unfortunately the window proportions can greatly affect the value of $G$, and it is 
therefore important not to just rely on Geweke's\index{authorindex}{Geweke, J.} recommendation (0.1/0.5), 
which are the default values in the diagnostic suites \boa\index{subjectindex}{boa@\boa!Geweke} and 
\coda\negmedspace.\index{subjectindex}{coda@\coda!Geweke}  \index{subjectindex}{time-series!chain values}

\begin{examplelist}
	\item	{\bf Geweke Diagnostic, Tobit Model.}	\index{subjectindex}{death penalty}
		We first perform the Geweke diagnostic for the Tobit model described above using \boa.  In order
		to be cautious, the default settings are used and compared with windows of 20\% on either side
		of the middle point (a more cynical approach since it includes a larger early window and smaller later
		window).  These results are given in Table~\ref{Geweke.Tobit.Table}.

        	\begin{center}
        	\begin{table}[h]
        	\blstable
        	\tabletitle{\textsc{Geweke Diagnostic, Tobit Model}}\label{Geweke.Tobit.Table}
		\begin{tabular}{lrrcrr}
				      & \multicolumn{2}{c}{$0.1$ vs. $0.5$} & $\qquad$ & \multicolumn{2}{c}{$0.2$ vs. $0.2$}\\ 
				      & Z-score    & p-value	&	& Z-score     & p-value \\
				      \cline{2-3} \cline{5-6}
          	\texttt{Intercept}         &  0.1901  & 0.4246 	&	& -0.6632     & 0.2536  \\
		    \texttt{Past Rates}        &  0.4342  & 0.3321 	&	&  0.5781     & 0.2816 	\\
		    \texttt{Political Culture} &  0.0915  & 0.4636 	&	& -0.4210     & 0.3369 	\\
		    \texttt{Current Opinion}   &  1.0500  & 0.1436 	&	&  0.8934     & 0.1858 	\\
		    \texttt{Ideology}          & -0.7092  & 0.2391 	&	&  0.4351     & 0.3317 	\\
		    \texttt{Murder Rate}       &  1.1230  & 0.1307 	&	&  0.5769     & 0.2820 	\\
		\end{tabular} 
		\end{table}
		\end{center} \bls
		The output here provides little to worry about.  No values are observed to be in the tail of the normal 
		distribution at standard levels (although such points are arbitrary anyway), and so there is no evidence
		provided here to indicate nonconvergence.  The diagnostic for \texttt{Current Opinion} shows a little bit of 
		extra-variability, but not enough to cause alarm.


	\item	{\bf Geweke Diagnostic, Military Personnel Model.}
		Using the default window widths (other choices were shown to produce similar results), 
		\coda\index{subjectindex}{coda@\coda!Geweke} returns the Geweke\index{authorindex}{Geweke, J.} diagnostics here
		in Table~\ref{Geweke.Normal.Table}.  The Geweke\index{authorindex}{Geweke, J.} diagnostic confirms our previous 
		skepticism about $\alpha_{\mu}$, $\alpha_{\tau}$, $\beta_{2,\mu}$, $\beta_{2,\tau}$, $\beta_{3,\mu}$, and $\tau_c$.
		The corresponding $p$-values are well into the tails of the standard normal for both window configurations.  The
		similarity comes from the large sample size involved in both analyses.  
		\begin{center}
                \begin{table}[h]
                \blstable
                \tabletitle{\textsc{Geweke Diagnostic, Military Personnel Model}}\label{Geweke.Normal.Table}
                \begin{tabular}{lrrcrr}
                                      & \multicolumn{2}{c}{$0.1$ vs. $0.5$} & $\qquad$ & \multicolumn{2}{c}{$0.2$ vs. $0.2$}\\
                                      & Z-score    & p-value    &       & Z-score     & p-value \\
                                      \cline{2-3} \cline{5-6}
				      $\alpha_{\mu}$   &   3.9268 & 0.0001 &	&   3.9261 & 0.0000 \\
				      $\alpha_{\tau}$  &   0.4336 & 0.3323 &	&   0.7866 & 0.2158 \\
				      $\beta_{1,\mu}$  &   0.2288 & 0.4095 & 	&   0.2202 & 0.4129 \\
				      $\beta_{1,\tau}$ &  -2.1575 & 0.0144 &    &  -1.1873 & 0.1176 \\
				      $\beta_{2,\mu}$  &  44.0073 & 0.0001 &  	&  53.2793 & 0.0001 \\
				      $\beta_{2,\tau}$ &  -6.1943 & 0.0001 &  	&  -3.9272 & 0.0001 \\ 
				      $\beta_{3,\mu}$  &  -2.6983 & 0.0035 &  	&  -2.7704 & 0.0028 \\
				      $\beta_{3,\tau}$ &   0.5392 & 0.2949 &	&   0.9035 & 0.1831 \\
				      $\tau_c$         & -35.9739 & 0.0001 &	& -47.9269 & 0.0001 \\
		\end{tabular} 
		\end{table}
		\end{center} \bls
		\vspace{11pt}
		
\end{examplelist}

\index{subjectindex}{convergence diagnostic!Geweke|)}
\subsubsection{Gelman and Rubin's Multiple Sequence Diagnostic}\label{gelman.rubin.diag}
\index{subjectindex}{convergence diagnostic!Gelman and Rubin|(} 
Gelman\index{authorindex}{Gelman, A.} and Rubin's \index{authorindex}{Rubin, D. B.} (1992a) convergence diagnostic is based on
comparing a set of chains with different starting points that are overdispersed relative to the target distribution (greater
variability than the presumed limiting distribution).  The method is based on normal theory approximations to the marginal
posteriors using an ANOVA-based test with a normal or a Student's-$t$ distribution-based diagnostic.  Thus convergence decisions
can be made \emph{inferentially}.  \index{subjectindex}{convergence diagnostic!ANOVA test} \index{subjectindex}{Student's-$t$}

The test is run according to the following steps for each scalar parameter of interest 
($\theta$):\index{subjectindex}{convergence diagnostic!Gelman and Rubin!steps}
\begin{enumerate}
	\item 	Run $m \ge 2$ chains of length $2n$ from overdispersed starting points with
		subscripts denoting distinct chains, $(1), (2),\ldots, (m)$:
		\begin{align}
		&&\theta^{[0]}_{(1)}, &&\theta^{[1]}_{(1)}, &&\ldots &&\theta^{[n]}_{(1)}, &&\ldots &&\theta^{[2n-1]}_{(1)},
			&&\theta^{[2n]}_{(1)}\nonumber\\[3pt]
		&&\theta^{[0]}_{(2)}, &&\theta^{[1]}_{(2)}, &&\ldots &&\theta^{[n]}_{(1)}, &&\ldots &&\theta^{[2n-1]}_{(2)},
			&&\theta^{[2n]}_{(2)}\nonumber\\
		&&\vdots \nonumber\\
		&&\theta^{[0]}_{(m)}, &&\theta^{[1]}_{(m)}, &&\ldots &&\theta^{[n]}_{(1)}, &&\ldots &&\theta^{[2n-1]}_{(m)},
			&&\theta^{[2n]}_{(m)}.
		\end{align}
		The starting points can be determined by overdispersing around suspected
		or known modes.  Discard the first $n$ chain iterations.

	\item	For each parameter of interest calculate the following:
		\begin{bayeslist}
			\item {\bf Within chain variance:} 	
			\begin{equation}
			    W = \frac{1}{m(n-1)}\sum_{j=1}^{m}\sum_{i=1}^{n}
				(\theta^{[i]}_{(j)}-\bar{\theta}_{(j)})^2
			\end{equation}
			where $\bar{\theta}_{(j)}$ is the mean of the $n$ values for the 
			$j^{th}$ chain.

			\item {\bf Between chain variance:} 	
			\begin{equation}
			    B = \frac{n}{m-1}\sum_{j=1}^{m}
				(\bar{\theta}_{(j)} - \bar{\bar{\theta}})^2
			\end{equation}
			where $\bar{\bar{\theta}}$ is the grand mean (i.e. the mean of
			means since each subchain is of equal length).

			\item {\bf Estimated variance:} 
			\begin{equation}
				\widehat{\Var}(\theta) = (1 - 1/n)W + (1/n)B. 
			\end{equation}
		\end{bayeslist}

	\item	The convergence diagnostic is a single value for each dimension, called the 
		\emph{potential scale reduction factor} (or shrink factor):
		\begin{equation}
		    \widehat{R} = \sqrt{\frac{ \widehat{\Var}(\theta) }{ W }}.
		\end{equation}
		Along with this comes an upper confidence interval bound.  The \coda\ output also provides an omnibus 
		single scalar version called the {multivariate potential scale reduction factor}.
		\index{subjectindex}{potential scale reduction factor}
		\index{subjectindex}{multivariate potential scale reduction factor}

	\item	Values of $\widehat{R}$ near 1 are evidence that the $m$ chains are all
		operating on the same distribution (in practice values less than roughly 
		1.1 or 1.2 are acceptable according to Gelman\index{authorindex}{Gelman, A.} [1996]).
		%\index{subjectindex}{convergence diagnostic!Gelman and Rubin!rule of thumb}
\end{enumerate}
The logic behind this test is quite simple.  Before convergence, $W$ underestimates total
posterior variation in $\theta$ because  the chains have not fully explored the target 
distribution and $W$ is therefore based on smaller differences.  Conversely, 
$\widehat{\Var}(\theta)$ overestimates total posterior variance because the starting
points are intentionally overdispersed relative to the target.  However, once the chains
have converged, the difference should be incidental since the chains are exploring the
same region and are therefore overlapping.\index{subjectindex}{convergence diagnostic!Gelman and Rubin!overdispersion}

Commonly, the number of separate chains is about 5 to 10, but more complicated model 
specifications and more posterior complexity may require the specification of additional
starting points.\index{subjectindex}{convergence diagnostic!Gelman and Rubin!number of chains}  
Note that the test here is described for only one dimension and in practice each of the 
$m$ chains starts from a point in $\Re^k$ for a $k$-length vector of estimation 
parameters $\T$.  Thus dimensionality can considerably add to the work involved in 
selecting overdispersed starting points.

Practically, it is very easy to monitor convergence by periodically checking the value of $\widehat{R}$ as the Markov chain 
runs (\winbugs\ \index{subjectindex}{bugs@\bugs!Gelman and Rubin} readily supplies a slightly different version of this).  
Therefore we can run the chain until satisfied with the statistic and then treat the most recent $n$ values as empirical 
values from the desired target distribution.  

\index{subjectindex}{convergence diagnostic!Gelman and Rubin!many short runs}  
The primary difficulty with the Gelman\index{authorindex}{Gelman, A.} and Rubin\index{authorindex}{Rubin, D. B.} diagnostic 
(noted by many observers) is that it is not always easy to obtain suitably overdispersed starting points, since determining 
their position requires some knowledge of the target distribution to begin with.
\index{subjectindex}{convergence diagnostic!Gelman and Rubin!problems}  Geyer\index{authorindex}{Geyer, C. J.} (1992) points 
out that the determination of good starting points is actually \emph{essential} in the Gelman\index{authorindex}{Gelman, A.} 
and Rubin\index{authorindex}{Rubin, D. B.} diagnostic because of the underdispersed/overdispersed distributional contrast.  
Cowles\index{authorindex}{Cowles, M. K.} and Carlin\index{authorindex}{Carlin, B. P.} (1996) worry about the heavy reliance 
on normal approximations, and it is obvious that substantial deviations from normality make the choice of starting values 
much more difficult (somewhat mitigated by the Brooks-Gelman\index{authorindex}{Brooks, S. P.}\index{authorindex}{Gelman, A.} 
modification [1998a, 1998b] which uses the more defensible Student's-$t$ distribution assumption).\index{subjectindex}{Student's-$t$}  
Also, Brooks and  Giudici (2000)\index{authorindex}{Brooks, S. P.} \index{authorindex}{Giudici, P.}
give an extended split-variance refinement of this diagnostic focused on model choice, including reversible jump MCMC (see 
Section~\ref{section:reversible.jump} in Chapter~\ref{MCMC.Extensions.Chapter} starting on
page~\pageref{section:reversible.jump}).  \index{subjectindex}{Markov chain Monte Carlo (MCMC)!reversible jump}

In addressing the starting point issue, 
Gelman\index{authorindex}{Gelman, A.} and Rubin\index{authorindex}{Rubin, D. B.} (1992a) advise the use of the 
EM algorithm\index{subjectindex}{EM algorithm!use in MCMC} to find modes and caution that EM should be started 
from several different points if multimodality\index{subjectindex}{multimodality} is suspected (1992a, 459).  In
practice, it is not always easy to write a customized EM algorithm for a reasonably intricate model.  With the
case of known multimodality Gelman\index{authorindex}{Gelman, A.} and Rubin\index{authorindex}{Rubin, D. B.} advise 
using a more complicated procedure that involves constructing a mixture of normals centered at the found modes, and 
``sharpened'' with importance sampling as a means of determining the scale of overdispersion required.  

\begin{examplelist}
	\item	{\bf The Gelman and Rubin Diagnostic for the Tobit Model.}	\index{subjectindex}{death penalty}
		Returning to the Tobit model, we perform the Gelman and Rubin diagnostic with five separate chain 
		paths with starting points perturbed off of the MLE from a regular GLM to overdisperse.  The draw 
		for the $z_i$ values precluded dramatically overdispersed starting points in that dimension since the
		random truncated normal generator becomes extremely inefficient for highly unlikely parameter values 
		relative to the associated data.  Each chain is again run for $50,000$ iterations at these starting
		points with the first $40,000$ iterations discarded in all cases.  The \coda\ diagnostic suite, with 
		a normal distribution assumption, is used to produce the following results, which show no evidence 
		of nonconvergence (Table~\ref{GR.Tobit.Table}).  Table~\ref{GR.Tobit.Table} also gives the
		multivariate potential scale reduction factor from \coda\ output.  The MPSRF is an additional statistic
		from Brooks\index{authorindex}{Brooks, S. P.} and Gelman\index{authorindex}{Gelman, A.} (1998b) that 
		seeks to give a fully multivariate view of chain convergence using the full variance-covariance matrix 
		of the output and therefore providing a more conservative view.  This is particularly useful for 
		high-dimensional models as a convenient omnibus test, although Brooks\index{authorindex}{Brooks, S. P.} 
		and Gelman\index{authorindex}{Gelman, A.} suggest looking at the individual PSRF statistics for 
		important parameters.

		\begin{center}
                \begin{table}[h]
                \blstable
                \tabletitle{\textsc{\normalsize Gelman and Rubin Diagnostic, Tobit Model}}\label{GR.Tobit.Table}
		$\qquad$\begin{tabular}{lrrrrrr}
                	& \multicolumn{1}{c}{}
                	& \multicolumn{1}{c}{\texttt Past}
                	& \multicolumn{1}{c}{\texttt Political}
                	& \multicolumn{1}{c}{\texttt Current}
                	& \multicolumn{1}{c}{}
                	& \multicolumn{1}{c}{\texttt Murder} \\[-3pt]
                	& \multicolumn{1}{c}{\texttt Intercept}
                	& \multicolumn{1}{c}{\texttt Rates}
                	& \multicolumn{1}{c}{\texttt Culture}
                	& \multicolumn{1}{c}{\texttt Opinion}
                	& \multicolumn{1}{c}{\texttt Ideology}
                	& \multicolumn{1}{c}{\texttt Rate} \\
			\hline
			PSRF		& 1.01 & 1.00 & 1.02 & 1.01 & 1.01 & 1.02 \\
			95\% Upper CI	& 1.03 & 1.02 & 1.05 & 1.05 & 1.02 & 1.07 \\
			\hline
			\multicolumn{7}{c}{Multivariate Potential Scale Reduction Factor: 1.03}\\
		\end{tabular} 
		\end{table}
		\end{center} \bls
		\vspace{11pt}

	\item	{\bf The Gelman and Rubin Diagnostic for the Military Personnel Model.}
		Again five separate Markov chains are run using overdispersed starting points.  These
		were determined by constructing a wide grid\index{subjectindex}{gridding} along the sample space of each of the
		five parameters.  Then the joint posterior is applied systematically to go through the
		dimensions to look for modes.  This can be a very time-consuming process if the grid
		is reasonably granular, and it is often possible to get away with fairly large bins.

		We now run the Gelman\index{authorindex}{Gelman, A.} and Rubin\index{authorindex}{Rubin, D. B.} procedure on the 
		model of Military Personnel using \boa\index{subjectindex}{boa@\boa!Gelman and Rubin} 
		defaults for the five chains, including the convention of using only the second half of the recorded change 
		values.   Overdispersed starting points were determined from the posterior means and HPD intervals in 
		Table~\ref{Military.Summary.Table} on a marginal basis.  The results are given in Table~\ref{GR.Normal.Table}.

        \begin{table}[h]
        \parbox[c]{\linewidth}{ \hspace{0.1in}
            \blstable
            \tabletitle{\textsc{Gelman and Rubin Diagnostic, Military Personnel Model}}\label{GR.Normal.Table}
		    \begin{tabular}{lrrrrrrrrr}
                	& \multicolumn{1}{c}{$\alpha_{\mu}$}  & \multicolumn{1}{c}{$\alpha_{\tau}$}
                	& \multicolumn{1}{c}{$\beta_{1,\mu}$} & \multicolumn{1}{c}{$\beta_{1,\tau}$}
                	& \multicolumn{1}{c}{$\beta_{2,\mu}$} & \multicolumn{1}{c}{$\beta_{2,\tau}$}
                	& \multicolumn{1}{c}{$\beta_{3,\mu}$} & \multicolumn{1}{c}{$\beta_{3,\tau}$}
                	& \multicolumn{1}{c}{$\tau_c$} \\
			    \hline
			    PSRF 		& 1.00 & 1.00 & 2.28 &  2.01 & 25.32  & 2.49 & 1.04 &  4.70 & 1.36 \\
			    95\% Upper CI	& 1.01 & 1.00 & 3.58 & 10.11 & 128.56 & 4.13 & 1.04 & 15.49 & 1.82 \\
			    \hline
			    \multicolumn{10}{c}{Multivariate Potential Scale Reduction Factor: 21.7}\\
		    \end{tabular} 
		}
		\end{table}
        \bls
		
        Here all but three of the ${\widehat{R}}$ statistics in the first row of the table give cause for concern.  Only
        $\alpha_{\mu}$, $\alpha_{\tau}$, and $\beta_{3,\tau}$ provide PSRF values that we would be comfortable with.  Note also
        that the PSRF value for $\beta_{2,\mu}$ is enormous.  The second line of the table provides the upper 95\% credible
        interval for the statistics and shows large tails for the three $\beta$ parameter variance components$\beta_{1,\tau}$,
        $\beta_{2,\tau}$, and $\beta_{3,\tau}$.  All of this provides strong evidence of overall nonconvergence of the Markov
        chain.  Markov chains with Gibbs or Metropolis kernels do not converge partially by dimension (Gill 2010).  In addition,
        the Brooks \index{authorindex}{Brooks, S. P.} and Gelman\index{authorindex}{Gelman, A.} (1998a, 1998b) correction also
        gives a multivariate summary for the whole model, which is $21.7$ in this case and is clearly dominated by the result for
        $\beta_{2,\mu}$.
\end{examplelist}
\index{subjectindex}{convergence diagnostic!Gelman and Rubin|)} 

\subsubsection{The Heidelberger and Welch Diagnostic}
\index{subjectindex}{convergence diagnostic!Heidelberger and Welch|(}
This diagnostic works in its implemented form for individual parameters in single chains.  It is easy to code on one's own and
conveniently supplied in \boa\ and \coda.  Heidelberger\index{authorindex}{Heidelberger, P.} and Welch
\index{authorindex}{Welch, P. D.} (1983) originally developed the algorithm in another context for simulations in operations
research and based on Brownian bridge theory, using the Cram\'{e}r-von Mises statistic
(Heidelberger\index{authorindex}{Heidelberger, P.} and Welch\index{authorindex}{Welch, P. D.} 1981a, 1981b;
Schruben\index{authorindex}{Schruben, L. W.} 1982; and Schruben,\index{authorindex}{Schruben, L. W.}
Singh,\index{authorindex}{Singh, H.} and Tierney\index{authorindex}{Tierney, L.} 
    1983).\footnote{A Brownian Bridge is a \index{authorindex}{Priestley, M. B.}
	Wiener process (also called Brownian motion, Priestley [1981, pp.167-8]) over the interval $[0,1]$, in 
	which the starting and stopping points are tied to the endpoints 0 and 1
	and the ``string'' in between varies.  The details are beyond the scope of this 
	work and the interested reader is directed to 
	Chu\index{authorindex}{Chu, C-S. J.} and White\index{authorindex}{White, H.} (1992), 
	Lee\index{authorindex}{Lee, S.} (1998), as well as Nevzorov\index{authorindex}{Nevzorov, V. B.} and 
	Zhukova\index{authorindex}{Zhukova, E. E.} (1996).}
\index{subjectindex}{Brownian Bridge}\index{subjectindex}{Cram\'{e}r-von Mises statistic}\index{subjectindex}{Wiener process}

Like the Geweke\index{authorindex}{Geweke, J.} diagnostic, it has an 
inherently time-series orientation and uses the spectral density estimate.  \index{subjectindex}{spectral density} 
The null hypothesis for this test is that the chain is currently in the stationary distribution and the test starts with the full 
set of iterations produced from running the sampler from time zero.  The general form for steps are then as follows.
\begin{enumerate}
	\item	Specify: a number of iterations ($N$, generally given by the current status of the chain at the proposed stopping 
		point), an accuracy ($\epsilon$), and an alpha level for the test.  
	\item 	Evaluate the first 10\% of the chain iterations.
	\item	Calculate the test statistic on these samples and observe whether it indicates a tail value.
	\item	If the test rejects the null, the first 10\% of the iterations are discarded and the test is run again.  
	\item	This process continues until either 50\% of the data have been dismissed or the test fails to reject the null with 
		the remaining iterations.  
\end{enumerate}
If some proportion of the data are found to be consistent with stationarity,\index{subjectindex}{Markov chain!stationary distribution} 
then the \emph{halfwidth} analysis part of the diagnostic is performed.  

The process is actually a variant of the Kolmogorov-Smirnov nonparametric test for large 
sample sizes referred to as the Cram\'{e}r-von Mises\index{subjectindex}{Cram\'{e}r-von Mises statistic} 
test after the form of the test statistic.  A parameter, $s$, is defined as the proportion 
of the continuing sum of the chain, ranging from zero to one.  We are interested in the 
difference between the sum of the first $sT$ of the chain (where $T$ is the total length 
of the chain), and the mean of the complete set of chain values scaled by $sT$.  If the 
chain were in its stationary distribution, then this difference would be negligible.  For the 
disparity between empirical and theoretical CDFs, the Cram\'{e}r-von Mises test statistic is
\begin{equation}
	C_n(F) = \int_X [F_n(x) - F(x)]^2dF(x),	
\end{equation}
with a predefined rejection region $C_n(F) > c$, established by normal tail values under
asymptotic assumptions and the hypothesis that $F_n(x) \ne F(x)$.

There are some additional aspects that must be defined to get from the Cram\'{e}r-von Mises 
to the Heidelberger\index{authorindex}{Heidelberger, P.} and Welch\index{authorindex}{Welch, P. D.} diagnostic.  
Define formally the following quantities: \index{subjectindex}{convergence diagnostic!Heidelberger and Welch!parameters}
\begin{bayeslist}
	\item	$T = $ the total length of the ``accepted'' chain after discards.
	\item	$s \in [0\range 1] = $ the test chain proportion.
	\item	$T_{\lfloor sT \rfloor} = \sum_{i=1}^{\lfloor sT \rfloor}\theta_i = $ 
		the sum of the chain values from one to the integer value just below $sT$.
	\item	$\lfloor sT \rfloor \bar{\theta} = $ the chain mean times the 
		integer value just below $sT$.
	\item	$s(0) = $ the spectral density of the chain.
\end{bayeslist}
The notation $\lfloor X \rfloor$ denotes the integer value of $X$ (the ``floor'' function).  Using these produced quantities, 
for any given $s$, we can construct the test statistic:
\begin{equation}
        B_T(s) = \frac{ T_{\lfloor sT \rfloor} - \lfloor sT \rfloor \bar{\theta} } { \sqrt{ T s(0) } },              
\end{equation}
which is the Cram\'{e}r-von Mises test statistic for sums as cumulative values scaled by 
the spectral density.  Now $B_T(s)$ can be treated as an approximate Brownian bridge and 
tested using normal tail values to decide on the 10\% discards (where some adjustments are 
necessary because $s(0)$ is estimated rather than known).  

The halfwidth part of the test compares two quantities:
\index{subjectindex}{convergence diagnostic!Heidelberger and Welch!halfwidth test}
\begin{bayeslist}
	\item	Using the proportion of the data not discarded, the halfwidth of the 
		$(1-\alpha)$\% credible interval is calculated around the sample mean,
		where the estimated asymptotic standard error is the square root of 
		spectral density divided by the remaining sample size, $s(0)/n^*$. 
	\item	If the mean divided by the halfwidth is lower than $\epsilon$, then
		the halfwidth test is passed for this dimension.
	\item	If this test fails, then longer runs are required to accurately produce
		an empirical estimate of the mean and assert convergence.
\end{bayeslist}
It should be remembered that the quality of this second part of the diagnostic
is completely dependent on the ability of the first to produce a subchain in the true limiting
distribution, and if it has settled for some time on a local maxima, then the halfwidth analysis
can be wrong.  Finally, it should be noted that most practitioners pay attention only to the
Cram\'{e}r-von Mises test statistic part of the test since it has an easy interpretation like
the Geweke statistic.

\begin{examplelist}
	\item	{\bf Heidelberger and Welch Diagnostic for the Tobit Model.}	\index{subjectindex}{death penalty}
		Running the H-W diagnostic at $\epsilon=0.1$, $\alpha=0.05$ in \boa\ produces:

		\begin{footnotesize}
		\begin{R.Code}
		Stationarity        Test  Keep Discard C-von-M Halfwidth     Mean Halfwidth  Ratio
		Intercept         passed 10000       0  0.2288    passed -14.5451    0.0971 -0.006
		Past Rates        passed 10000       0  0.1926    passed 171.1460    0.1661  0.001
		Political Culture passed 10000       0  0.0397    passed   0.3461    0.0038  0.011
		Current Opinion   passed 10000       0  0.2211    passed   3.9738    0.0274  0.007
		Ideology          passed 10000       0  0.1592    passed   3.1423    0.0299  0.009
		Murder Rate       passed 10000       0  0.0511    failed   0.0088    0.0020  0.227
		\end{R.Code} 
		\end{footnotesize}
		\begin{comment}
                hw.mat <- matrix(c(-14.5451,0.0971,171.1460,0.1661,0.3461,0.0038,3.9738,0.0274,3.1423,0.0299,0.0088,0.0020),
			byrow=TRUE,ncol=2)
		( hw.mat <- cbind(hw.mat,hw.mat[,2]/hw.mat[,1]) )
		\end{comment}

		Notice that all of the chain values passed the Cram\'{e}r-von Mises test component, which is not 
		surprising given the previously observed health of this chain run and the fact that we have already 
		discarded $40,000$ values to begin the chain.  However, on the halfwidth test, the dimension for 
		\texttt{Murder Rate} fails since the ratio of the halfwidth to the mean is \texttt{0.227}, which is
		the only one greater than $\epsilon=0.1$.   Recall that this variable was the only one found to be 
		statistically unreliable in the model given in Table~\ref{tobit.table1} (irrespective of the unimportant 
		constant).  This is worth further investigation.  The individual autocorrelations revealed no obvious 
		stickiness problems:
		\begin{R.Code}
		              Lag 1      Lag 5      Lag 10       Lag 50
		Murder Rate   0.212      0.014      -0.004       0.0001
		\end{R.Code} 
		but there was evidence that this parameter is strongly correlated with \texttt{Political Culture}:
		\begin{R.Code}
		Past.Rates   Political Culture   Current Opinion   Ideology
		    -0.123              -0.649             0.139      0.182	
		\end{R.Code} 
		(which should explain some of the mixing).  Recall, however, that slow mixing is not a serious 
		problem if the full sample space is still fully explored.  The wealth of other diagnostics certainly 
		point us in this direction.

		Figure~\ref{murder.fig} provides a traceplot and histogram of the last $2,000$ iterations of the marginal 
		chain values for \texttt{Murder Rate} with the mean and 95\% credible intervals indicated in each.  Both figures 
		support stability of the chain over this last period.  The traceplot shows some minor snaking, possibly 
		indicating mixing at a pace slower than the other dimensions, but actually nothing even slightly alarming 
		is revealed about convergence.  The chain is stable around its current mean, balanced between the interval 
		ends, and the histogram shows strong evidence of the central limit theorem.   
		\begin{figure}[h!]
		\parbox[l]{\linewidth}{ \hspace{0.14in}
        		\centerline{ \epsfig{file=Images/mcmc_utilitarian.figure04.ps,height=4.80in,width=2.7in,clip=,angle= 270}  } }
			\vspace{8pt}
			\caption{\textsc{Traceplot and Histogram, Last 2,000 Iterations}} \label{murder.fig}
		\end{figure}

	\item	{\bf The Heidelberger and Welch Diagnostic for the Military Personnel Model.}
		Using the standard defaults in \boa\index{subjectindex}{boa@\boa!Heidelberger and Welch} (again at the default parameter 
		values of $\epsilon=0.1$, $\alpha=0.05$), the H-W test gives the following output:

		\begin{footnotesize}
		\begin{R.Code}
		Stationarity   Test    Keep  Discard C-von-M  Halfwidth      Mean  Halfwidth    Ratio
		alpha.mu     passed  150000        0  0.3162     failed    0.7403     0.8404   1.1352
		alpha.tau    passed  150000        0  0.2236     passed   10.0155     0.2672   0.0267
		beta1.mu     passed  150000        0  0.1825     failed    0.0965     0.0238   0.2466
		beta1.tau    passed  150000        0  0.2458     passed   13.3070     0.2432   0.0183
		beta2.mu     failed   60000    90000  6.4482     passed    1.9996     0.0114   0.0057
		beta2.tau    failed   60000    90000  2.5148     passed   25.9008     0.7142   0.0276
		beta3.mu     passed  120000    30000  0.3397     passed   -0.0011     0.0001  -0.0909
		beta3.tau    passed  150000        0  0.1642     passed  180.0586     0.1941   0.0011
		tau.c        failed   60000    90000  6.0374     passed    0.0330     0.0004   0.0121
		\end{R.Code} 
		\end{footnotesize}
                \begin{comment}
                hw.mat <- matrix(c(0.7403,0.8404,10.0155,0.2672,0.0965,0.0238,13.3070,0.2432,1.9996,0.0114,25.9008,
				   0.7142,-0.0011,0.0001,180.0586,0.1941,0.0330,0.0004),byrow=TRUE,ncol=2)
                ( round(hw.mat <- cbind(hw.mat,hw.mat[,2]/hw.mat[,1]),4) )
                \end{comment}
		
		\noindent Obviously the results show poor convergence properties according to this diagnostic.  The first 
		test is the stationary test from the Brownian bridge approximation, and three marginal results show a
		failure to obtain acceptable Cram\'{e}r-von Mises test statistics for discarding less than 50\% of the 
		data in $15,000$ increments.  The associated large values of \texttt{C-von-M} fall into the tail region past 
		a stipulated critical value.  In the second stage of the diagnostic we can ignore the results for 
		$\beta_{2,\mu}$, $\beta_{2,\tau}$, and $\tau_c$, since they have already failed.  Now notice that 
		$\alpha_{\mu}$ and $\beta_{1,\mu}$ fail here with a small value for the halfwidth, which gives a subsequent
		ratio of the halfwidth to the mean that exceeds $\epsilon=0.1$.
\end{examplelist}
\index{subjectindex}{convergence diagnostic!Heidelberger and Welch|)}

\subsubsection{The Raftery and Lewis Integrated Diagnostic}\label{raftery.lewis.diag}
\index{subjectindex}{convergence diagnostic!Raftery and Lewis|(}
The diagnostic proposed and developed in Raftery\index{authorindex}{Raftery, A. E.} and Banfield\index{authorindex}{Banfield, J. D.} 
(1991), Raftery\index{authorindex}{Raftery, A. E.} and Lewis\index{authorindex}{Lewis, S. M.} (1992), and summarized in Raftery
\index{authorindex}{Raftery, A. E.} and Lewis\index{authorindex}{Lewis, S. M.} (1996) provides a rough indication of convergence for 
a running chain.  It was originally designed to elicit desired thresholds from the user and then estimate the chain length 
that provides a satisfactory result.  Consequently, the Raftery\index{authorindex}{Raftery, A. E.} and Lewis
\index{authorindex}{Lewis, S. M.} diagnostic addresses the burn-in period, thinning of the chain, the posterior quantities of 
interest, and probabilistic reliability, as well as the number of iterations.\index{subjectindex}{thinning chains}
\index{subjectindex}{burn-in period of a chain}

For a single given parameter of interest, the steps are given as follows:
\index{subjectindex}{convergence diagnostic!Raftery and Lewis!steps}
\begin{enumerate}
	\item	Select the posterior quantile of interest, $q$, such as a tail value or 
		some substantively interesting point:
		\begin{equation}
			q = p(f(\theta) \le f(\theta_q)|\X).
		\end{equation}

	\item	Select an acceptable tolerance for this quantile, $r$, and the desired
		probability of being within that tolerance, $s$. For example 
		(Raftery\index{authorindex}{Raftery, A. E.} and Lewis\index{authorindex}{Lewis, S. M.} 1996), 
		suppose we want to report a 95\% 
		credible interval around the median with reliability between 92.5\% and
		97.5\%.  This is equivalent to the assignments: $q=0.5$, $r=0.0125$,
		and $s=0.95$.  

	\item	Select a convergence tolerance value $\epsilon$, which is used to 
		determine a stopping point based on a parallel Markov chain process
		(defaulted to 0.001).\index{subjectindex}{parallel Markov chain process}
	
	\item	Run a ``pilot'' sampler whose length is determined as if there is no
		\index{subjectindex}{pilot sample}
		autocorrelation in the parallel chain, given by rounding:
		\begin{equation}
			n_{\text{pilot}} = \left[ \Phi^{-1}\left(\frac{s+1}{2}\right)
					   	  \frac{\sqrt{q(1-q)}}{r} \right]^2,
		\end{equation}
		where $\Phi^{-1}()$ denotes the inverse of the normal CDF.  For the example
		above at the defaults, $n_{\text{pilot}} = 2294$. 

	\item	The program (\boa\index{subjectindex}{boa@\boa!Raftery and Lewis}\negmedspace, 
		\coda\index{subjectindex}{coda@\coda!Raftery and Lewis}, or 
		\texttt{mcgibbsit}\index{subjectindex}{mcgibbsit@\texttt{mcgibbsit}}) 
		will then return: the length of the burn-in period ($M$), the estimated 
		number of post burn-in iterations required to meet the goals ($N$), and 
		the thinning interval ($k$).  The user then runs a Markov chain according 
		to these parameters.\index{subjectindex}{thinning chains}
		\index{subjectindex}{burn-in period of a chain}
\end{enumerate}
The method is based on setting up a parallel (but not Markov) chain during the pilot run, 
where the iterations are a binary series according to whether the generated value in the 
primary chain at that stage is less than the chosen quantile.  Raftery\index{authorindex}{Raftery, A. E.} 
and Lewis\index{authorindex}{Lewis, S. M.} extrapolate values that give this secondary chain of zeros and 
ones some Markovian properties, and are then able to estimate first-chain probabilities 
that satisfy the user constraints (see Raftery\index{authorindex}{Raftery, A. E.} and Lewis\index{authorindex}{Lewis, S. M.} 
[1996, pp.116-118] for specifics).  A nice feature of this process is that the final run 
of $N$ iterations can be treated as a new pilot run, so that if the diagnostic returns a 
new recommended $N$, it would be motivation for additional sampling.

Unfortunately this process must be individually repeated for every quantile of every 
parameter of interest.  Also, the number of iterations required ($N$) is different
for different quantiles picked (in the example above, the number of required iterations
would be only 436 here if $q=0.95$ were of interest instead of  $q=0.5$).  Since the
process is inherently iterative, picking quantiles and monitoring pilot runs, then
the process can be quite time-consuming (in the human sense) for large numbers of parameters.

\index{subjectindex}{convergence diagnostic!Raftery and Lewis!problems}
Robert\index{authorindex}{Robert, C. P.} and Casella\index{authorindex}{Casella, G.} (1999, First Edition) list a number 
of more serious problems with the Raftery\index{authorindex}{Raftery, A. E.} and Lewis\index{authorindex}{Lewis, S. M.} 
diagnostic: the inherently single-dimension nature does not account for potential correlations between coefficients (and 
therefore provides evidence of collective marginal convergence rather than joint convergence, a point also mentioned
by Brooks\index{authorindex}{Brooks, S. P.} and Roberts\index{authorindex}{Roberts, G. O.} [1997]), the dependence on the 
representativeness of the pilot sample, and suspect nature of the Markov approximation in the secondary chain; see 
Guihenneuc-Joyaux\index{authorindex}{Guihenneuc-Jouyaux, C.} and Robert\index{authorindex}{Robert, C. P.} (1998) for a 
more rigorous but complex approach based on a \emph{divergence evaluation} of two chains (Section 4.4).  
Brooks\index{authorindex}{Brooks, S. P.} and Roberts\index{authorindex}{Roberts, G. O.} (1998) also warn against 
widespread use of the default $q=0.025$ setting since it can lead to substantial underestimation of the burn-in period.  
\index{subjectindex}{burn-in period of a chain} Nonetheless, the Raftery\index{authorindex}{Raftery, A. E.} and 
Lewis\index{authorindex}{Lewis, S. M.} diagnostic often works quite well with simple models and can give a general 
approximation of the scope of the number of iterations required in more elaborate specifications.  In addition, its 
inclusion in \boa\ and \coda\ mean that many practitioners will make use of it regardless of any warnings that might 
appear in the general statistics literature.

\begin{examplelist}
	\item	{\bf The Raftery and Lewis Diagnostic for the Tobit Model.}	
		\index{subjectindex}{death penalty}
		Running the Raftery and Lewis diagnostic at \boa\ defaults, $q=0.025$, $r=\pm 0.005$, $s=0.95$, produces:
		\begin{R.Code}
		                     Burn-in  Total  Lower Bound  Dependence
		                     (M)      (N)    (Nmin)       Factor (I)
		Intercept            3        4061   3746         1.08
		Past Rates           3        4061   3746         1.08    
		Political Culture    3        4129   3746         1.10    
		Current Opinion      2        3802   3746         1.01
		Ideology             3        4028   3746         1.08
		Murder Rate          3        4061   3746         1.08    
		\end{R.Code}
		where we should not be alarmed by anything.  The low burn-in numbers result from starting
		the diagnostic \emph{after} burn-in $40,000$ iterations.   \index{subjectindex}{burn-in period of a chain}
		Note that \boa\ gives a thinning value in the summary whereas \coda\ does not.

		The \texttt{Lower Bound} is the $n_{\text{pilot}}$ estimate of the number of iterations necessary 
		to satisfy $r$ and $s$ under the assumption of independent chain values, which is less than 
		100,000 so no warning was given.  \texttt{Burn-in}\index{subjectindex}{burn-in period of a chain} is the 
		estimated $m$, and \texttt{Thin}\index{subjectindex}{thinning chains} is the $k $ value used in the 
		calculations.  \texttt{Total} is the estimated number of iterations required $N$.  The last 
		column gives the ``dependence factor'':
		\begin{equation}
			I_{sd} = \frac{M}{n_{\text{pilot}}},	
		\end{equation}
		which is interpreted by Raftery\index{authorindex}{Raftery, A. E.} and Lewis \index{authorindex}{Lewis, S. M.} (1992) 
		as the proportional increase in the number of iterations attributable to serial dependence (the version of $I_{sd}$ 
		implemented in \boa\index{subjectindex}{boa@\boa!Raftery and Lewis} is slightly different than that given by Raftery
		\index{authorindex}{Raftery, A. E.} and Lewis).\index{authorindex}{Lewis, S. M.}  Their prescriptive advice is to worry 
		about dependence factors exceeding 5, as it might be due to an influential starting value, high correlations between 
		coefficients, or poor mixing.  

	\item	{\bf The Raftery and Lewis Diagnostic for the Military Personnel Model.}
		We now run the integrated diagnostic with the values with $q=0.05$, $r=\pm 0.005$, and $s=0.95$ for the Military 
		Personnel example.  The routine in \boa\index{subjectindex}{boa@\boa!Raftery and Lewis} gives the output:
		\begin{R.Code}
		             Burn-in  Total    Lower Bound  Dependence
		             (M)      (N)      (Nmin)       Factor (I)
		alpha.mu     864      889248   3746         237.00
		alpha.tau    336      341740   3746          91.20
		beta1.mu     168      178500   3746          47.70
		beta1.tau    40       44616    3746          11.90
		beta2.mu     608      701176   3746         187.00
		beta2.tau    24       30752    3746           8.21
		beta3.mu     2        3817     3746           1.02
		beta3.tau    2        3819     3746           1.02
		tau.c        124      146568   3746          39.10
		\end{R.Code}
		\vspace{6pt}
		No pilot run was necessary since 150,000 iterations were performed prior to running the diagnostic in \R\ using 
		\boa\index{subjectindex}{boa@\boa!Raftery and Lewis} (if this was an insufficient period, 
		\boa\index{subjectindex}{boa@\boa!Raftery and Lewis} informs the user).  In this second example, we should be worried
		about all but two of the dependence factors (\texttt{beta3.mu} and \texttt{beta3.tau}), given the length of the chain 
		and the correlations given in Table~\ref{Military.Corr}.  While this diagnostic is gives only a general 
		indication of problems, such results as these are strong evidence that the chain has not yet reached its 
		stationary distribution.
\end{examplelist}
\index{subjectindex}{convergence diagnostic!Raftery and Lewis|)}

\subsection{Summary of Diagnostic Similarities and Differences}
The empirical examples make it clear that the four diagnostics described in detail show different characteristics of the Markov
chain.  Given the potential complexities of model space, the variability of posterior forms, and possibly high dimensions, it is
not surprising that the stochastic behavior of the chain can be difficult to describe with a single summary statistic.  It is 
therefore strongly recommended that researchers run all of the standard formal diagnostics with \texttt{superdiag} in addition to
generating graphical displays.  Obviously not all of these need to be included in the writeup, but they are certain to be useful
in understanding chain behavior.  Shockingly, there are deep misunderstandings about this process, even by producers of commercial
software.  For example, the \texttt{Amos} User's Guide in \spss\ provides an MCMC estimation process for structural equation
models.  The software manual states ``The default burn-in period of 500 is quite conservative, much longer than needed for most
problems'' (Arbuckle 2009).  Obviously this is a dangerous recommendation, even for simple models.
\index{authorindex}{Arbuckle, J. L.}  

\begin{table}[h] \blstable
\parbox[c]{\linewidth}{ \hspace{-0.15in}
    \tabletitle{\textsc{Diagnostic Scorecard, Tobit Model}}\label{Tobit.Scorecard}
    \vspace{6pt}
    \begin{tabular}{lrrrr}
                         & Geweke & Gelman-Rubin & Heidelberger-Welch & Raftery-Lewis \\
        \hline
        \texttt{Intercept}         & \ding{51}            & \ding{51}     & \ding{51}     & \ding{51}     \\
        \texttt{Past Rates}        & \ding{51}            & \ding{51}     & \ding{51}     & \ding{51}     \\
        \texttt{Political Culture} & \ding{51}            & \ding{51}     & \ding{51}     & \ding{51}     \\
        \texttt{Current Opinion}   & \ding{51}            & \ding{51}     & \ding{51}     & \ding{51}     \\
        \texttt{Ideology}          & \ding{51}            & \ding{51}     & \ding{51}     & \ding{51}     \\
        \texttt{Murder Rate}       & \ding{51}            & \ding{51}     & \ding{55}     & \ding{51}     \\
    \end{tabular} 
}    
\end{table} 

To summarize the convergence assessment for the two empirical examples, we can look at a complete score-card by 
parameter-dimension.  First consider the summary for the Tobit model in Table~\ref{Tobit.Scorecard} that summarizes each of the
four diagnostics.  Only the Heidelberger and Welch test for Murder Rate, and only for the halfwidth part of the test, fails
here.  Such a small deviation is not worth worrying about given the other results.  It is important to remember that these are
all \emph{distributional} hypothesis tests and so running multiple parameters through multiple tests will lead to some tail
values.  To be more concrete, suppose that we ran a Geweke test on a model with 100 parameters and set $\alpha=0.05$ for the
difference of means test.  Then we would \emph{expect} about 5 parameters to fail under the null hypothesis that the complete
chain had converged.

The scorecard for the Military Personnel model in Table~\ref{Military.Scorecard} does not show such positive results. Overall
the Raftery and Lewis diagnostic gives the most pessimistic view of convergence, even though it is the bluntest instrument.
On the opposite end of the scale the Heidelberger and Welch diagnostic is the most positive with six of the nine parameter
dimensions passing the test.  The two most popular diagnostics disagree on $\alpha_{\mu}$, $\beta_{1,\mu}$,$\beta_{3,\mu}$,
and $\beta_{3,\tau}$.  This is actually a useful outcome because it demonstrates that the diagnostics are looking at 
different characteristics of stability.  Such differences highlight the importance of using multiple diagnostic tools.

\begin{table}[h] 
\parbox[c]{\linewidth}{
    \blstable
    \tabletitle{\textsc{Diagnostic Scorecard, Military Personnel Model}}\label{Military.Scorecard}
    \vspace{6pt}
    \begin{tabular}{lrrrr}
			    & Geweke & Gelman-Rubin & Heidelberger-Welch & Raftery-Lewis \\
	    \hline 
	    $\alpha_{\mu}$   & \ding{55} 		& \ding{51}	& \ding{51}	& \ding{55}	\\
	    $\alpha_{\tau}$  & \ding{51}		& \ding{51}	& \ding{51}	& \ding{55}	\\
	    $\beta_{1,\mu}$  & \ding{51}		& \ding{55}	& \ding{51}	& \ding{55}	\\
	    $\beta_{1,\tau}$ & \ding{55}		& \ding{55}	& \ding{51}	& \ding{55}	\\
	    $\beta_{2,\mu}$  & \ding{55}		& \ding{55}	& \ding{55}	& \ding{55}	\\
	    $\beta_{2,\tau}$ & \ding{55}		& \ding{55}	& \ding{55}	& \ding{55}	\\
	    $\beta_{3,\mu}$  & \ding{55}		& \ding{51}	& \ding{51}	& \ding{51}	\\ 
	    $\beta_{3,\tau}$ & \ding{51}		& \ding{55}	& \ding{51}	& \ding{51}	\\ 
	    $\tau_c$         & \ding{55}		& \ding{55}	& \ding{55}	& \ding{55}	\\
    \end{tabular} 
}
\end{table} 

\subsection{Other Empirical Diagnostics}
There are a wealth of additional diagnostics that are less convenient since they are not provided by \boa\ and \coda.  These
others generally require some programming or time spent understanding public domain code.  Cowles\index{authorindex}{Cowles, M.
K.} and Carlin \index{authorindex}{Carlin, B. P.} (1996, p.890) provide a very nice tabular summary of the 13 most popular methods
as of 1996 (some time ago in MCMC-terms).  Early debates on diagnostic strategies centered essentially on the ``one long chain''
\index{subjectindex}{convergence diagnostic!Geweke!long run}  versus the ``many short chain'' perspectives
\index{subjectindex}{convergence diagnostic!Gelman and Rubin!many short runs}  emblemized by Geweke\index{authorindex}{Geweke, J.}
versus Gelman\index{authorindex}{Gelman, A.} and Rubin.\index{authorindex}{Rubin, D. B.}  Essentially both sides won the debate in
that many of the current strategies can be characterized as ``many long run'' methods since computing power has become
significantly more economical.  What is very clear from published discussions is that it is always possible to find some example
that ``fools'' a given convergence diagnostic (see the discussion of the Gelman\index{authorindex}{Gelman, A.} and Rubin
\index{authorindex}{Rubin, D. B.} [1992b] and Geyer\index{authorindex}{Geyer, C. J.} [1992] papers).  The following are very brief
descriptions of some additional proposed diagnostics that require individual coding for each model application.  Because these are
customized for each individual application, they cannot be ``canned'' in \bugs\ \index{subjectindex}{bugs@\bugs!diagnostics} or
other software.

{\bf Zellner and Min (1995).}\label{zellner.min.diag}\index{subjectindex}{convergence diagnostic!Zellner and Min}
\index{authorindex}{Zellner, A.}\index{authorindex}{Min, C-K.}
This diagnostic was first introduced here in Chapter~\ref{MCMC.Theory.Chapter}, but without giving the details below.  Assume that
a $\T$ vector of unknown parameter estimates can be segmented into two parts: ($\T_1, \T_2$), which is easy in the case of Gibbs
sampling.  From the definition of conditionals, we know that $ \pi(\T_1, \T_2|\D) = \pi(\T_1|\T_2,\D)\pi(\T_2|\D) =
\pi(\T_2|\T_1,\D)\pi(\T_1|\D)$, for given $\D$ indicating both the data and the prior.  If $\hat{\pi}_n(\T_1, \T_2,\D)$ (note the
``hat'' on $\hat{\pi}$) is the vector position at the candidate stopping point of the chain at iteration $n$, then the
``difference convergence criterion'' \index{subjectindex}{difference convergence criterion} statistic is formed by the
differential Rao-Blackwellized\index{subjectindex}{Rao-Blackwellization}  estimate (discussed below) of the conditional times the
current value, and is close to zero if the chain has converged:
$\hat{\eta}_n=\pi(\T_1|\T_2,\D)\hat{\pi}_n(\T_2|\D)-\pi(\T_2|\T_1,\D)\hat{\pi}_n(\T_1|\D)$, where the distribution
$\hat{\pi}(\T_i|\D)$ comes from a smoothed empirical estimate.\index{subjectindex}{smoothing} Sometimes there is a natural
segmentation of the $\T$ vector into the two components, but in general this is an artificial decision made by the researcher.  A
related quantity based on the same identity, called the ``ratio convergence criterion,'' \index{subjectindex}{ratio convergence
criterion} is given by the ratio: $\hat{\gamma}_n = (\pi(\T_1|\T_2,\D)\hat{\pi}_n(\T_2|\D))/
\pi(\T_2|\T_1,\D)\hat{\pi}_n(\T_1|\D))$, which should be approximately one at convergence.  Zellner\index{authorindex}{Zellner,
A.} and Min\index{authorindex}{Min, C-K.} also give the procedure for obtaining the posterior odds for the hypothesis $\Hz \eta=0$
versus the hypothesis $\Ho \eta \ne 0$, with equal probability priors for the two alternatives and a Cauchy prior on $\eta$.
Using a reasonably large number of preceding values for $\hat{\eta}_i$ ($i<n$), we can calculate an empirical variance with the
slightly unrealistic assumption of serial independence: $\hat{\phi}_n$.  So the posterior odds of $H_0$ over $H_1$ are: $K_{OA} =
\sqrt{N\pi/2}( 1+(\hat{\eta}_n^2)(n\hat{\phi}_n^2) ) \exp[ -(\hat{\eta}_n^2)(2\hat{\phi}_n^2) ]$.

{\bf Ritter and Tanner (1992).}\label{ritter.tanner.diag}\index{subjectindex}{convergence diagnostic!Ritter and Tanner}  
The ``Gibbs stopper''\index{subjectindex}{Gibbs stopper} of Ritter\index{authorindex}{Ritter, C.} and Tanner
\index{authorindex}{Tanner, M. A.} assigns importance weights (see Chapter~\ref{MC.Chapter}, Section~\ref{SIR.Section}) to each
iteration of a Gibbs sampler so that a comparison is made between the current draw and a normalized expression for the joint
posterior density.  This method supports both a single chain or multiple chains (by segmenting the single chain) and assesses the
bias of the full joint distribution of the Gibbs sampler (only).  The Gibbs stopper, however, can be slow in complex models.
\index{subjectindex}{bias!Markov chain Monte Carlo results}

{\bf Roberts (1992, 1994).}\index{subjectindex}{convergence diagnostic!Roberts}  \index{authorindex}{Roberts, G. O.}
This ``distance evaluation'' diagnostic, initially restricted to the Gibbs sampler, gives a bias assessment of the chain
simultaneously across all dimensions with a graphic summary.  The idea is to cycle through multiple Gibbs samplers (generalized to
other forms in 1994) in a stipulated order and then cycle back in the reverse order, creating a dual
chain\index{subjectindex}{dual chain}.  Roberts\index{authorindex}{Roberts, G. O.} suggests as many as 10 to 20 separate chains
started from overdispersed starting points.  The resulting statistic compares within and between behavior with a summary that has
known asymptotics.  \index{subjectindex}{bias!Markov chain Monte Carlo results}

{\bf Liu, Liu, and Rubin (1992).}\index{subjectindex}{convergence diagnostic!Liu, Liu, and Rubin}
\index{authorindex}{Liu, C.}\index{authorindex}{Liu, J. S.}\index{authorindex}{Rubin, D. B.}
These authors posit a ``global control variable''\index{subjectindex}{global control variable} to estimate the bias of the full
joint distribution using multiple chains.  The test statistic is a function of this variable whose expected value is the variance
of the ratio of the posterior at the distribution implied by the current iteration to that at the limiting distribution.  It is
then necessary to use either graphical methods or the test setup of Gelman\index{authorindex}{Gelman, A.} and
Rubin\index{authorindex}{Rubin, D. B.} to make stopping decisions.  

{\bf Yu and Mykland (1998).}\index{subjectindex}{convergence diagnostic!Yu and Mykland}
\index{authorindex}{Yu, B.}\index{authorindex}{Mykland, P.}
A barely customized and particularly easy-to-implement diagnostic is the CUSUM plot (see also Brooks [1998b]).
\index{subjectindex}{CUSUM plot}\index{authorindex}{Brooks, S. P.} This plot is created by first calculating a running tally of
the following statistic from the post-burn-in chain values ($1,\ldots,t$): \index{subjectindex}{burn-in period of a chain}
$S_t = \sum_{i=1}^{t} (\theta_t-\bar{\theta})$, and then plotting this cumulative sum against the time index.  The argument is
that smoothness in the resulting graph and a tendency to spend large periods away from zero are indications of slow mixing.  Note
that this is essentially the information provided in a plot such as the left-hand side of Figures~\ref{Tobit.Traceplots} and
\ref{Military.Traceplots}.

{\bf Mykland, Tierney, and Yu (1995).}\index{subjectindex}{convergence diagnostic!Mykland, Tierney, and Yu}
\index{authorindex}{Mykland, P.}\index{authorindex}{Tierney, L.}\index{authorindex}{Yu, B.}
This diagnostic is based on splitting a Markov chain into small sets according to renewal criteria.\index{subjectindex}{renewal
criteria}  A Markov chain is \emph{regenerative} \index{subjectindex}{Markov chain!regenerative} (a less restrictive form of
renewal) \index{subjectindex}{Markov chain!renewal} if there are times when future values are fully independent of past values and
all future values are iid.  \index{subjectindex}{iid}
Notationally, a renewal set $A$ exists if for a given real $0<\epsilon<1$ and marginal probability
$\eta(B)$ on another set $B$: $p(\theta_{n+1} \in B|\theta_n) \ge \epsilon \eta(B),\; \forall B$ (see
Robert\index{authorindex}{Robert, C. P.} 1995, Section 3 for additional details and examples).  The diagnostic is a graphical
summary of the regeneration probability from iteration to iteration (and thus implemented more directly with the
Metropolis-Hastings algorithm and on discrete state spaces), where stability indicates convergence.  This procedure usually
involves modifying the algorithm to perform the regeneration calculations as it runs (as opposed to summarizing existing chain
runs), and the resulting hybrid processes can be complex (see: Johnson\index{authorindex}{Johnson, V. E.} [1998], and
Gilks,\index{authorindex}{Gilks, W. R.} Roberts\index{authorindex}{Roberts, G. O.}, and Sahu\index{authorindex}{Sahu, S. K.}
[1998]).  Giakoumatos (2005) \index{authorindex}{Giakoumatos, S. G.} produce a convergence diagnostic based on
subsampling of this method and drawbacks for unobserved ARCH models, and Flegal \index{authorindex}{Flegal, J. M.}
provides a general convergence test based on subsampling bootstrap methods.  Other than such specialized settings there has
not been much new work in this area since the 1990s and the four main formal diagnostics described in this chapter dominate 
applied work.
\index{subjectindex}{converge diagnostic!empirical|)}

\subsection{Why Not to Worry Too Much about Stationarity} \index{subjectindex}{Markov chain!stationary distribution}
Considerable attention has been given to convergence diagnostics (here and elsewhere) for good reason: Markov chains that are not
mixing through the target distribution do not provide useful inferential values.  In this chapter we have seen a number of ways
for conveniently asserting convergence or nonconvergence.  However, it can be shown that these efforts are relatively cautious in
their practical implications with applied work (Robert and Casella 2004, 461).  While finite runs of the chain always fail to
achieve asymptotic properties, each step of the chain does bring us closer, in probability, to a realization from the stationary
(invariant) distribution (Robert\index{authorindex}{Robert, C. P.} 2001, p.303).  Notably, Geyer\index{authorindex}{Geyer, C. J.}
(1991, p.158) points out that the accuracy of calculated quantities depends on the adequacy of the burn-in period, which can never
be validated for certain.\index{subjectindex}{burn-in period of a chain} This does not mean that non-asymptotic results are
necessarily useless. In fact, the ergodic theorem guarantees that averages from  sample values give strongly consistent estimates
(Meyn and Tweedie 1993), and therefore the resulting inferences are no less reliable than other sample-based procedures.
\index{authorindex}{Meyn, S. P.} \index{authorindex}{Tweedie, R. L.} \index{subjectindex}{Markov
chain!properties!ergodicity}\index{subjectindex}{ergodic}

Recall the discussion in Section~\ref{defining.reaching.convergence}, where it was shown that convergence to stationarity is
different from convergence of empirical averages.  The arbitrary statistic of interest, $h(X)$, from a Harris recurrent Markov
chain, is summarized with $\bar{h} = \frac{1}{n}\sum_{i=1}^{n} h(x_i)$, with the property: $\sum_{i=1}^{n} h(X_i)/n
\longrightarrow E_fh(X)$ as $n \rightarrow \infty$.  The difference between the expected value of $h(X)$ in the true distribution
at time $n$, and the expected value of $h(X)$ in the stationary distribution, is:
\begin{equation}
		\biggl[ \frac{1}{n}\sum_{i=1}^{n} h(X_i) - E_{x_0}h(X) \biggr] - 
		\biggl[ E_fh(X) -  E_{x_0}h(X) \biggr]
		\rightarrow 0 \mbox{ as } n \rightarrow \infty.
	\end{equation}
\index{subjectindex}{Markov chain!properties!Harris recurrent}

For a geometrically ergodic Markov chain, this quantity converges geometrically fast to zero since $||E_fh(x)-E_{x_0}h(x)|| \le k
\delta^n$ for some positive $k$, and $\delta \in (0:1)$.\index{subjectindex}{ergodic!geometric}  The first bracketed term is the
difference between the empirical average and its expectation at time $n$.  Except at the trivial point where $n=0$, these are
never non-asymptotically equivalent values \emph{and this demonstrates that even in stationarity, the empirical average has not
converged}.  In fact, all we know for certain from the central limit theorem is that:\index{subjectindex}{central limit theorem}
$(\frac{1}{n}\sum_{i=1}^{n} h(X_i) -  E_{x_0}h(X))/(\sigma/\sqrt{n})$ converges in distribution to a standard normal, but does not
bring along convergence of empirical averages.  

Thus, convergence to stationarity has no bearing on the convergence of the empirical averages, which are usually of primary
interest. Moreover, if stationarity is still a concern, merely start the chain in the stationary distribution (easy for
Metropolis-Hastings because the stationary distribution is explicitly given) by applying an accept-reject method (a simple trial
method based on testing candidate values from some arbitrary instrumental density; see Robert\index{authorindex}{Robert, C. P.}
and Casella\index{authorindex}{Casella, G.} [2004, Chapter~1]) until a random variable from the stationary distribution is
produced, then run the chain forward from there.  This works because a chain started in the stationary
distribution\index{subjectindex}{Markov chain!stationary distribution} remains in the stationary distribution, and it is typically
the case that the wait for the accept-reject algorithm to produce a random variable from the stationary distribution will be
shorter than any reasonable burn-in period. \index{subjectindex}{burn-in period of a chain}

\index{subjectindex}{Markov chain Monte Carlo (MCMC)!convergence!acceleration}
\section{Mixing and Acceleration}
Simple social science model specifications, even those with hierarchical features, often mix through the target distribution
rather easily with standard MCMC kernels and present no estimation challenges with MCMC procedures.  Conversely, mixing problems
most commonly occur in high-dimension state spaces.  The example models given with the \bugs\
\index{subjectindex}{bugs@\bugs!furnished examples} software are very well-behaved in this regard, but as model specifications get
larger (more parameters to estimate) and more complex, it is often the case that even when the chain has converged, it will move
slowly through the stationary distribution.  The problem with this is that if the chain is stopped too early, then empirical
estimates will be biased.  \index{subjectindex}{bias!Markov chain Monte Carlo results}

Slow mixing chains also make the problem of convergence diagnosis more difficult because a chain that has not fully explored the
limiting distribution will give biased results based on a subset of the appropriate state space.  \index{subjectindex}{Markov
chain!mixing} \index{subjectindex}{bias!Markov chain Monte Carlo results}

While the problem of mixing diminishes with increasingly powerful desktop computing, it is still important to have some tools that
can increase mixing rates.  These typically involve ``tuning'' (case-specific model estimation changes) since in general they are
modifications of standard algorithms that are done on a case-by-case basis.\index{subjectindex}{tuning} We will also provide a
completely different class of approaches to solving this problem in the next chapter based on modifying the transition kernel in
order to facilitate easier traversal of the chain.

\subsection{Reparameterization}\label{Reparameterization.Section}
Often slow mixing through the target distribution can be attributed to high correlation between model parameters.  This is
particularly exacerbated with the Gibbs sampler\index{subjectindex}{Gibbs sampler} since each sub-step is restricted to a move
that is \emph{parallel} to an axis by construction, and the full conditional distributions will specify small horizontal or
vertical steps (Nandram and Chen 1996).   \index{authorindex}{Nandram, B.} \index{authorindex}{Chen, M-H.} Therefore correlations,
which by definition define nonparallel structures in the posterior, will tend to restrict free movement through the parameter
space, resulting in small steps and poor mixing properties.  High intra-parameter correlation is also a problem with the
Metropolis-Hastings algorithm in that it also slows mixing, but is made obvious by observing many rejected candidate values.  So
while the Gibbs sampler will chug along in a limited region without indicating such a problem to the observer, a similarly
affected Metropolis-Hastings algorithm will tend to reject many candidate jumping positions and therefore be more obvious in its
behavior.

	\begin{figure}[h!]
	\begin{center}
	\parbox[l]{\linewidth}{ 
		\centerline{ \epsfig{file=Images/mcmc_utilitarian.figure06.ps,width=5.5in,height=3.5in,clip=,angle=0} } }
		\caption{\textsc{A Comparison of Gibbs Sampling for Normals}}\label{bivariate.norm.gibbs.steps}
	\end{center}
	\end{figure}

As an example consider two parameters with standard normal marginal posteriors and a correlation term, $\rho$.  In
Figure~\ref{bivariate.norm.gibbs.steps} each panel shows the first 10 steps of a Gibbs sampler starting at $(1,-1)$.  The total
Euclidean distance \index{subjectindex}{Markov chain!Euclidean distance traveled}\index{subjectindex}{Markov chain!mixing}%
traveled is an order of magnitude greater for the case where the correlation coefficient is $0.00$ instead of $-0.95$, even though
the two marginal posteriors are defined on the same support.  Notice that the chain on the uncorrelated surface travels about
seven times further in the same number of steps.  This means that the second Markov chain will mix through the target distribution
much more efficiently than the first since it is not constrained from taking big orthogonal moves in two steps. 

Usually this problem is easy to detect since slow mixing leads to underestimates of the variance, an effect that appears clearly
in Figure~\ref{bivariate.norm.gibbs.steps}.  In addition, as already shown, \bugs\ \index{subjectindex}{bugs@\bugs!correlation
matrix} supplies the parameter correlation matrix.  Cowles\index{authorindex}{Cowles, M. K.},
Roberts,\index{authorindex}{Roberts, G. O.} and Rosenthal\index{authorindex}{Rosenthal, J. S.} (1999) used a simulation study of
this problem, also with standard normal marginal distributions, and showed that the variance underestimation can be appreciable.
Worse yet, they demonstrate that discarding iterations by thinning the chain exacerbates the problem.\index{subjectindex}{thinning
chains}

A number of solutions via reparameterization have been suggested to deal with this problem.  
\index{subjectindex}{Markov chain!reparameterization}  In the case of normal and related models simple algebraic forms include:
$\gamma_1 = \theta_1 + \theta_2$, $\gamma_2 = 3(\theta_1 - \theta_2)$ (Gilks\index{authorindex}{Gilks, W. R.} and
Roberts\index{authorindex}{Roberts, G. O.} 1996), centering the covariates \index{subjectindex}{Markov
chain!reparameterization!centering} $\x_j' = \x_j - \bar{\theta_j}$ (Hills\index{authorindex}{Hills, S. E.} and
Smith\index{authorindex}{Smith, A. F. M.} 1992, Zuur, Garthwaite, and Fryer 2002), \index{authorindex}{Zuur, G.}
\index{authorindex}{Garthwaite, P. H.} \index{authorindex}{Fryer, R. J.} centering the covariates by replacing something like
$Y_{ijk} = \mu + \alpha_i + \beta_{ij} + \epsilon_{ijk}$ in the normal linear model with $Y_{ijk} = \eta + \rho_{ij} +
\epsilon_{ijk}$  where $\eta_{i} = \mu + \alpha_i$, and $\rho_{ij} = \mu + \alpha_i + \beta_{ij}$
(Gelfand,\index{authorindex}{Gelfand, A. E.} Sahu,\index{authorindex}{Sahu, S. K.} and Carlin\index{authorindex}{Carlin, B. P.}
1995), or just centering by ``sweeping'' \index{subjectindex}{Markov chain!reparameterization!sweeping} all of the parameter means
into a single term (Vines,\index{authorindex}{Vines, S. K.} Gilks,\index{authorindex}{Gilks, W. R.} and
Wild\index{authorindex}{Wild, P.} 1996).  These reparameterizations are particularistic to the context of the model and the data
and require considerable thought about the forms chosen as it possible to actually make the mixing worse.

\subsection{Grouping and Collapsing the Gibbs Sampler}\label{grouping.collapsing.gibbs}
Liu\index{authorindex}{Liu, J. S.} (1994) suggests altering the conditional calculations of the Gibbs sampler in the following way 
in order to mitigate the effects of high cross-chain correlations.  Using the standard notation, but for only three variables, consider 
the following two modifications:\\[11pt]
	\begin{tabular}{lcr}
	\parbox[l]{.45\linewidth}{
		\begin{center}
		$\qquad$\\[-2pt]
		{\bf {Grouped Gibbs Sampler}}\index{subjectindex}{Gibbs sampler!grouped}
		\begin{align}
			\theta_1^{[j]},\theta_2^{[j]} &\sim \pi(\theta_1,\theta_2|\theta_3^{[j-1]})		\nonumber\\[4pt]
			\theta_3^{[j]}                &\sim \pi(\theta_3|\theta_1^{[j]},\theta_2^{[j]})	\nonumber
		\end{align} 
		\end{center}
	}
	& $\;$ &
	\parbox[r]{.45\linewidth}{
		\begin{center}
		$\qquad$\\[-2pt]
		{\bf {Collapsed Gibbs Sampler}}\index{subjectindex}{Gibbs sampler!collapsed}
		\begin{align}
			\theta_1^{[j]},\theta_2^{[j]} &\sim \pi(\theta_1,\theta_2)				\nonumber\\[4pt]
			\theta_3^{[j]}                &\sim \pi(\theta_3|\theta_1^{[j]},\theta_2^{[j]})	\nonumber
		\end{align} 
		\end{center}
	}
	\end{tabular}\vspace{11pt}

\noindent where the difference is that the collapsed Gibbs sampler first draws from the unconditional joint distribution of
$\theta_1$ and $\theta_2$, whereas the grouped (also called blocked) Gibbs sampler first conditions on $\theta_3$.  So it must be
possible to express and sample from the joint forms dictated by grouping or collapsing (the first two lines using $\theta_1$ and
$\theta_2$ above).

The appeal of grouping and collapsing is that they shift the high correlations from the Gibbs sampling process over to a more
direct random vector generation process (Seewald\index{authorindex}{Seewald, W.} 1992).  This principle means that if there are
difficult components of the Gibbs sampler (with regard to convergence and mixing), then we can devise a ``side process'' to draw
values in a more efficient manner, since high correlations can be ``internalized'' into a single joint draw.

Obviously these modifications are not a panacea; Roberts\index{authorindex}{Roberts, G. O.} and Sahu\index{authorindex}{Sahu, S.
K.} (1997) warn that incautious specification of the blocking can even \emph{slow down} the convergence of the Markov chain (see
the example in Whittaker\index{authorindex}{Whittaker, J.} [1990]).  It is also possible to encounter situations where it is
difficult to sample from the unconditional joint form, although Chen,\index{authorindex}{Chen, M-H.}
Shao,\index{authorindex}{Shao, Q-M.} and Ibrahim\index{authorindex}{Ibrahim, J. G.} (2000, p.78) point out that it is possible to
run sub-chains within the first step to accomplish this (that is, to draw $\theta_1$ and $\theta_2$ from a two-component sub-Gibbs
sampler).\index{subjectindex}{sub-chaining}
	 
The choice of grouping versus collapsing is typically determined by the structure of the problem: the parametric form will
frequently dictate which of the two forms are available.  It is generally better in the presence of correlation problems to use
grouping (Chen,\index{authorindex}{Chen, M-H.} Shao,\index{authorindex}{Shao, Q-M.} and Ibrahim\index{authorindex}{Ibrahim, J. G.}
2000).  The real challenge is to specify the useful configuration of blocking collapsing in the sampler since this is a customized
process outside the province of \bugs.\index{subjectindex}{bugs@\bugs!limitation}

\subsection{Adding Auxiliary Variables}\label{sec:auxiliary.variables}
\index{subjectindex}{Markov chain Monte Carlo (MCMC)!auxiliary variable process}
Another idea for improving mixing and convergence is the strategy of introducing an additional variable into the Markov chain
process as a means of improving mixing and convergence (Besag\index{authorindex}{Besag, J.} and Green\index{authorindex}{Green, P.
J.} 1993, Edwards\index{authorindex}{Edwards, R. G.} and Sokal\index{authorindex}{Sokal, A. D.} 1988,
Swendsen\index{authorindex}{Swendsen, R. H.} and Wang\index{authorindex}{Wang, J. S.} 1987, Mira and Tierney 2001a).  The
principle is to augment a $k$-dimensional vector of coefficients, $\T \in \TT$, with (at least) one new variable, $u \in
\mathbf{U}$, so that the $(k+1)$-dimensional vector on $\TT \times \mathbf{U}$ space has good chain properties.  The augmenting
variables sometimes have direct interpretational value, but this is not necessary as the real goal is to improve the Markov chain.
Actually the Metropolis-Hastings algorithm is already an auxiliary variable process since the candidate-generating distribution
\emph{is} auxiliary to the distribution of interest.\index{subjectindex}{Metropolis-Hastings algorithm}
Higdon\index{authorindex}{Higdon, D. M.} (1998) points out that the Metropolis-Hastings algorithm is an auxiliary variable process
because the use of the uniform random variable for acceptance decisions can be rewritten in this form.
\index{subjectindex}{Markov chain Monte Carlo (MCMC)!auxiliary variable process}

A number of extensions have been developed to deal with circumstances that arise when one of the conditional probability
statements is not directly available or when some less simple alternative to the transition kernel is required
(Fredenhagen\index{authorindex}{Fredenhagen, K.} and Marcu\index{authorindex}{Marcu, M.} 1987).  There is a small literature on
the theoretical properties of auxiliary variable Markov chains (Brooks\index{authorindex}{Brooks, S. P.} 1998b).  Hurn (1997)
\index{authorindex}{Hurn, M.} finds that auxiliary variable algorithms have problems with posterior distributions that are highly
multimodal or have strong interaction components.  More recent developments include a linkage to the slice sampler
(below),\index{subjectindex}{slice sampler} (Wakefield,\index{authorindex}{Wakefield, J. C.} Gelfand,\index{authorindex}{Gelfand,
A. E.} and Smith\index{authorindex}{Smith, A. F. M.} 1991; Besag\index{authorindex}{Besag, J.} and Green\index{authorindex}{Green,
P. J.} 1993).

\subsection{The Slice Sampler}\index{subjectindex}{Markov chain!slice sampler}
The slice sample is a very important tool for modern MCMC software.
As noted, introducing an additional variable into the Markov chain process can improve mixing or convergence.  The strategy with
slice sampling is to augment a $k$-dimensional vector of coefficients, $\T \in \boldsymbol{\Theta}$, with an additional variable,
$\psi \in \boldsymbol{\Psi}$, with the result that the $(k+1)$-dimensional vector on $\boldsymbol{\Theta} \times
\boldsymbol{\Psi}$ space has better mixing through this space (Neal 2003).  \index{authorindex}{Neal, R. M.}

First consider the posterior distribution of $\T$ is given as $\pi(\T)$, where conditioning on the data is assumed.  Now define
$\pi(\psi|\T)$ and the joint posterior distribution given by $\pi(\T,\psi) = \pi(\psi|\T)\pi(\T)$.  Clearly the distribution
$\pi(\psi|\T)$ is picked to be easy to sample from, otherwise the implementation problems are worse.  In total there are two
conditional transition kernels required: one that updates $\T$, $P[\T\rightarrow\T'|\psi]$, and one that updates $\psi$,
$P[\psi\rightarrow \psi'|\T]$.  In the case of Gibbs sampling these are exactly the full conditional distributions:
$\pi(\T'|\psi)$ and $\pi(\psi'|\T)$, meaning that the algorithm cycles normally through the $\T$ statements (now conditional on
one more parameter) and then adds a step for $\psi$.  This two-step aspect of the process ensures that the stationary process
remains $\pi(\T)$ since the detailed balance equation is maintained.  The nice part is that we run the chain normally, presumably
with better mixing, and then just discard the $\psi$ values.  \index{subjectindex}{detailed balance
equation}\index{subjectindex}{Markov chain!Gibbs sampler}

Generally the added variable in the slice sampler is a uniform draw, so we update the marginals and the joint distribution 
with uniforms in the following way:
\begin{equation}
		\psi^{[j+1]}|\T^{[j]} \sim \mathcal{U}(0,\pi(\T^{[j]}) \qquad
			\T^{[j+1]}|\psi^{[j+1]} \sim \mathcal{U}(\T\range\pi(\T^{[j+1]})>\psi^{[j+1]})
\end{equation}
in going from the $j$th to the $(j+1)$st step.  In the basic setup the target distribution is expressed as a product of the
marginals, $\pi(\theta) = \prod\pi(\theta_i)$.  This means that the auxiliary variable is sampled from a uniform bounded by zero
and the $\T$, then these $\T$ are in turn sampled from a uniform bounded below by the auxiliary variable(s).
\index{subjectindex}{auxiliary variables}

Roberts\index{authorindex}{Roberts, G. O.} and Rosenthal\index{authorindex}{Rosenthal, J. S.} (1998) show that the slice sampler
has many good theoretical properties, such as geometric convergence, thus making it one of the more successful auxiliary variable
applications (see also Mira and Tierney [2001b]).  For specific applications see Damien,\index{authorindex}{Damien, P.}
Wakefield,\index{authorindex}{Wakefield, J. C.} and Walker\index{authorindex}{Walker, S.} (1999),
Tierney\index{authorindex}{Tierney, L.} and Mira\index{authorindex}{Mira, A.} (1999), as well as Robert\index{authorindex}{Robert,
C. P.} and Casella\index{authorindex}{Casella, G.} (2004, Chapter~8) for a list of difficulties.  \index{subjectindex}{Markov
chain!geometric convergence} \index{subjectindex}{Markov chain!auxiliary variables} 
\index{authorindex}{Robert, C. P.} \index{authorindex}{Casella, G.}

\index{authorindex}{Altman, M.} \index{authorindex}{Gill, J.} \index{authorindex}{McDonald, M. P.}
A simple example given by Robert and Casella (1999), and implemented in \R\ by Altman, Gill, and McDonald (2003), uses a uniform
distribution to generate desired normals.  Begin with at target $f(\theta) \propto \exp(-\theta^2/2)$, and stipulate a uniform such 
that:
\begin{equation}
	      \psi|\theta \sim U[0,\exp(-\theta^2/2)], \qquad\qquad
	      \theta|\psi \sim U[-\sqrt{-2\log(\psi)},\sqrt{-2\log(\psi)}],      \nonumber
\end{equation}
which makes this $\psi$ an auxiliary variable with direct sampling properties.  Notice that the first statement could easily be 
modified to generate other forms.  The \R\ code to implement this follows directly:
\begin{R.Code}
n <- 1000; t.vals <- 0; p.vals <- 0
for (i in 2:n)  \{
    p.vals <- c(p.vals,runif(1,0,exp(-0.5*t.vals[(i-1)]^2)))
    t.vals <- c(t.vals,runif(1,-sqrt(-2*log(p.vals[i])),
                sqrt(-2*log(p.vals[i]))))
\}
\end{R.Code}
The resulting \texttt{t.vals} draws are displayed in Figure~\ref{slice.fig}, which shows them to closely resemble normally
distributed random variables.  We can consider this particular example as an MCMC version of the Box-M\"{u}ller algorithm (1958).
\index{authorindex}{Box, G. E. P.}\index{authorindex}{Muller, M. E.@M\"{u}ller, M. E.}
\index{subjectindex}{Box-Muller method@Box-M\"{u}ller method}

\begin{figure}[h!]
\begin{center}
\parbox[l]{\linewidth}{
		\hspace{0.1in}
		\centerline{ \epsfig{file=Images/mcmc_utilitarian.figure01.ps,width=5.8in,height=3.2in,clip=,angle=0} } }
\caption{\textsc{Slice Sampler Output}} \label{slice.fig}
\end{center}
\end{figure}

\section{Chib's Method for Calculating the Marginal Likelihood Integral}\label{Section:chibs.method}
\index{authorindex}{Jeliazkov, I.} \index{authorindex}{Geweke, J.} \index{authorindex}{Chib, S.}
As noted in Chapter~\ref{Testing.Chapter}, the integrals in Bayes Factors can be challenging to estimate with reasonably realistic
models.  Usually hierarchical specifications lead to these complications, but hierarchical modeling is one of the strengths of the
Bayesian paradigm.  One general class of tools for getting these marginal likelihoods is provided by Chib (1995) for Gibbs
sampling and Chib and Jeliazkov (2001) for Metropolis-Hastings.  See also Geweke (2005, pp.257-261).  \index{authorindex}{Geweke, J.} 
The particular objective is to
calculate $p(\x|M_i) = \int_{\T}f_i(\x|\T_i) p_i(\T_i)d\T_i$ in \eqref{bayes.Factor.1} (page~\pageref{bayes.Factor.1}) for the
$i$th model.  It is easier to look at this quantity if we rearrange Bayes' Law and take logs (dropping the model index since we
will consider only one model here for now):

\begin{equation}\label{log.marginal.likelihood}
		\log p(\x) = \ell(\T'|\x) + \log p(\T') - \log \pi(\T'|\x)
\end{equation}
where $\T'$ is a completely arbitrary (but acceptable) point in the sample space.  Typically we will choose a point from the high
density region such as the posterior mean.  So if we had an estimate of $\pi(\T'|\x)$ from simulation, this would be a
straightforward calculation.  We will now describe the method for Metropolis-Hastings.  The Gibbs sampling version, a direct
extension of data augmentation (Tanner and Wong 1987) is described clearly in Chib (1995), and the algorithm is described in
Chapter~\ref{Testing.Chapter}, Exercise~\ref{exercise.chib.greenberg} (page~\pageref{exercise.chib.greenberg}).  
The Metropolis-Hastings case (Chib and Jeliazkov 2001) is particularly elegant since it is tied to the acceptance process.  
Consider the standard form of the candidate acceptance probability for a generic Metropolis-Hastings setup:
\index{authorindex}{Chib, S.} \index{authorindex}{Tanner, M. A.} \index{authorindex}{Wong, W. H.}
\index{authorindex}{Chib, S.} \index{authorindex}{Jeliazkov, I.}
\begin{equation}\label{MH.acceptance.criteria}
	    \alpha(\T',\T) = \text{min}\left[ \frac{\pi(\T')}{\pi(\T)} \frac{q_t(\T|\T')}{q_t(\T'|\T)}, 1 \right].
\end{equation}
Recall that the probability of transitioning to arbitrary point $\T'$ is the probability that the candidate-generating distribution 
produces $\T'$ times the probability that it is accepted from above:
\begin{equation}
		p(\T,\T') = q(\T'|\T)\alpha(\T',\T)
\end{equation}
such that for any arbitrary point, the detailed balance equation, \eqref{reversibility.equation} from 
page~\pageref{reversibility.equation}, can be expressed as:	\index{subjectindex}{detailed balance equation}
	\begin{equation}
	    \pi(\T)q(\T'|\T)\alpha(\T',\T) =  \pi(\T')q(\T|\T')\alpha(\T,\T').
	\end{equation}
The innovation is to now take integrals of both sides with respect to $\T$, realizing that $\pi(\T')$ is a function evaluation 
at an arbitrary point and can therefore be moved outside of the integral.  Doing this and rearranging slightly gives:
	\begin{equation}
		\pi(\T') = \frac{ \int\TT \pi(\T)q(\T'|\T)\alpha(\T',\T)d\T }{ \int_\TT q(\T|\T')\alpha(\T,\T')d\T },
	\end{equation}
which is really just a ratio of two expected value calculations:
\begin{equation}\label{chib2.equation}
		\pi(\T') = \frac{ E_{\pi(\T)}[q(\T'|\T)\alpha(\T',\T)] }{ E_{q(\T|\T')}[\alpha(\T,\T')] }.
\end{equation}
Of course what expected value calculations do is set us up for simulation solutions, and what Chib and Jeliazkov realized is that
in the course of running a standard Metropolis-Hastings algorithm for marginal posterior distributions, we can get the marginal
likelihood along the way with not much extra trouble.  So replace \eqref{chib2.equation} with its simulation analog:
\begin{equation}
		\pi_{\text{sim}}(\T') = \frac{ \frac{1}{M}\sum_{m=1}^M \alpha(\T',\T_m) q(\T'|\T_m) }
					     { \frac{1}{N}\sum_{n=1}^N \alpha(\T_N,\T')             },
\end{equation}
which of course uses known functions and values readily at hand.  Typically, but not by necessity, $M=N$ here.  

Recall that $\T'$ is chosen arbitrarily but within some high density region of the posterior.  So this process substitutes a
difficult integration process with simulation of the posterior density at a single point by completing
\eqref{log.marginal.likelihood} with the simulated result:
\begin{equation}
		\log p_{\text{sim}}(\x) = \ell(\T'|\x) + \log p(\T') - \log \pi_{\text{sim}}(\T'|\x).
\end{equation}
The other quantities on the right-hand side are readily available, so the marginal likelihood is calculated.

\section{Rao-Blackwellizing for Improved Variance\\ Estimation}\index{subjectindex}{Rao-Blackwellization|(}
Gelfand\index{authorindex}{Gelfand, A. E.} and Smith\index{authorindex}{Smith, A. F. M.} (1990) introduced the idea of
``Rao-Blackwellization'' as a means of exploiting the conditional nature of Gibbs sampling in order to produce improved variance
estimates for statistics of interest through conditioning (the pre-MCMC simulation idea appears to originate with the text of
Hammersley\index{authorindex}{Hammersley, J. M.} and Handscomb\index{authorindex}{Handscomb, D. C.} [1964]).  This technique can
also be applied to the Metropolis-Hastings algorithm (Casella\index{authorindex}{Casella, G.} and
Robert\index{authorindex}{Robert, C. P.} 1996), data augmentation (Liu,\index{authorindex}{Liu, J. S.} Wong,
\index{authorindex}{Wong, W. H.} and Kong\index{authorindex}{Kong, A.} 1994), EM (Meng\index{authorindex}{Meng, X-L.} and
Schilling\index{authorindex}{Schilling, S.} 1996), nonparametric Bayes (Liu\index{authorindex}{Liu, J. S.} 1996b), to detect
outliers (Haro-L\'{o}pez,\index{authorindex}{Haro-L\'{o}pez, R.} Mallick,\index{authorindex}{Mallick, B. K.} and
Smith\index{authorindex}{Smith, A. F. M.} 2000), and also in general non-Bayesian procedures.

The Rao-Blackwell Theorem\index{subjectindex}{Rao-Blackwell Theorem} states that estimates conditioned on sufficient statistics
\index{subjectindex}{sufficient statistic} are subsequently unaffected or improved in terms of the variance of their sampling
distribution.  When applied to variance calculations (the interest here), the theorem can be expressed through the relationship
between conditional and unconditional variance and is not always based on sufficient statistics.  Start with a statistic of
interest such as a mean or quantile for one parameter of a two-parameter Markov chain: $h(\theta_1)$.  The variance relationship
is defined by:
\begin{equation}
		\Var[h(\theta_1)] = \Var[E(h(\theta_1|\theta_2))] + E(\Var[h(\theta_1|\theta_2)]). 
\end{equation}
So by simply rearranging we can show that the variance of the expected value of the conditional cannot be lower since variances 
are never negative:
\begin{equation}
		\Var[E(h(\theta_1|\theta_2)] = \Var[h(\theta_1)] - E(\Var[h(\theta_1|\theta_2)]), 
\end{equation}
provided that the method for producing the Markov chain values does not introduce exacerbating correlations between terms (see
Liu,\index{authorindex}{Liu, J. S.} Wong,\index{authorindex}{Wong, W. H.} and Kong\index{authorindex}{Kong, A.} [1995] and
Geyer\index{authorindex}{Geyer, C. J.} [1995] for specifics).  A standard means of comparison is the asymptotic relative
efficiency (ARE), \index{subjectindex}{asymptotic relative efficiency} defined by the ratio of the variance of the limiting
distribution of two estimates.  In the case of a two-variable Gibbs sampler, these are for $\theta_1$:
\begin{align}\label{rao.black-are}
		\underset {n\rightarrow\infty} {\Var[h(\theta_1)]} &= \Var[h(\theta_1)]
					+ 2\sum_{i=1}^{\infty}\Cov(h(\theta_1^{[0]}),h(\theta_1^{[i]}))	\nonumber\\
		\underset {n\rightarrow\infty} {\Var[E(h(\theta_1|\theta_2))]} &= \Var[E(h(\theta_1|\theta_2))]
					\nonumber\\
					&\qquad\qquad\qquad
						+ 2\sum_{i=1}^{\infty}\Cov(E[h(\theta_1^{[0]}|\theta_2^{[0]})],
						E[h(\theta_1^{[i]}|\theta_2^{[i]})] ),
\end{align}
where the brackets in the super-script denote Markov chain iterations.  Levine\index{authorindex}{Levine, R.} (1996) showed that 
the ARE of the first unconditional variance over the Rao-Blackwellized variance is always greater than or equal to one, thus 
favoring use of the Rao-Blackwellized version.

Fortunately, the generalization of this last result issue is not typically a problem in that most conventional applications 
and a number of authors have provided suitable conditions.  For instance, Liu,\index{authorindex}{Liu, J. S.} Wong,
\index{authorindex}{Wong, W. H.} and Kong's\index{authorindex}{Kong, A.} (1994, 1995) identification of an 
\emph{interleaving property} is satisfied by the conditions:
\begin{bayeslist}
		\item 	$\theta_1^{[i]}$ and $\theta_1^{[i+1]}$ are conditionally independent given 
			$\theta_2^{[i]}$,
		\item 	$\theta_2^{[i-1]}$ and $\theta_2^{[i]}$ are conditionally independent given 
			$\theta_1^{[i]}$,
		\item	and the chain has converged such that $(\theta_1^{[i]},\theta_2^{[i-1]})$, 
			$(\theta_1^{[i]},\theta_2^{[i]})$ are iid.
\end{bayeslist}
\index{subjectindex}{iid}

The unconditional and Rao-Blackwellized empirical estimate of $h(\theta_1)$ from $n$ post-convergence Markov chain values are given 
by the ergodic theorem as: \index{subjectindex}{ergodic theorem}
\begin{equation}
		h(\theta_1)_{uc} = \frac{1}{n}\sum_{i=1}^n h(\theta_1^{[i]}),	\qquad
		h(\theta_1)_{rb} = \frac{1}{n}\sum_{i=1}^n E(h(\theta_1^{[i]}|\theta_2^{[i]}).	
\end{equation}
Furthermore, both of these estimators are unbiased and are known to converge to the marginal density $h(\theta_1)$.

The general strategy of Gelfand\index{authorindex}{Gelfand, A. E.} and Smith\index{authorindex}{Smith, A. F. M.} (1990) is
therefore to condition each parameter estimate on the others: if $\pi(\theta_i)$ is a marginal density estimate
(smoothed\index{subjectindex}{smoothing} or empirical), then the Rao-Blackwellized estimate can be calculated by
$\hat{\pi}(\theta_i) = \frac{1}{k-1}\sum_{j=1}^{k-1}\pi(\theta_i|\theta_{j,-i})$, for k parameters as an empirical estimate of
\index{subjectindex}{Rao-Blackwellization!conditioning on parameters} $\int_{\theta_j}
\pi(\theta_i|\theta_{j,-i})\pi(\theta_{j,-i})d\theta_{j,-i}$.  It turns out that this is not too hard to do and can lead to vastly
improved variance estimates.  

Other more elaborate schemes can also be implemented.  For instance if there exists a parallel chain structure (as in the
Raftery\index{authorindex}{Raftery, A. E.} and Lewis\index{authorindex}{Lewis, S. M.} diagnostic) 
\index{subjectindex}{parallel Markov chain process}% denoted by $Z_1,\ldots Z_n$, then we can also Rao-Blackwellize by
conditioning on this chain at each iteration: $\hat{\pi}(\theta_i) = \frac{1}{k-1}\sum_{j=1}^{k-1}\pi(\theta_i|\z_{j,-i})$.
\index{subjectindex}{Rao-Blackwellization!conditioning on auxiliary values} Casella\index{authorindex}{Casella, G.} and
Robert\index{authorindex}{Robert, C. P.} (1996) use the rejected values in a Metropolis-Hastings algorithm in the following way.
If $\theta_a$ are the accepted values of the algorithm and $\theta_c$ are the candidate values, only some of which are accepted in
the application of the Metropolis-Hastings algorithm in the process of obtaining $n$ chain values, then the uniform random
variable used to make the accept/reject decision \index{subjectindex}{accept/reject decision} is an ancillary
statistic\index{subjectindex}{ancillary statistic} and can be conditioned on (integrated over).  If the estimator is expressed
usually using only the accepted values:
\begin{equation}
		 h(\theta_1)_{uc} = \frac{1}{n}\sum_{i=1}^n h(\theta_a^{[i]}),		
\end{equation}
then we can use an indicator function to re-express to include the rejected values:
\begin{equation}
		h(\theta_1)_{rb} = \frac{1}{n}\sum_{i=1}^n \left[ h(\theta_c^{[i]}) 
				   \sum_{j=i}^{n}I_{(\theta_a^{[i]} = \theta_c^{[i]})} \right].
\end{equation}
\index{subjectindex}{Metropolis-Hastings algorithm!acceptance ratio}
Recall that the acceptance ratio was defined in Chapter~\ref{MCMC.Chapter} by $a(\theta_c,\theta_{i-1})$ and the acceptance
probability of the candidate value $k$ versus status quo value $i$ ($\rho_{ik}$) is the minimum of $a(\theta_c,\theta_{i-1})$ and
$1$.  Now define at the $i^{th}$ step going forth by the $j$:
\vspace{-6pt}\begin{align}
		\xi_{ij} &= \prod_{k=i+1}^{j} (1-\rho_{ik}) \quad i<j, \qquad\text{and}\quad \xi_{ii}=1	\nonumber\\
		\delta_i &= p(\theta_a^{[i]} = \theta_c^{[i]}|\theta_c^{[1]},\ldots,\theta_c^{[i]})  
			 = \sum_{j=1}^{i-1}\delta_j\xi_{j(i-1)}\rho_{ji}				\nonumber\\
		\phi_i   &= \delta_i\sum_{j=i}^{n}\xi_{ij}. 				
\end{align}\vspace{-6pt}
Then we get the following version of the Rao-Blackwellized estimator:
\begin{equation}
		h(\theta_1)_{rb} = \frac{1}{n}\sum_{i=1}^n \phi_i h(\theta_c).
\end{equation}
Casella\index{authorindex}{Casella, G.} and Robert\index{authorindex}{Robert, C. P.} subsequently show circumstances where this
leads to a superior estimator over the unconditional version.  This process is not limited in its application to the
Metropolis-Hastings algorithm, and Chen,\index{authorindex}{Chen, M-H.} Shao,\index{authorindex}{Shao, Q-M.} and Ibrahim
\index{authorindex}{Ibrahim, J. G.} (2000) give extensions (page~\pageref{hit.and.run.section}).  
\index{subjectindex}{Hit-and-Run algorithm}

Unfortunately, the application of Rao-Blackwellization is particularistic to each given model, and can require extensive
customized coding of the chain since it is integrally tied into the functioning of the chain.  This may deter a number of users,
despite its appealing characteristics.

\section*{Utilitarian Epilogue}
Some issues covered here are conceptually and managerially simple, like starting points, thinning, and the idea of a burn-in
period.  Most frequent users of MCMC tools fall into good habits and are aware such details need to be considered.  More
important is the consideration of the length of the chain to be run, conditional on the complexity of the model.  Discussion 
ranged herein from the four dominant diagnostic tests to a range of less common alternatives and graphical approaches.  At some
point experienced users with relatively complex model specification leave the comfortable world of \bugs\ and write customized
samplers.  In such endeavors tools like reparameterization, grouping/collapsing, augmentation, and alternatives like the slice
sampler can be enormously helpful in producing efficiently mixing Markov chains.  Rao-Blackwellization from sampler output also
has the potential to improve results with user-developed samplers.  One purpose of the short discussion of these methods in this
chapter is to direct interested researchers into the ongoing literature on sampler development.

\section{Exercises}
\begin{exercises}
	\item 	For the hierarchical model of firearm-related deaths in Chapter~\ref{Model.Quality.Chapter}, perform the following 
            diagnostic tests from Section~\ref{convergence.diagnostic.section} in \coda\index{subjectindex}{coda@\coda!Gelman and Rubin} 
            or \boa\negthinspace:\index{subjectindex}{boa@\boa!Gelman and Rubin} Gelman\index{authorindex}{Gelman, A.} and 
			 Rubin,\index{authorindex}{Rubin, D. B.} Geweke,\index{authorindex}{Geweke, J.} 
			 Raftery\index{authorindex}{Raftery, A. E.} and Lewis,\index{authorindex}{Lewis, S. M.} 
			 Heidelberger\index{authorindex}{Heidelberger, P.} and Welsh.
			 \index{subjectindex}{convergence diagnostic!Raftery and Lewis}
			 \index{subjectindex}{convergence diagnostic!Heidelberger and Welch}
			 \index{subjectindex}{convergence diagnostic!Geweke}
			 \index{subjectindex}{convergence diagnostic!Gelman and Rubin}

            % NEW
	\item	(Robert and Casella 2004, Chapter~8). In Chapter~\ref{Testing.Chapter}, Exercise~\ref{exercise.chib.greenberg}
            (page~\pageref{exercise.chib.greenberg}), samples from the truncated normal 
            distribution $f(x) = k\exp[-\half(x+3)^2]I_{0,1}$ ($k$ a normalizing constant) were generated.  Rejection sampling
            here can be inefficient, so write a slice sampler in \R\ according to the following steps at iteration $t$:
            \begin{enumerate}
                \item   draw $u^{[t+1]} \sim \mathcal{U}_{u} (0\range f(x^{[t]}))$  (the vertical step)    
                \item   draw $x^{[t+1]} \sim \mathcal{U}_{f(x)} (u^{[t+1]}\range 1)$ (the horizontal step)
            \end{enumerate}
            where the second step means that a value of $x$ is drawn uniformly such that  $k\exp[-\half(x+3)^2]I_{0,1} > uf(x^{[t]})$.
            \begin{comment}
            k <- 1/pnorm(1,3,1) - pnorm(0,3,1)
            x <- rep(0.25,100); u <- rep(0.005,100)
            for (i in 2:30)  {
              u[i] <- runif(1,0,k*dnorm(x[(i-1)],3,1))
              w <- u[i]*dnorm(x[(i-1)],3,1)
              print(paste(i,w,-2*log(w)))
              x[i] <- runif(1,0,(sqrt(-2*log(w))-3))
            }
            \end{comment}

	\item   Plot in the same graph a time sequence of 0 to 100 versus the annealing cooling schedules: logarithmic, geometric, 
            semi-quadratic, and linear.  What are the trade-offs associated with each with regard to convergence and processing 
            time?  (\emph{This question is left here to be compatible with the answer key to the Second Edition. Read
            Section~{annealing.section} before attempting.})

            % NEW
	\item	Explain how dramatic increases in computational power has made the 1990s debate about ``one long run'' versus ``many
            short runs'' for assessing convergence unimportant.

	\item   Show how the full conditional distributions for Gibbs sampling in the Bayesian Tobit model on 
		    page~\pageref{Gawande.Full.Conditionals} can be changed into grouped or collapsed Gibbs sampler forms 
            (Section~\ref{grouping.collapsing.gibbs}).

            % NEW
	\item	Write a sampler in \R\ to implement the grouped or collapsed Gibbs sampler from the problem above and use it to
            estimate the marginal posteriors for the death penalty support data (Example~\ref{death.penalty.example}, 
            page~\pageref{death.penalty.example}) by modifying the code in this chapter's {\bf Computational Addendum}.

    \item 	\bugs\ \index{subjectindex}{bugs@\bugs!diag@\texttt{diag}} includes a quick diagnostic command,
			\begin{normalfont}\texttt{diag}\end{normalfont}, which implements a version of Geweke's\index{authorindex}{Geweke, J.} 
            (1992) time-series diagnostic for the first 25\% of the data and the last 50\% of the data.  Rather than calculate 
            spectral densities for the denominator of the $G$ statistic, \bugs\ divides the two periods into 25 equal-sized bins 
			and then takes the variance of the means from within these bins.  Run the example in Section~\ref{Military.Example} 
            using the \bugs\ code in the {\bf Computational Addendum} with 10,000 iterations, and load the data into \R\ with the 
            command \begin{normalfont}\index{subjectindex}{boa@\boa!Geweke}.  
            \texttt{military.mcmc <- boa.importBUGS("my.bugs.dir/bugs")}.  \end{normalfont}  Write a function to calculate the 
            shortcut for Geweke's\index{authorindex}{Geweke, J.} diagnostic for each of the five chains (columns here).  Compare 
            your answer with \bugs.  \index{subjectindex}{convergence diagnostic!Geweke}\index{subjectindex}{time-series!chain values}

            % NEW
    \item	David C. Baldus, Charles Pulaski, and George Woodworth (1983, 1990), 
            \index{authorindex}{Baldus, D. C.}\index{authorindex}{Pulaski, C. A.}\index{authorindex}{Woodworth, G.}
            i.e., the ``Baldus Study,'' looked at the potential disparity in the imposition of the death sentence in Georgia based
            on the race of the murder victim and the race of the defendant.  Using the dataset \texttt{baldus} in \texttt{BaM},
            write a model in \bugs\ or \jags\ with the variable \texttt{sentence} (death sentence imposed: 325 0 cases, 127 1 cases)
            and specify a linear model including a multivariate normal prior (\texttt{dmnorm}).  Modify this prior specification
            to make it a multivariate version of Zellner's g-prior\index{authorindex}{Zellner, A.} 
            \index{subjectindex}{prior distribution!Zellner's g-prior} (Chapter~\ref{Prior.Chapter}, 
            Exercise~\ref{zellner.g.prior.exercise}, on page~\pageref{zellner.g.prior.exercise}).

    \item 	For the model of marriage rates in Italy given in Section~\ref{italy.example}, calculate the Zellner
            \index{authorindex}{Zellner, A.} and Min\index{authorindex}{Min, C-K.} diagnostic with the segmentation: 
            $\T_1 = \alpha$, $\T_2 = \beta$, for the posteriors defined at the iteration points: $\hat{\pi}_{1,000}(\alpha,\beta,\LA,\X)$ 
            and $\hat{\pi}_{10,000}(\alpha,\beta,\LA,\X)$.  Test $\Hz \eta=0$ versus $\Ho \eta \ne 0$ with the statistic $K_{OA}$ 
            assuming serial independence.  \index{subjectindex}{convergence diagnostic!Zellner and Min}

            % NEW
	\item	Write an \R\ function that implements the Ritter and Tanner diagnostic (page~\pageref{ritter.tanner.diag}) and apply
            it to the Tobit death penalty example (starting on \pageref{death.penalty.example}).

    \item   (Raftery\index{authorindex}{Raftery, A. E.} and Lewis\index{authorindex}{Lewis, S. M.} 1996).
            Write a Gibbs sampler in \R\ to produce a Markov chain to sample for $\theta_1$, $\theta_2$, and $\theta_3$,
            which are distributed according to the trivariate normal distribution
            \begin{equation}
                                     \left[ \begin{array}{r} \theta_1\\ \theta_2\\ \theta_3 \end{array} \right]
                                     \sim \mathcal{N}\left(
                                         \left[ \begin{array}{r} 0\\ 0\\ 0 \end{array} \right],
                                         \left[ \begin{array}{rrr} 99 & -7 & -7 \\ -7 & 1 & 0 \\ -7 & 0 & 0
                                                \end{array} \right]
                                     \right) \nonumber
            \end{equation}
            by cycling through the normal conditional distributions with variances:
            $V(\theta_1|\theta_2,\theta_3)=1$, $V(\theta_2|\theta_1,\theta_3)=1/50$ and $V(\theta_3|\theta_1,\theta_2)=1/50$.  
            Contrast the Geweke\index{authorindex}{Geweke, J.} diagnostic with the Gelman\index{authorindex}{Gelman, A.} and
            Rubin\index{authorindex}{Rubin, D. B.} diagnostic for 1,000 iterations.  
            \index{subjectindex}{convergence diagnostic!Geweke}
            \index{subjectindex}{convergence diagnostic!Gelman and Rubin}

            % NEW
	\item	Write a dynamic animation that shows the witch's hat distribution melting with simulated annealing.

  	\item   Prove that data augmentation is a special case of Gibbs sampling.

            % NEW
	\item	The military personnel model in Section~\ref{Military.Example}\index{subjectindex}{example!Warsaw Pact military expenditures}
            fits poorly because of lack of independence in the countries (columns) and the years (rows).  Write the appropriate
            model in \bugs/\jags\ that accounts for one very influential country and the time series effect.  Compare your results
            to those in Table~\ref{Military.Summary.Table}.

    \item 	For the following simple bivariate normal model
			\begin{equation}
				     \left[ \begin{array}{r} \theta_1\\ \theta_2 \end{array} \right] 
				     \sim \mathcal{N}\left( 
			 		 \left[ \begin{array}{r} 0\\ 0 \end{array} \right],		
				 	 \left[ \begin{array}{rr}1&\rho\\ \rho&1\\ \end{array} \right]
				     \right), \nonumber
			\end{equation}
			the Gibbs sampler draws iteratively according to:
			\begin{align}
					\theta_1|\theta_2 &\sim \mathcal{N}(\rho\theta_2, 1-\rho^2)	\nonumber\\
					\theta_2|\theta_1 &\sim \mathcal{N}(\rho\theta_1, 1-\rho^2).	\nonumber
				 \end{align}
			Write a simple implementation of the Gibbs sampler in \R, then calculate the unconditional and conditional 
            (Rao-Blackwellized) posterior standard error, for both $\rho=0.05$ and $\rho=0.95$.  Compare these four values 
            and comment.  \index{subjectindex}{Rao-Blackwellization}

            % NEW
	\item	The Lehmann-Scheff\'{e} theorem \index{subjectindex}{Lehmann-Scheffe theorem@Lehmann-Scheff\'{e} theorem} states that 
            an unbiased estimate that is conditioned on a complete \emph{and} sufficient statistic is the unique most efficient
            estimator: if $T$ is a complete and sufficient statistic and $E(f(T)) = \theta$, then $f(T)$ is the minimum variance
            (unbiased) estimator of $\theta$.  This is therefore a stronger statement than the Rao-Blackwell theorem.  Why then is
            the Rao-Blackwellized posterior standard error more useful for MCMC purposes than a ``Lehmann-Scheff\'{e}'' posterior
            standard error?

    \item 	Produce the ARE result for the Rao-Blackwellization from \eqref{rao.black-are} with a two-variable 
            Metropolis-Hastings algorithm instead of the Gibbs sampler that was used.  \index{subjectindex}{Rao-Blackwellization}

            % NEW
	\item	The Behrens-Fisher problem \index{subjectindex}{Behrens-Fisher problem} is an old problem that is awkward in
            non-Bayesian settings and straightforward with a Bayesian treatment.  Suppose there are two independent samples that
            are both normally distributed according to:
            \begin{equation*}
                x_{11},\ldots,x_{1n_1} \sim \mathcal{N}(\mu_1,\sigma^2_1), \qquad 
                x_{21},\ldots,x_{2n_1} \sim \mathcal{N}(\mu_2,\sigma^2_2) 
            \end{equation*}
            where all four parameters are unknown.  Following the advice of Berger (1985) \index{authorindex}{Berger, J. O.}
            specify the following prior distributions:
            \begin{align*}
                p(\mu_1) \propto 1  & p(\mu_2) \propto 1  \\
                p(\sigma^2_1) \propto 1/\sigma^2_1  & p(\sigma^2_2) \propto 1/\sigma^2_2.
            \end{align*}
            Using two selected countries from military personnel data
            in Section~\ref{Military.Example},\index{subjectindex}{example!Warsaw Pact military expenditures}
            write a sampler to produce posterior values from $\mu_1-\mu_2$.

	\item	Modify the Metropolis-Hastings \R\ code on page~\pageref{metropolis.normal.example.code} to produce 
		    the marginal likelihood with Chib's method. \index{authorindex}{Chib, S.}

            % NEW 
	\item	Write an implementation of Chib and Jeliazkov's (2001) \index{authorindex}{Chib, S.} \index{authorindex}{Jeliazkov, I.}
            method for obtaining the marginal likelihood from Gibbs sampling output in \R.
            \index{authorindex}{Jeliazkov, I.} \index{authorindex}{Chib, S.}
				
\end{exercises}

\section{Computational Addendum: Code for Chapter Examples}
\index{subjectindex}{bugs@\bugs}

\subsection{\texttt{R} Code for the Death Penalty Support Model}  
%norr.df <- read.table("http://jgill.wustl.edu/data/norr.dat",header=TRUE)
%norr.df <- read.table("~/Article.Converge/norr.dat",header=TRUE)
%NEED  data(norr) IN THE BaM PACKAGE [added 5/14/2015]
\begin{R.Code}
# PACKAGES AND DATA LOAD
lapply(c("BaM","msm","survival","LearnBayes"),library,character.only=TRUE) 
# msm FOR THE rtnorm FUNCTION, survival FOR THE survreg FUNCTION
# LearnBayes FOR THE rigamma FUNCTION
data(norr)
# SETUP REDUCED DATA STRUCTURE FOR GIBBS
attach(norr.df)
Y <- c(ds9395p)
X <- cbind(rep(1,nrow(norr.df)),ep4089n,polcul,d8892r2,id8892m2,murder90)
detach(norr.df)
dimnames(X)[[2]] <- c("Intercept","Past Rates","Political Culture",
                      "Current Opinion","Ideology","Murder Rate")

# RUN BIVARIATE REGRESSIONS TO GET PRIOR MEANS FOR BETAS
beta0 <- rep(NA,ncol(X)); beta0[1] <- 1
for (i in 2:ncol(X))   beta0[i] <- summary(lm(Y~X[,i]))$coef[2,1]

# SET PARAMETERS FOR GIBBS
gamma0 <- 300; gamma1 <- 100                    # HYPERPARAMETERS
B0 <- 0.02                                      # SCALING ON SIGMA^0
mc.start <- 1; mc.stop <- 50000                 # MCMC CONTROL
norr.tob <- survreg(Surv(Y,Y>0,type='left') ~ X[,-1], 
                    dist='gaussian',data=norr.df)
B.mat  <- matrix(summary(norr.tob)$coef,nrow=1) # STARTING POINTS
s.sq   <- matrix(rep(10,6), nrow=1)             # PRIOR on SIGMA^2 OF BETA
y.star <- Y                                     # LATENT DATA START

# GIBBS SAMPLER 
for (i in mc.start:mc.stop)  \{
    for (j in 1:length(Y)) \{
        if (Y[j] == 0)  y.star[j] <- rtnorm(1,X[j,]%*%B.mat[i,],
                                            s.sq[i,],lower=-Inf,upper=0)
    \}
    delta <- t(y.star - X %*% B.mat[i,]) %*% (y.star - X %*% B.mat[i,])
    s.temp <- rep(Inf,ncol(s.sq)); while(!is.finite(sum(s.temp)))
                        s.temp <- rigamma(ncol(s.sq), (gamma0+length(Y))/2, 
                                          (gamma1+delta)/2)
    s.sq <- rbind(s.sq,s.temp)
    B.hat <- solve( B0 + t(X)%*%X ) %*% (B0*beta0 + t(X)%*%y.star)
    B.var <-  make.symmetric( solve( s.sq[(i+1)]^(-1)*B0 
                              + s.sq[(i+1)]^(-1)*t(X)%*%X ) )
    B.mat <- rbind( B.mat, rmultinorm(1,B.hat,B.var,tol=1e-06) )
    if (i %% 100 == 0)  print(paste("iteration:",i))
\}
\end{R.Code}

\subsection{\texttt{JAGS} Code for the Military Personnel Model}
\begin{Bugs.Code}
model \{
    for (i in 1:YEAR) \{
       for (j in 1:COUNTRY) \{
          mu[i,j] <- alpha[i] + beta1[i]*(x[j]) + beta2[i]*(x[j]^2) 
                   + beta3[i]*cos(x[j])
          y[i,j]   ~ dnorm(mu[i,j],tau.c)
       \}
       alpha[i] ~ dnorm(alpha.mu,alpha.tau)
       beta1[i] ~ dnorm(beta1.mu,beta1.tau)
       beta2[i] ~ dnorm(beta2.mu,beta2.tau)
       beta3[i] ~ dnorm(beta3.mu,beta3.tau)
    \}
    alpha.mu  ~ dnorm(1,0.1)
    alpha.tau ~ dgamma(1,0.1)
    beta1.mu  ~ dnorm(0,0.1)
    beta1.tau ~ dgamma(1,0.1)
    beta2.mu  ~ dnorm(0,0.1)
    beta2.tau ~ dgamma(1,0.1)
    beta3.mu  ~ dnorm(0,0.1)
    beta3.tau ~ dgamma(1,0.1)
    tau.c     ~ dgamma(1,0.1)
\}
\end{Bugs.Code}


\thispagestyle{empty}
\chapter{Markov Chain Monte Carlo Extensions}\label{MCMC.Extensions.Chapter}
\setcounter{examplecounter}{1}

This chapter reviews some recent extensions and modifications of the classic MCMC algorithms described in previous chapters.
Generally, researchers develop these methods to deal with problematic estimation conditions such as multimodality, extremely high
dimension, and difficult convergence issues.  This chapter also discusses some more recent developments that speed-up the process of
MCMC estimation in ways that extend standard Bayesian stochastic estimation. These deviate from the normal orthodoxy but perhaps
point towards future development. One very promising direction described here is Hamiltonian Monte Carlo, which is a variant of
the Metropolis-Hastings algorithm. This literature is incredibly dynamic will remain so for the next decade or more, so some of
these sections merely point at emerging literatures.

\section{Simulated Annealing}\label{annealing.section}\index{subjectindex}{simulated annealing|(}
Kirkpatrick,\index{authorindex}{Kirkpatrick, S.} Gelatt,\index{authorindex}{Gelatt, C. D.} and Vecchi \index{authorindex}{Vecchi,
M. P.} (1983) and \v{C}ern\'{y}\index{authorindex}{Cerny@\v{C}ern\'{y}, V.} (1985) proposed the MCMC traversal technique
\emph{simulated annealing}, which, like Metropolis-Hastings, has roots in statistical physics\index{subjectindex}{statistical
physics} but is also useful in general stochastic simulation for Bayesian modeling.  Simulated annealing is a flexible stochastic
procedure for iteratively traversing highly textured sample spaces where conditions have been changed to make movement easier.
Correctly set up and run, the annealing algorithm is guaranteed to explore the global maxima (Lundy\index{authorindex}{Lundy, M.}
1985), unlike the EM algorithm, which seeks the nearest mode and remains there.\index{subjectindex}{EM algorithm}  This makes it an
enormously useful approach in MCMC work with multimodal\index{subjectindex}{multimodality} posterior forms, although it can also
be very slow both in movement and with necessary human involvement (van Laarhoven\index{authorindex}{van Laarhoven, P. J. M.} and
Aarts\index{authorindex}{Aarts, E. H. L.} 1987).  There is also a related literature on simulated annealing for pure optimization
(Eglese 1990, Fleischer 1995) rather than posterior description which is our interest here.  \index{authorindex}{Eglese, R. W.}
\index{authorindex}{Fleischer, M. A.}

We have seen that it is possible for a Markov chain to get ``stuck'' for an inordinately long period of time in a nonoptimal
region of the sample space exploring attractive modal features that are not the primary density areas of
interest.\index{subjectindex}{nonoptimal posterior regions}  This is particularly troublesome when such a region separates two
high-density areas and the chain finds it difficult to fully explore both.  Usually this problem is more common in higher
dimensions, but imagine a two-dimensional, bimodal structure with a large mode, a small mode, and a wide gulf in the middle.  It
may be the case in this example that the trough in the middle seriously impedes the chain from easily passing from one mode to the
other.

The analogous idea behind simulated annealing is the principle that less brittle solid metals are produced when allowed to cool
slowly from a liquid state.  In fact the metallurgic process \emph{is} called annealing.  Imagine that we can increase the
``temperature'' of the state space, melting down prominent features: flattening out modes and valleys such that the chain finds it
easier to move through previously low-density regions.\index{subjectindex}{simulated annealing!temperature}  Then when the
temperature is reduced back to normal levels the chain has hopefully passed through impediments to free travel.  This process is
done by temporarily altering all of the probabilities in the transition kernel for a Metropolis-Hastings chain, running the Markov
chain for a time, and then returning the probabilities to their previous values.  

\index{subjectindex}{simulated annealing!temperature schedule} 
The primary decision is the determination of the temperature schedule that dictates the process by which the chain returns
to its original ``cold'' state.  Start by defining a temperature parameter at time $t$: $T_t > 1$, and modifying the 
transition kernel according to $\pi^*(\theta) = \pi(\theta)^{\frac{1}{T}}$, being careful to renormalize.  So heating the 
kernel by making $T$ large flattens out its probability structure toward a uniform distribution.  If there are multiple 
modes, they will melt into the surface and therefore no longer be strong attractions.  It is always necessary to define an 
initial temperature, $T_0$, that provides sufficient melting, then determine a rate that slowly decreases $T_t$ until 
reaching one and therefore returning to the original cold transition kernel probabilities, which are those of inferential
value.  The cooling schedule recognizes the competing phenomenon: \emph{(1)} slow cooling enables greater coverage of the 
sample space, and \emph{(2)} faster cooling gives more reasonable simulation times.  \index{subjectindex}{cooling schedule}

Consider what happens to a Metropolis-Hastings algorithm in this scenario.  As the jumping distribution generates candidate 
positions, very few of these will be rejected and the Markov chain will rarely stay in place.  This is ``good;'' it means 
that the chain can freely explore the sample space without impediments.  It is also ``bad'' in that there is obviously
much less of a tendency to remain in the (previous) high density areas.  This is where the researcher-specified cooling 
process comes in.  

The following modified Metropolis-Hastings setup, which was the original idea behind simulated annealing as first proposed by 
Metropolis \etal (1953), gives a simple Markov chain for example purposes.  Suppose that at time (step) $k$ we are at 
temperature $T_k$, having started at step $0$, and the heated up temperature $T_0$.  Then the next value is chosen according 
to the algorithm:\index{subjectindex}{simulated annealing!as Metropolis-Hastings}\index{subjectindex}{simulated annealing!steps}
\begin{bayeslist}
	\item At the $t^{th}$ step draw $\theta'$ from a uniform distribution around the
	      current position, $\theta^{[t]}$.

	\item	Define: $a(\theta',\theta)=\exp[-(\pi(\theta_j')-\pi(\theta_j^{[t]}))/T_t]$, and
		make the decision:
		\renewcommand{\arraystretch}{4} \begin{equation}\label{Simulated.Annealing.Rule}
        	\theta_j^{[t+1]} =
        	\begin{cases}
                	\theta_j' & \text{with probability}\qquad \text{min}\left( a(\theta',\theta), 1\right) \\[12pt]
                	\theta_j^{[t]} & \text{with probability}\qquad 1-\text{min}\left( a(\theta',\theta), 1 \right). \\
        	\end{cases}
		\end{equation} \renewcommand{\arraystretch}{1}

	\item	After sampling sufficiently that convergence is concluded at this temperature,
		move up the temperature schedule from $T_t$ to $T_{t+1}$.
		\index{subjectindex}{simulated annealing!temperature schedule}

	\item	Repeat steps 1-3 until the temperature schedule has been completed.
\end{bayeslist}
Therefore the transition matrix is first heated up such that the chain converges weakly over a near-uniform distribution.  
Once the cooling begins, the chain is observed to converge at progressively cooler temperatures until the transition matrix 
returns to its original state.  This process can be repeated as many times as necessary to collect a sufficient number of 
cold chain values. This iterative process, which requires human interaction in this basic case, can be generalized in several ways
described below to reduce the required management by the researcher.

The hottest jumping distribution for generating candidate values does not need to be uniform, but often is specified to be.  While
the Markov chain described here is not homogeneous as is the standard Metropolis-Hastings algorithm,
H\`{a}jek\index{authorindex}{Hajak, B.@H\`{a}jek, B.} (1988) showed that the discrete state space still has required convergence
properties by considering the recorded ``cold'' chain values to be the only expression of the Markovian process similar (but
different) than the idea of thinning with an appeal to a classic theorem about grouping Markovian iterations in Meyn and Tweedie
(1993). \index{authorindex}{Meyn, S. P.}\index{authorindex}{Tweedie, R. L.}
Similar properties hold for the continuous case and are given by Duflo\index{authorindex}{Duflo, M.} (1996),
Geman\index{authorindex}{Geman, S.} and Hwang \index{authorindex}{Hwang, C-R.} (1986), Holley,\index{authorindex}{Holley, R. A.}
Kusuoka,\index{authorindex}{Kusuoka, S.} and Stroock \index{authorindex}{Stroock, D. W.} (1989), and Jeng
\index{authorindex}{Jeng, F-C.} and Woods \index{authorindex}{Woods, J. W.} (1990) in more complex settings.

In general the temperature (cooling) schedule\index{subjectindex}{simulated annealing!cooling schedule} should gradually cool 
down by decreasing $T_t$ very slightly on each iteration after $T_0$.  A number of schemes have been proposed such as a 
logarithmic scale (Geman\index{authorindex}{Geman, S.}\index{authorindex}{Geman, D.} and Geman 1984): $T_t = kT_1/\log(t)$, 
a geometric-type process: $T_t = \epsilon T_{t-1}, 0\!<\!\epsilon\!<\!1$ (Mitra,\index{authorindex}{Mitra, D.} Romeo,
\index{authorindex}{Romeo, F.} and Sangiovanni-Vincentelli\index{authorindex}{Sangiovanni-Vincentelli, A. L.} 1986), as well 
as more complex algorithms from statistical physics (Aarts\index{authorindex}{Aarts, E. H. L.} and Kors
\index{authorindex}{Kors, T. J.} 1989).  H\`{a}jek\index{authorindex}{Hajak, B.@H\`{a}jek, B.}
(1988) showed that a logarithmic cooling 
schedule proportional to the height of the global maximum (often obtainable with EM\index{subjectindex}{EM algorithm} or some 
other search tool) converges asymptotically to the full set of maxima. 

The choice of a cooling schedule\index{subjectindex}{simulated annealing!cooling schedule!trade-off} is necessarily a compromise
between slow cooling to ensure greater coverage of the sample space and faster cooling back to zero in order to give a reasonable
simulation time.  This is not necessarily a stark trade-off.  For instance, in the case of the Metropolis-Hastings algorithm the
bounds of the jumping distribution can be adjusted so that as the temperature cools down the number of rejected jumping points
does not go up dramatically (Gelfand\index{authorindex}{Gelfand, A. E.} and Mitter \index{authorindex}{Mitter, S. K.} 1993).

\begin{examplelist}
	\item	{\bf An Illustrative Simple Simulated Annealing Setup.}\index{subjectindex}{simulated annealing!simple example}
		This is a contrived and pathological example designed purely to show the characteristics of the simulated 
		annealing algorithm.  Consider a $21 \times 21$ matrix representing a two-dimensional discrete posterior state 
		space.  This matrix of unnormalized posterior density values is design to deter a standard Metropolis-Hastings 
		chain from reaching and describing the global maxima values in the corners by putting an attractive mode in the 
		middle, ringing it with compelling but nonoptimal points: \index{subjectindex}{nonoptimal posterior regions} 

		\begin{tiny}\begin{equation}
		\setlength{\tabcolsep}{12pt}
    		\begin{array}{rrrrrrrrrrrrrrrrrrrrr}
                $9\;\;$ & $8\;\;$ & $3\;\;$ & $3\;\;$ & $1\;\;$ & $1\;\;$ & $1\;\;$ & $1\;\;$ & $1\;\;$ & $1\;\;$ & $1\;\;$
                         & $1\;\;$ & $1\;\;$ & $1\;\;$ & $1\;\;$ & $1\;\;$ & $1\;\;$ & $3\;\;$ & $3\;\;$ & $8\;\;$ & $9\;\;$ \\
                $8\;\;$ & $3\;\;$ & $3\;\;$ & $1\;\;$ & $1\;\;$ & $1\;\;$ & $1\;\;$ & $1\;\;$ & $1\;\;$ & $1\;\;$ & $1\;\;$
                         & $1\;\;$ & $1\;\;$ & $1\;\;$ & $1\;\;$ & $1\;\;$ & $1\;\;$ & $1\;\;$ & $3\;\;$ & $3\;\;$ & $8\;\;$ \\
                $3\;\;$ & $3\;\;$ & $1\;\;$ & $1\;\;$ & $1\;\;$ & $1\;\;$ & $1\;\;$ & $1\;\;$ & $3\;\;$ & $3\;\;$ & $3\;\;$
                         & $3\;\;$ & $3\;\;$ & $1\;\;$ & $1\;\;$ & $1\;\;$ & $1\;\;$ & $1\;\;$ & $1\;\;$ & $3\;\;$ & $3\;\;$ \\
                $3\;\;$ & $1\;\;$ & $1\;\;$ & $1\;\;$ & $1\;\;$ & $1\;\;$ & $1\;\;$ & $3\;\;$ & $6\;\;$ & $6\;\;$ & $6\;\;$
                         & $6\;\;$ & $6\;\;$ & $3\;\;$ & $1\;\;$ & $1\;\;$ & $1\;\;$ & $1\;\;$ & $1\;\;$ & $1\;\;$ & $3\;\;$ \\
                $1\;\;$ & $1\;\;$ & $1\;\;$ & $1\;\;$ & $1\;\;$ & $1\;\;$ & $3\;\;$ & $6\;\;$ & $3\;\;$ & $3\;\;$ & $3\;\;$
                         & $3\;\;$ & $3\;\;$ & $6\;\;$ & $3\;\;$ & $1\;\;$ & $1\;\;$ & $1\;\;$ & $1\;\;$ & $1\;\;$ & $1\;\;$ \\
                $1\;\;$ & $1\;\;$ & $1\;\;$ & $1\;\;$ & $1\;\;$ & $3\;\;$ & $6\;\;$ & $3\;\;$ & $1\;\;$ & $1\;\;$ & $1\;\;$
                         & $1\;\;$ & $1\;\;$ & $3\;\;$ & $6\;\;$ & $3\;\;$ & $1\;\;$ & $1\;\;$ & $1\;\;$ & $1\;\;$ & $1\;\;$ \\
                $1\;\;$ & $1\;\;$ & $1\;\;$ & $1\;\;$ & $3\;\;$ & $6\;\;$ & $3\;\;$ & $1\;\;$ & $1\;\;$ & $1\;\;$ & $1\;\;$
                         & $1\;\;$ & $1\;\;$ & $1\;\;$ & $3\;\;$ & $6\;\;$ & $3\;\;$ & $1\;\;$ & $1\;\;$ & $1\;\;$ & $1\;\;$ \\
                $1\;\;$ & $1\;\;$ & $1\;\;$ & $3\;\;$ & $6\;\;$ & $3\;\;$ & $1\;\;$ & $1\;\;$ & $1\;\;$ & $1\;\;$ & $1\;\;$
                         & $1\;\;$ & $1\;\;$ & $1\;\;$ & $1\;\;$ & $3\;\;$ & $6\;\;$ & $3\;\;$ & $1\;\;$ & $1\;\;$ & $1\;\;$ \\
                $1\;\;$ & $1\;\;$ & $3\;\;$ & $6\;\;$ & $3\;\;$ & $1\;\;$ & $1\;\;$ & $1\;\;$ & $4\;\;$ & $4\;\;$ & $4\;\;$
                         & $4\;\;$ & $4\;\;$ & $1\;\;$ & $1\;\;$ & $1\;\;$ & $3\;\;$ & $6\;\;$ & $3\;\;$ & $1\;\;$ & $1\;\;$ \\
                $1\;\;$ & $1\;\;$ & $3\;\;$ & $6\;\;$ & $3\;\;$ & $1\;\;$ & $1\;\;$ & $1\;\;$ & $4\;\;$ & $6\;\;$ & $6\;\;$
                         & $6\;\;$ & $4\;\;$ & $1\;\;$ & $1\;\;$ & $1\;\;$ & $3\;\;$ & $6\;\;$ & $3\;\;$ & $1\;\;$ & $1\;\;$ \\
                $1\;\;$ & $1\;\;$ & $3\;\;$ & $6\;\;$ & $3\;\;$ & $1\;\;$ & $1\;\;$ & $1\;\;$ & $4\;\;$ & $6\;\;$ & $6\;\;$
                         & $6\;\;$ & $4\;\;$ & $1\;\;$ & $1\;\;$ & $1\;\;$ & $3\;\;$ & $6\;\;$ & $3\;\;$ & $1\;\;$ & $1\;\;$ \\
                $1\;\;$ & $1\;\;$ & $3\;\;$ & $6\;\;$ & $3\;\;$ & $1\;\;$ & $1\;\;$ & $1\;\;$ & $4\;\;$ & $6\;\;$ & $6\;\;$
                         & $6\;\;$ & $4\;\;$ & $1\;\;$ & $1\;\;$ & $1\;\;$ & $3\;\;$ & $8\;\;$ & $3\;\;$ & $1\;\;$ & $1\;\;$ \\
                $1\;\;$ & $1\;\;$ & $3\;\;$ & $8\;\;$ & $3\;\;$ & $1\;\;$ & $1\;\;$ & $1\;\;$ & $4\;\;$ & $4\;\;$ & $4\;\;$
                         & $4\;\;$ & $4\;\;$ & $1\;\;$ & $1\;\;$ & $1\;\;$ & $3\;\;$ & $6\;\;$ & $3\;\;$ & $1\;\;$ & $1\;\;$ \\
                $1\;\;$ & $1\;\;$ & $1\;\;$ & $3\;\;$ & $6\;\;$ & $3\;\;$ & $1\;\;$ & $1\;\;$ & $1\;\;$ & $1\;\;$ & $1\;\;$
                         & $1\;\;$ & $1\;\;$ & $1\;\;$ & $1\;\;$ & $3\;\;$ & $6\;\;$ & $3\;\;$ & $1\;\;$ & $1\;\;$ & $1\;\;$ \\
                $1\;\;$ & $1\;\;$ & $1\;\;$ & $1\;\;$ & $3\;\;$ & $6\;\;$ & $3\;\;$ & $1\;\;$ & $1\;\;$ & $1\;\;$ & $1\;\;$
                         & $1\;\;$ & $1\;\;$ & $1\;\;$ & $3\;\;$ & $6\;\;$ & $3\;\;$ & $1\;\;$ & $1\;\;$ & $1\;\;$ & $1\;\;$ \\
                $1\;\;$ & $1\;\;$ & $1\;\;$ & $1\;\;$ & $1\;\;$ & $3\;\;$ & $6\;\;$ & $3\;\;$ & $1\;\;$ & $1\;\;$ & $1\;\;$
                         & $1\;\;$ & $1\;\;$ & $3\;\;$ & $6\;\;$ & $3\;\;$ & $1\;\;$ & $1\;\;$ & $1\;\;$ & $1\;\;$ & $1\;\;$ \\
                $1\;\;$ & $1\;\;$ & $1\;\;$ & $1\;\;$ & $1\;\;$ & $1\;\;$ & $3\;\;$ & $6\;\;$ & $3\;\;$ & $3\;\;$ & $3\;\;$
                         & $3\;\;$ & $3\;\;$ & $6\;\;$ & $3\;\;$ & $1\;\;$ & $1\;\;$ & $1\;\;$ & $1\;\;$ & $1\;\;$ & $1\;\;$ \\
                $3\;\;$ & $1\;\;$ & $1\;\;$ & $1\;\;$ & $1\;\;$ & $1\;\;$ & $1\;\;$ & $3\;\;$ & $6\;\;$ & $6\;\;$ & $6\;\;$
                         & $6\;\;$ & $6\;\;$ & $3\;\;$ & $1\;\;$ & $1\;\;$ & $1\;\;$ & $1\;\;$ & $1\;\;$ & $1\;\;$ & $3\;\;$ \\
                $3\;\;$ & $3\;\;$ & $1\;\;$ & $1\;\;$ & $1\;\;$ & $1\;\;$ & $1\;\;$ & $1\;\;$ & $3\;\;$ & $3\;\;$ & $3\;\;$
                         & $3\;\;$ & $3\;\;$ & $1\;\;$ & $1\;\;$ & $1\;\;$ & $1\;\;$ & $1\;\;$ & $1\;\;$ & $3\;\;$ & $3\;\;$ \\
                $8\;\;$ & $3\;\;$ & $3\;\;$ & $1\;\;$ & $1\;\;$ & $1\;\;$ & $1\;\;$ & $1\;\;$ & $1\;\;$ & $1\;\;$ & $1\;\;$
                         & $1\;\;$ & $1\;\;$ & $1\;\;$ & $1\;\;$ & $1\;\;$ & $1\;\;$ & $1\;\;$ & $3\;\;$ & $3\;\;$ & $8\;\;$ \\
                $9\;\;$ & $8\;\;$ & $3\;\;$ & $3\;\;$ & $1\;\;$ & $1\;\;$ & $1\;\;$ & $1\;\;$ & $1\;\;$ & $1\;\;$ & $1\;\;$
                         & $1\;\;$ & $1\;\;$ & $1\;\;$ & $1\;\;$ & $1\;\;$ & $1\;\;$ & $3\;\;$ & $3\;\;$ & $8\;\;$ & $9\;\;$ \\[5pt]
    		\end{array} \nonumber
		\setlength{\tabcolsep}{6pt}
		\end{equation}\end{tiny}

		As a demonstration, we first run the simulated annealing chain without any temperature schedule
		(in other words a Metropolis-Hastings chain), so that the decision criteria is based on
		$\exp[-( \pi(\theta_j')-\pi(\theta_j^{[t]}) )]$. At each iteration the eight adjoining cells 
		for any presently occupied location (fewer for locations on boundaries) is equally
		likely as a candidate point: a discrete uniform jumping distribution.  The starting point
		is randomly selected within the state space and the algorithm is run for 10,000 iterations.
		The resulting distribution of visits is summarized by the matrix: 
		
		%\vspace{-18pt}
		\begin{equation}
    		\begin{smallmatrix}
    		0\;\; &0\;\; &0\;\; &0  &0  &0  &0  &0 &0 &0 &0 &0 &0 &0 &0 &0 &0 &0\;\; &0\;\; &0\;\; &0       \\[2pt]
    		0\;\; &0\;\; &0\;\; &0  &0  &0  &0  &0 &0 &0 &0 &1 &0 &0 &0 &0 &0 &0\;\; &0\;\; &0\;\; &0       \\[2pt]
    		0\;\; &0\;\; &0\;\; &0  &0  &0  &0  &0 &11 &4 &3 &4 &6 &0 &0 &0 &0 &0\;\; &0\;\; &0\;\; &0      \\[2pt]
    		0\;\; &0\;\; &0\;\; &0  &0  &0  &0  &13 &132  &169 &166 &92 &93 &1 &0 &0 &0 &0\;\; &0\;\; &0\;\; &0\\[2pt]
    		0\;\; &0\;\; &0\;\; &0  &0  &0  &1  &87 &5 &7 &6&7 &9 &66 &0 &0 &0 &0\;\; &0\;\; &0\;\; &0     \\[2pt]
    		0\;\; &0\;\; &0\;\; &0  &0  &2  &54 &3 &0 &0 &2&4 &0 &4 &106 &2 &0 &0\;\; &0\;\; &0\;\; &0    \\[2pt]
    		0\;\; &0\;\; &0\;\; &0  &0  &69 &4  &0 &2 &3 &5&5 &2 &0 &4 &128 &3 &0\;\; &0\;\; &0\;\; &0    \\[2pt]
    		0\;\; &0\;\; &1\;\; &4  &77 &1  &1  &1 &0 &3 &5&4 &3 &0 &0 &1 &123 &0\;\; &0\;\; &0\;\; &0    \\[2pt]
    		1\;\; &2\;\; &5\;\; &100&4  &1  &1  &0 &27 &22 &39 &25 &18 &1 &1 &1 &6 &82\;\; &2\;\; &0\;\; &0\\[2pt]
    		2\;\; &0\;\; &4\;\; &68 &9  &2  &1  &1 &34 &1681 &1539 &1590 &25 &1 &1 &0 &6 &103\;\; &3\;\; &0\;\; &0\\[2pt]
    		2\;\; &0\;\; &3\;\; &62 &5  &2  &1  &1 &23 &1624 &1573 &1637 &30 &1 &1 &0 &8 &133\;\; &7\;\; &0\;\; &0\\[2pt]
    		1\;\; &1\;\; &0\;\; &135&4  &0  &0  &0 &27 &1525 &1561 &1588 &20 &0 &2 &2 &4 &179\;\; &2\;\; &1\;\; &0\\[2pt]
    		1\;\; &1\;\; &9\;\; &182&3  &1  &0  &1 &36 &52 &28 &21 &16 &2 &2 &2 &9 &127\;\; &4\;\; &0\;\; &0\\[2pt]
    		1\;\; &1\;\; &3\;\; &6  &226&6  &0  &1 &2 &2 &2 &0 &1 &1 &1 &5 &130 &5\;\; &0\;\; &0\;\; &0   \\[2pt]
    		1\;\; &1\;\; &0\;\; &0  &4  &189&6  &1 &1 &2 &3 &1 &2 &0 &1 &66 &6 &1\;\; &0\;\; &0\;\; &0    \\[2pt]
    		3\;\; &1\;\; &0\;\; &0  &0  &10 &151&6 &3 &1 &1 &1 &0 &2 &71 &6 &1 &0\;\; &0\;\; &0\;\; &0  \\[2pt]
    		0\;\; &2\;\; &0\;\; &0  &1  &1  &1  &162 &12 &16 &5 &0 &0 &118 &1 &0 &0 &0\;\; &0\;\; &0\;\; &0 \\[2pt]
    		0\;\; &0\;\; &1\;\; &1  &2  &1  &1  &14 &195 &175 &106 &81 &102 &0 &0 &0 &0& 0\;\; &0\;\; &0\;\; &0\\[2pt]
    		0\;\; &0\;\; &0\;\; &0  &1  &0  &1  &0 &3 &6 &6 &7 &1 &0 &0 &0 &0 &0\;\; &0\;\; &0\;\; &0       \\[2pt]
    		0\;\; &0\;\; &0\;\; &0  &0  &0  &0  &0 &0 &0 &2 &0 &1 &0 &0 &0 &0 &0\;\; &0\;\; &0\;\; &0       \\[2pt]
    		0\;\; &0\;\; &0\;\; &0  &0  &0  &0  &0 &0 &0 &0 &3 &1 &0 &0 &0 &0 &0\;\; &0\;\; &0\;\; &0       \\[10pt]
    		\end{smallmatrix} \nonumber
		\end{equation}
		%\vspace{-18pt}
		
		It is clear that as intended, the Metropolis-Hastings algorithm missed the high modal values in 
		the corners but spent quite some time following the ring around the center and exploring the nonoptimal 
			\index{subjectindex}{nonoptimal posterior regions}%
		mode area in the center, despite the fact that all of these points are lower than the corner values.
		The rejection rate was 0.6014, meaning that the algorithm was fairly inefficient as well.  This is
		an indication that the chain chose to avoid low-density areas rather than having had the 
		opportunity to mix well.\index{subjectindex}{simulated annealing!rejection rate}
		
		Now we add a temperature schedule according to the popular logarithmic choice: 
		$T_i=\log(2)/\log(i)$ for $i=1,\ldots, 100$, and sample 100 times at each of these cooling 
		levels.  In real applications we would not be so cavalier about the number of chain values 
		at each temperature, and we would instead take care to observe evidence of convergence each 
		time (hence the reason that simulated annealing can take a long time).  This setup produces
		the following matrix of visits for the first temperature schedule point:
		
		\begin{equation}
    		\begin{smallmatrix}
        		9&\;\;3&\;\;4&\;\;6&\;\;5&\;\;7&\;\;8&\;\;17&\;\;18&\;\;19&\;\;15&\;\;14&\;\;17&\;\;
                		18&\;\;22&\;\;25&\;\;36&\;\;45&\;\;33&\;\;34&\;\;24         \\[2pt]
        		2&\;\;5&\;\;5&\;\;4&\;\;10&\;\;9&\;\;15&\;\;30&\;\;32&\;\;26&\;\;22&\;\;23&\;\;27&\;\;
                		38&\;\;25&\;\;55&\;\;53&\;\;32&\;\;52&\;\;46&\;\;46       \\[2pt]
        		2&\;\;1&\;\;4&\;\;7&\;\;4&\;\;14&\;\;16&\;\;20&\;\;40&\;\;28&\;\;41&\;\;38&\;\;45&\;\;
                		31&\;\;39&\;\;41&\;\;47&\;\;36&\;\;44&\;\;58&\;\;24       \\[2pt]
        		5&\;\;2&\;\;4&\;\;6&\;\;16&\;\;14&\;\;13&\;\;15&\;\;62&\;\;77&\;\;46&\;\;47&\;\;49&\;\;
                		45&\;\;33&\;\;34&\;\;51&\;\;44&\;\;38&\;\;31&\;\;22      \\[2pt]
        		6&\;\;4&\;\;3&\;\;4&\;\;13&\;\;9&\;\;14&\;\;48&\;\;22&\;\;22&\;\;39&\;\;42&\;\;28&\;\;
                		44&\;\;50&\;\;33&\;\;38&\;\;37&\;\;35&\;\;18&\;\;13       \\[2pt]
        		5&\;\;15&\;\;10&\;\;11&\;\;11&\;\;25&\;\;64&\;\;35&\;\;20&\;\;18&\;\;26&\;\;23&\;\;
                		30&\;\;29&\;\;44&\;\;54&\;\;24&\;\;27&\;\;23&\;\;19&\;\;11   \\[2pt]
        		9&\;\;14&\;\;11&\;\;6&\;\;18&\;\;49&\;\;23&\;\;21&\;\;19&\;\;21&\;\;24&\;\;18&\;\;
                		18&\;\;20&\;\;32&\;\;35&\;\;36&\;\;14&\;\;19&\;\;19&\;\;9     \\[2pt]
        		9&\;\;17&\;\;20&\;\;20&\;\;56&\;\;16&\;\;16&\;\;14&\;\;23&\;\;19&\;\;17&\;\;14&\;\;
                		20&\;\;19&\;\;23&\;\;33&\;\;32&\;\;22&\;\;22&\;\;23&\;\;10   \\[2pt]
        		11&\;\;18&\;\;14&\;\;30&\;\;20&\;\;23&\;\;16&\;\;25&\;\;29&\;\;25&\;\;30&\;\;20&\;\;
                		26&\;\;23&\;\;26&\;\;30&\;\;32&\;\;47&\;\;24&\;\;15&\;\;9   \\[2pt]
        		8&\;\;16&\;\;29&\;\;49&\;\;23&\;\;17&\;\;22&\;\;19&\;\;34&\;\;44&\;\;39&\;\;31&\;\;
                		39&\;\;22&\;\;20&\;\;23&\;\;38&\;\;31&\;\;26&\;\;14&\;\;8    \\[2pt]
        		12&\;\;15&\;\;25&\;\;27&\;\;18&\;\;16&\;\;18&\;\;24&\;\;37&\;\;50&\;\;35&\;\;47&\;\;
                		39&\;\;20&\;\;27&\;\;23&\;\;35&\;\;28&\;\;48&\;\;24&\;\;10  \\[2pt]
        		5&\;\;9&\;\;15&\;\;29&\;\;19&\;\;18&\;\;24&\;\;25&\;\;58&\;\;54&\;\;44&\;\;45&\;\;
                		40&\;\;23&\;\;14&\;\;35&\;\;26&\;\;46&\;\;21&\;\;26&\;\;9     \\[2pt]
        		8&\;\;9&\;\;13&\;\;25&\;\;22&\;\;18&\;\;24&\;\;39&\;\;53&\;\;55&\;\;33&\;\;34&\;\;
                		27&\;\;21&\;\;25&\;\;26&\;\;28&\;\;25&\;\;21&\;\;18&\;\;9     \\[2pt]
        		8&\;\;10&\;\;14&\;\;24&\;\;39&\;\;24&\;\;12&\;\;35&\;\;30&\;\;26&\;\;28&\;\;20&\;\;
                		18&\;\;17&\;\;26&\;\;35&\;\;39&\;\;26&\;\;20&\;\;15&\;\;8    \\[2pt]
        		11&\;\;14&\;\;9&\;\;20&\;\;25&\;\;27&\;\;22&\;\;17&\;\;22&\;\;30&\;\;29&\;\;23&\;\;
                		17&\;\;22&\;\;40&\;\;44&\;\;32&\;\;20&\;\;16&\;\;16&\;\;14   \\[2pt]
        		7&\;\;14&\;\;9&\;\;16&\;\;14&\;\;14&\;\;32&\;\;21&\;\;21&\;\;23&\;\;23&\;\;22&\;\;
                		29&\;\;35&\;\;45&\;\;32&\;\;12&\;\;12&\;\;19&\;\;21&\;\;24    \\[2pt]
        		8&\;\;12&\;\;16&\;\;13&\;\;13&\;\;15&\;\;25&\;\;33&\;\;36&\;\;39&\;\;28&\;\;22&\;\;
                		33&\;\;38&\;\;20&\;\;23&\;\;16&\;\;14&\;\;23&\;\;28&\;\;17   \\[2pt]
        		8&\;\;21&\;\;20&\;\;12&\;\;13&\;\;25&\;\;18&\;\;35&\;\;49&\;\;40&\;\;31&\;\;33&\;\;
                		39&\;\;26&\;\;27&\;\;20&\;\;15&\;\;14&\;\;23&\;\;23&\;\;18   \\[2pt]
       				15&\;\;18&\;\;27&\;\;19&\;\;20&\;\;23&\;\;20&\;\;28&\;\;42&\;\;39&\;\;23&\;\;26&\;\;
                		25&\;\;31&\;\;24&\;\;28&\;\;21&\;\;15&\;\;12&\;\;11&\;\;22  \\[2pt]
        		18&\;\;19&\;\;14&\;\;30&\;\;32&\;\;29&\;\;37&\;\;31&\;\;34&\;\;30&\;\;21&\;\;17&\;\;
                		21&\;\;12&\;\;22&\;\;20&\;\;28&\;\;19&\;\;23&\;\;31&\;\;25  \\[2pt]
        		37&\;\;20&\;\;18&\;\;23&\;\;24&\;\;24&\;\;19&\;\;18&\;\;12&\;\;15&\;\;17&\;\;12&\;\;
                		14&\;\;15&\;\;12&\;\;22&\;\;12&\;\;20&\;\;20&\;\;19&\;\;35  \\[10pt]
    		\end{smallmatrix} \nonumber
		\end{equation}
		
        Not only does the annealing algorithm do a much better job of exploring the sample space, it also had a rejection rate of
        only 0.0676 (okay here due to the small sample space), an order of magnitude better than the version without a temperature
        schedule.  In addition, rejections remained rare even as the cooling levels reached back to normal levels.
\end{examplelist}

\subsection{General Points on Simulated Annealing}
A more general simulated annealing algorithm is given by Kirkpatrick, Gelatt, and Vecchi (1983), where the procedure is the same
as that given above, but cast in more Bayesian statistics than physics terms.  Specifically, we generate candidates in general 
Metropolis-Hastings fashion from any desired distribution, and we do not need the exponential metric.  The cold distribution
is again given by $\pi(\T)$, and we will assume a general temperature schedule $f(T_t|t)$.  So at iteration $t$ and temperature 
$T_t$ we have the current position $\T^{[t]}$, and perform the steps:
\begin{bayeslist}
        \item 	Generate multivariate $\T^\prime$ from a candidate-generating distribution $q(\T^\prime|\T)$ that does or does 
		not account for the current position.

        \item 	Define: 
                     \begin{equation*}
                        a(\T^\prime, \T^{[t]}) = \left[ 
				\frac{ q(\T^{[t]}|\T^\prime) }{ q(\T^\prime|\T^{[t]}) } 
				\frac{ \pi(\T^\prime)^{\frac{1}{T_t}} }{ \pi(\T^{[t]})^{\frac{1}{T_t}} }
			\right].
                     \end{equation*}
                    
        \item  $\T^{[t+1]}=\begin{cases}
                   	\T^\prime & \text{with probability} \qquad   \min(a(\T^\prime, \T^{[t]}) ,1) \\
                   	\T^{[t]}  & \text{with probability} \qquad 1-\min(a(\T^\prime, \T^{[t]}) ,1) 
                     \end{cases}$
	\item	Update $T_t$ to $T_{t+1}$ from the temperature schedule.
\end{bayeslist}
Notice that this is a straightforward extension of the Metropolis-Hastings algorithm.
		
Good overviews of simulated annealing are given by Bertsimas\index{authorindex}{Bertsimas, D.} and Tsitsiklis
\index{authorindex}{Tsitsiklis, J. N.} (1993),  Brooks\index{authorindex}{Brooks, S. P.} and Morgan
\index{authorindex}{Morgan, B. J. T.} (1995), and H\`{a}jek\index{authorindex}{Hajak, B.@H\`{a}jek, B.} (1985).  Neal 
(2001) connects importance sampling (Chapter~\ref{MC.Chapter}) to simulated annealing as a way to create importance sampling
distributions in high dimensions where it would otherwise be difficult to get reasonable acceptance rates.  Other extensions exist
and simulated annealing continues to be an active research area in terms of both theoretical development and applied work.
\index{authorindex}{Neal, R. M.}

A number of authors have worried about the various conditions necessary to assert convergence 
\index{subjectindex}{simulated annealing!convergence} (Ferrari,\index{authorindex}{Ferrari, P. A.} Frigessi, 
\index{authorindex}{Frigessi, A.} and Schonmann \index{authorindex}{Schonmann, R. J.} 1993; Gidas\index{authorindex}{Gidas, B.} 
1985; Locatelli\index{authorindex}{Locatelli, M.} 2000; Lundy\index{authorindex}{Lundy, M.} and Mees\index{authorindex}{Mees, A.}
1986, Szu and Hartley 1987) generally by restricting the form 
of the domain or specifying a uniform minorization condition for a specific chain.  As mentioned, the time to run useful 
simulated annealing programs can be long.  Several works look at estimating this time and how it might be improved 
(Chiang\index{authorindex}{Chiang, T-S.} and Chow\index{authorindex}{Chow, Y.} 1988; Ingber\index{authorindex}{Ingber, L.} 
1992; Tsitsiklis\index{authorindex}{Tsitsiklis, J. N.} 1988; Mitra,\index{authorindex}{Mitra, D.} Romeo,
\index{authorindex}{Romeo, F.} and Sangiovanni-Vincentelli\index{authorindex}{Sangiovanni-Vincentelli, A. L.} 1986).
\index{authorindex}{Szu, H.}\index{authorindex}{Hartley, R.}%

Applications are usually motivated by a need to optimize ill-behaved functions, often as a reasonable alternative to exhaustive 
searches of the sample space (e.g., Bohachevsky,\index{authorindex}{Bohachevsky, I. O.} Johnson,
\index{authorindex}{Johnson, M. E.} and Stein \index{authorindex}{Stein, M. L.} 1986; Kollman,\index{authorindex}{Kollman, K.} 
Miller,\index{authorindex}{Miller, J. H.} and Page\index{authorindex}{Page, S. E.} 1997).  Explicitly Bayesian works include 
Bernardo\index{authorindex}{Bernardo, J. M.} (1992), van Laarhoven\index{authorindex}{van Laarhoven, P. J. M.} \etal (1989), 
Geyer\index{authorindex}{Geyer, C. J.} and Thompson\index{authorindex}{Thompson, E. A.} (1995), and Press
\index{authorindex}{Press, W. H.} \etal (1986).  \index{subjectindex}{simulated annealing|)}

\subsection{Metropolis-Coupling}\index{subjectindex}{Metropolis-Coupling}
With high-dimensional and multimodal objective functions (posteriors) of interest, the candidate distribution and the 
temperature schedule in simulated annealing must be chosen with great care to allow adequate exploration of the space.  
Unfortunately it is possible to stipulate wildly inappropriate choices of both, thus preventing convergence or mixing with 
simulated annealing.  One early solution to this problem is \emph{Metropolis-Coupled Markov chain Monte Carlo} 
(MCMCMC)\index{subjectindex}{Metropolis-Coupled Markov chain Monte Carlo (MCMCMC)} (Geyer\index{authorindex}{Geyer, C. J.} 
1991).  This algorithm is characterized by the steps:
\begin{bayeslist}
	\item   Run $N$ parallel chains at different heat levels from $m^1$ to $m^{1/\beta_N}$, where the 
		temperature values have the characteristic: $\beta_1 = 1 < \beta_2 < \ldots < \beta_N$.
        \item   Thus $N$ transition kernels are defined, $K_1,K_2,\ldots,K_N$.
        \item   At time $t$ select two chains, $i$ and $j$, and attempt to swap states:
                                \begin{equation*}
                                        \ca_i^{[t]} \Leftarrow \ca_j^{[t]}, \qquad
                                        \ca_j^{[t]} \Leftarrow \ca_i^{[t]},
                                \end{equation*}
        \item   with a Metropolis decision, probability:
                                \begin{equation*}
                                \min\left\{1,
                                        \frac{ m_i(\ca_j^{[t]}) m_j(\ca_i^{[t]} }{ m_i(\ca_i^{[t]}) m_j(\ca_j^{[t]}) }
                                \right\}.
                                \end{equation*}
        \item   Record only the cold chain, $m^1$, for inferential purposes.
\end{bayeslist}
The key advantage to MCMCMC is that chains that get stuck in non-optimal maxima will eventually get swapped-out to some
other, presumably more free, state.  A notable disadvantage, though, is the need to possibly run \emph{many} parallel 
chains for problems with highly complex targets. Also, it is possible to run parallel chains concurrently using additional
software such as \texttt{snow} (Olivera and Gill 2011).
\index{authorindex}{Olivera, S.} \index{authorindex}{Gill, J.}

\subsection{Simulated Tempering}\index{subjectindex}{simulated tempering}
Marinari and Parisi\index{authorindex}{Marinari, E.} \index{authorindex}{Parisi, G.} (1992) and (independently) Geyer and 
Thompson \index{authorindex}{Geyer, C. J.} \index{authorindex}{Thompson, E. A.} (1995) propose an alternative algorithm called 
\emph{simulated tempering}, which reduces the MCMCMC algorithm to a single chain.  Essentially the temperature itself 
becomes a random variable so the system can heat \emph{and} cool as time proceeds.  Why would one want do this in a 
simulated annealing process?  Now elderly chains can still avoid being trapped at local maxima by getting more general 
Metropolis-Hastings candidate positions.  That is, step 1 of the simulated annealing algorithm above is replaced with:
\begin{enumerate}
	\item[$1a.$]  Generate $\beta$ from some distribution of temperature, $f(\beta)$.
	\item[$1b.$]  Generate $\ca_j^\prime\sim \left[\ca_j^{[t^\prime]} + \mathcal{N}(0,\sigma^{2}) \right]$.
\end{enumerate}
The number of $f(\beta)$ choices is obviously vast, but this decision can be simplified by using a discrete distribution 
that resembles some desired, but not implemented, cooling schedule.  Often this can be rigged to provide a large number of
cold draws.  The algorithm above can also be thought of as an augmented sampler in the context of Tanner and Wong (1987) 
where temperature is the augmentation variable.

\begin{figure}[h]
\vspace{-66pt}
\begin{center}
\parbox[l]{\linewidth}{ \hspace{0.45in}
	\hspace{-0.18in}\epsfig{file=Images/mcmc_advanced.figure01.ps,height=4.75in,width=3in,clip=,angle= 270}  
	\vspace{-33pt}
	\caption{\textsc{Simulated Annealing Applied to the Witch's Hat Distribution}}\label{Annealing.Heating.Fig}
} 
\end{center}
\end{figure}
\index{subjectindex}{simulated annealing!applied to witch's hat}
%\vspace{33pt}

\index{subjectindex}{witch's hat distribution}
Figure \ref{Annealing.Heating.Fig} shows the infamous witch's hat distribution centered at $[0.6,0.4]$ (see 
Exercise~\ref{MCMC.Extensions.Chapter}.\ref{witch's.hat.exercise} for details) which has been suggested as an MCMC diagnostic 
since it causes serious mixing problems 
with various algorithms, particularly in high dimensions.  Geyer and Thompson (1995) \index{authorindex}{Geyer, C. J.} 
\index{authorindex}{Thompson, E. A.} apply simulated tempering\index{subjectindex}{simulated tempering} to improve the mixing 
problem caused by the difficulty of moving from the brim area to the peak, and obtain an acceptance rate improvement from 
7\% to 72\%.  The panels of Figure \ref{Annealing.Heating.Fig} show the ``cold'' distribution at T=1 and heated up versions at 
T=25 and T=300 plotted on the unit square.  Notice how the troublesome crease at the base of the peak is melted down.

\subsection{Tempored Transitions}
\index{subjectindex}{tempered transitions} \index{subjectindex}{detailed balance equation}
Neal (1996) \index{authorindex}{Neal, R. M.} builds on simulated tempering with \emph{tempered transitions} to heat 
up the posterior distribution in-place so that a random walk can move more freely, but also to preserve the detailed balance 
equation (reversibility condition) at each step.  \index{authorindex}{Celeux, G.}\index{authorindex}{Hurn, M.}\index{authorindex}{Robert, C. P.} 
See also Celeux, Hurn, and Robert (2000) for an application to mixture distributions, and Liu and Sabatti 
\index{authorindex}{Liu, J. S.} \index{authorindex}{Sabatti, C.} (1999) for the ``simulated simpering'' variant.  The basic 
idea is to ``ladder'' up and down in heat at each time $t$ with random walk steps.  Each ladder step specifies a (non-normalized) 
stationary distribution defined on the same state space but at progressively hotter temperatures going up.  Finally the last 
(bottom) ladder value is accepted or discarded with a Metropolis decision.  This process is summarized by:
\begin{bayeslist}
	\item	$\tau_1$ is the target joint density,
	\item   $\beta_i$ is the temperature value at the $i$th ladder step,
        \item   tempered transitions: define a sequence of candidate densities
                                $m_i, i=1, \ldots, N$, where as $i$ increases the $m_i$ get
                                ``flatter'' going up the ladder, then again more peaked going
                                down the ladder.
        \item   parameterize: $m_i=m^{1/\beta_i}$,
        \item   where: $1 < \beta_1 < \beta_2 < \cdots < \beta_{N-1} < \beta_N$
        \item   then: $\beta_N > \beta_{N+1} > \cdots > \beta_{2N-2} > \beta_{2N-1} >1$.
        \item   Starting from the original candidate $m$, at each step we cycle through the $m_i$ as follows:
                \begin{enumerate}
                        \item   If we let $K(\ca,m)$ denote an MCMC kernel with position $\ca$ and stationary 
				distribution $m$,
                        \item   then we use the following transitions starting at iteration $t$:
                        \begin{alignat*}{2}
                                &\text{\bf step 0:}    &\quad \ca_{1,0}^\prime &\sim \mbox{K}(\ca_1^{[t]},\tau_1) \\
                                &\text{\bf step 1:}    &\quad \ca_{1,1}^\prime &\sim \mbox{K}(\ca_{1,0}^\prime,m_1) \\
                                &\quad\vdots &\quad &\\
                                &\text{\bf step N:}    &\quad \ca_{1,N}^\prime &\sim \mbox{K}(\ca_{1,N-1}^\prime,m_N) \\
                                &\text{\bf step N+1:} &\quad \ca_{1,N+1}^\prime &\sim \mbox{K}(\ca_{1,N}^\prime,m_{N-1}) \\
                                &\quad\vdots &\quad &\\
                                &\text{\bf step 2N-1:} &\quad \ca_{1,2N-1}^\prime &\sim \mbox{K}(\ca_{1,2N-2}^\prime,m_{1}).
                        \end{alignat*}
                        \item The sequence of $\ca_1$ values is then input into a final Metropolis-Hastings
                                acceptance step, accepting $\ca_{1,2N-1}^\prime$ as $\ca_1^{[t+1]}$ with probability:
                                \begin{equation*}
                                \min\left\{1,
                                        \frac{m_1(\ca_1^{[t]})}{\tau_1(\ca_1^{[t]})}
                                \ldots
                                        \frac{m_N(\ca_{1,N-1}^\prime)}{m_{N-1}(\ca_{1,N-1}^\prime)}
                                \ldots
                                        \frac{\tau_1(\ca_{1,2N-1}^\prime)}{m_1(\ca_{1,2N-1}^\prime)}
                                \right\},
                                \end{equation*}
                                which preserves the detailed balance condition.\index{subjectindex}{detailed balance equation}
                \end{enumerate}
\end{bayeslist}

To look at this in a slightly different way, we can also substitute the $\beta_i$ parameterization back in.  Now accept 
$\ca_{1,2N-1}^{[t]}$ as $\ca_1^{[t+1]}$ with probability:\\
\begin{align*}  
                \min{\huge \Biggl\{}1,& 
                    \left( \frac{ m^{1/\beta_1}(\ca_{1,0}^{[t]}) }{ m^{1}        (\ca_{1,0}^{[t]}) } \right)
                    \left( \frac{ m^{1/\beta_2}(\ca_{1,1}^{[t]}) }{ m^{1/\beta_1}(\ca_{1,1}^{[t]}) } \right)  
                    \left( \frac{ m^{1/\beta_3}(\ca_{1,2}^{[t]}) }{ m^{1/\beta_2}(\ca_{1,2}^{[t]}) } \right)  \nonumber \\[11pt]
                \ldots&         
                    \left( \frac{ m^{1/\beta_{N-1}}(\ca_{1,N-2}^{[t]}) }{ m^{1/\beta_{N-2}}(\ca_{1,N-2}^{[t]}) } \right) 
                    \left( \frac{ m^{1/\beta_N}    (\ca_{1,N-1}^{[t]}) }{ m^{1/\beta_{N-1}}(\ca_{1,N-1}^{[t]}) } \right)  
                    \left( \frac{ m^{1/\beta_{N+1}}(\ca_{1,N}^{[t]})   }{ m^{1/\beta_{N}}  (\ca_{1,N}^{[t]}) }   \right)  \nonumber \\[11pt]
                \ldots& 
                    \left( \frac{ m^{1/\beta_2}(\ca_{1,2N-3}^{[t]}) }{ m^{1/\beta_3}(\ca_{1,2N-3}^{[t]}) } \right)
                    \left( \frac{ m^{1/\beta_1}(\ca_{1,2N-2}^{[t]}) }{ m^{1/\beta_2}(\ca_{1,2N-2}^{[t]}) } \right)  
                    \left( \frac{ m^{1}        (\ca_{1,2N-1}^{[t]}) }{ m^{1/\beta_1}(\ca_{1,2N-1}^{[t]}) } \right)
                {\huge \Biggr\}}.       
\end{align*}            
Note that $\ca_{1,0}^{[t]} = \ca_1^{[t]}$.  We can also add a weighting function within each term above: 
$\frac{w(\ca_{1,0}^{[t]})}{w(\ca_{1,2N-1}^{[t]})}$ (sometimes called a \emph{pseudo-prior} in this context).  The original 
stationary distribution of the Markov chain is maintained as long as the $m_i$ satisfy a detailed balance condition, which 
is given in Neal (1996) with proof. \index{authorindex}{Neal, R. M.}\index{subjectindex}{detailed balance equation}

This sequence of transitions allows excellent exploration of the parameter space, as the density $m_N$ is typically chosen as 
very ``hot," for example, uniform on the entire space.  Setting $\beta_i - \beta_{i+1}$ as small gives higher acceptance rates 
but poorer mixing.  Conversely a large difference between $m$ and $m^{1/\beta}$ is good for mixing around the space but may 
lead to inordinately high rejection rates.  Both criteria can be satisfied with taller ladders: more steps and a higher 
maximum temperature.  
\begin{figure}[h]
\begin{center}
%\vspace{-300pt}
\vspace{-200pt}
\parbox[c]{\linewidth}{  \hspace{0.5in}
	\hspace{-2.75in} \vspace{2in}
	\epsfig{file=Images/mcmc_advanced.figure02.ps,clip=,angle=270,width=10in} 
	\vspace{-2.75in}
	\caption{\textsc{A Highly Multimodal Bounded Surface}} \label{ugly.figure}
}
\end{center}
\end{figure}

\subsection{Comparison of Algorithms}
It is interesting to compare these approaches with a deliberately ``ugly'' example objective function on $[-1,1]^2$:
\begin{align}\label{ugly.function}
	f(x,y) &= \text{abs}( (x\sin(20y-90) - y\cos(20x+45))^3  a\cos(\sin(90y+42)x)  \\
               &\qquad\qquad         +(x\cos(10y+10) - y\sin(10x+15))^2  a\cos(\cos(10x+24)y) ).
\end{align}
This function is displayed in Figure~\ref{ugly.figure}.  While the vast majority of posterior forms will not exhibit such 
challenging characteristics, it still serves as a benchmark for algorithm performance.  Furthermore, increasingly complex
model specifications \emph{are} much more prevalent in recent Bayesian and non-Bayesian work in the social sciences, 
leading to potentially similar forms.  This function, while bounded, presents problems for conventional MCMC algorithms
since it has multiple modes concentrated at the corners and a wide, flat plain in the middle that must be traversed to
fully describe the functional form.  

We now apply a regular random walk Metropolis-Hastings algorithm, simulated annealing, and tempered transitions to this
function.  Each Markov chain is run for 5,000 iterations (an insufficient but illustrative period), and the chain visits
are given in Figure~\ref{ugly.travel.figure}.  Such a number of iterations is revealing here because, while all of these
algorithms are ergodic and will therefore \emph{eventually} explore the full target form, our concern is with the rate
at which they do so.  Specifically, do the algorithms differ in the efficiency by which they mix through the space?

It is clear that the standard algorithm fails during this period to break out of the diagonal, and spends the bulk of its time
visiting two of the four corners where large modes exist.  The simulated annealing algorithm appears to be traversing the space
much better but still fails to break out of the diagonal area.  Conversely, the algorithm based on tempered transitions manages to
explore the full state space, even in this small number of iterations.  So the leverage that we gain from using the tempered
transitions is greater assurance that we have mixed through the target form in a fixed amount of time.

\begin{center}
\begin{figure}[h!]
\parbox[c]{\linewidth}{ \hspace{0.4in} 
    \epsfig{file=Images/mcmc_advanced.figure03.ps,clip=,angle=270,width=4.65in} 
}
\vspace{11pt}
\caption{\textsc{A Comparison of Travels}} \label{ugly.travel.figure}
\end{figure}
\end{center}

\subsection{Dynamic Tempered Transitions}
\index{authorindex}{Gill, J.} \index{authorindex}{Casella, G.}
Gill and Casella (2004) extend the idea of tempered transitions to account for current placement of the chain in the state space.
The objective is to escape the necessary trade-offs in ladder height (maximum heating level) and spacing between rungs (number of
steps).  When the area around the chain is highly irregular, it is better to have a lower (cooler maximum temperature) ladder in
order to get to the top of the mode and fully explore this area.  When the area around the chain is smooth, it is better to have a
longer (hotter maximum temperature) ladder in order to avoid being trapped in the low density region.  Also, setting the number of
rungs for a given ladder height as too small can reduce the acceptance rate because high-quality candidates may not be offered.
Conversely, setting the number of rungs as too large can also reduce the acceptance rate because of the product in the
Metropolis-Hastings decision step.

The general strategy in these regards is to specify a distribution of ladders all having the same number of rungs,
but differing heights (differing maximum temperatures).  We observe the multidimensional curvature at the current
Markov chain location and specify a greater probability of selecting a cooler ladder when this curvature is high,
and a greater probability of selecting a hotter ladder when the curvature is low.  The number of rungs is essentially
a nuisance parameter, which can be fixed at the beginning of the chain or tuned during the early runs by comparing
acceptance probabilities.  The actual goal is not to determine the exact optimal number of rungs, but to pick
a reasonable number that is not overly affecting the acceptance probabilities.

The key challenge is that by making the behavior of the Markov chain adjust to its surroundings (i.e., conditional on changes in
the posterior for $\ca$), we run the risk of creating a non-homogeneous Markov chain, and also losing the detailed balance
equation.  This would then deny the ability to assert ergodicity of the chain.  We can solve this problem by taking advantage of
the structure of the Metropolis algorithm.  \index{subjectindex}{detailed balance equation}

Let $f(\ca)$ be the stationary distribution (objective function), let $g(\ca'|\ca)$ be a candidate distribution, 
and let $K(\ca,\ca')$ be the associated transition kernel.  By the construction of the Metropolis algorithm, 
$K(\ca,\ca')$ is given by:
\begin{equation}
	K(\ca,\ca')=\min \left\{\frac{f(\ca')g(\ca|\ca')}{f(\ca)g(\ca'|\ca)}, 1 \right\}g(\ca'|\ca) 
			+ (1-r(\ca))\delta_\ca(\ca'),
\end{equation}
where:
\begin{equation}
	r(\ca) = \int \min \left\{\frac{f(\ca')g(\ca|\ca')}{f(\ca)g(\ca'|\ca)}, 1 \right\}g(\ca'|\ca) d\ca'
\end{equation}
and $\delta_\ca(\ca')=1$ if $\ca=\ca'$ and zero otherwise.  The kernel $K(\ca,\ca')$ now satisfies detailed 
balance with $f(\ca)$ as the stationary distribution (exactly from Robert and Casella 1999, Theorem 6.2.3).
\index{authorindex}{Robert, C. P.} \index{authorindex}{Casella, G.}

Now, for each $\ca$, let $\rho(\lambda|\ca)$ be a probability distribution, that is, $\rho(\lambda|\ca) \ge 0$ and
$\int \rho(\lambda|\ca) d \lambda =1$.  Here we are considering $\lambda$ to be continuous, to be general, but 
usually $\lambda$ will be discrete.  Our candidate distribution is:
\begin{equation}
        g^*_\lambda(\ca'|\ca) = \rho(\lambda|\ca),
\end{equation}
and forms the Metropolis kernel based on $g^*_\lambda(\ca'|\ca)$ and $f(\ca)$.  By construction, detailed balance 
is satisfied and we have an ergodic Markov chain.\index{subjectindex}{detailed balance equation}

As an example, suppose that there are $i=1, \ldots, k$ ladders, and as $i$ increases the ladders get hotter.  If 
$|f^{\prime \prime}(\ca)|$ is big (so we are near a mode) we might want to favor the cooler ladders.  To do this 
we can take $\rho(\lambda|\ca)$ to be a binomial mass function with $k$ trials and success probability $p(\ca)$, 
where
\begin{equation}
        \mbox{logit} p(\ca) = a -b |f^{\prime \prime}(\ca)|, \quad b > 0.
\end{equation}
So big values of $|f^{\prime \prime}(\ca)|$ would result in small $p(\ca)$, which would favor the smaller 
values of $i$ and the cooler ladders.  This way, on average, we spend sufficient time exploring 
the modal area.  Eventually, since there is always a positive probability of getting a hot ladder, the 
chain eventually escapes from every mode.   In flat areas, since hot ladders are more probable, this happens 
on average sooner.  

\section{Reversible Jump Algorithms}\label{section:reversible.jump}
\index{subjectindex}{Markov chain Monte Carlo (MCMC)!reversible jump|(} 
Green (1995) \index{authorindex}{Green, P. J.} introduces \emph{reversible jump Markov chain Monte Carlo} (RJMCMC) where the model
specification (i.e., variable selection) is part of the estimation process.  The key idea is to modify a Metropolis-Hastings kernel
to jump not only between points in the parameter space but also between different model specifications (although the Gibbs sampler
has been used in at least one RJMCMC algorithm, Keith \etal [2004]). Recently Geyer (2011) argues that RJMCMC is a fundamental
feature of MCMC and regular implementations are really special cases.  \index{authorindex}{Geyer, C. J.}

Reversible jump MCMC is a form of Bayesian model averaging (page~\pageref{model.averaging.section}) where the posterior
probability of a model is provided by the proportion of the total MCMC (post-convergence) run-time that the chain spends in the
space defined by that model.  Interestingly, the number of models considered does not need to be specified in advance, but model
priors are still required.  For example, Richardson and Green (1997) express this idea with ``births'' and ``deaths'' of model
alternatives (see also Zhang \etal [2004]).  We will very generally describe the algorithm here, and for details, see the original
article, or the reviews by Waagepetersen and Sorensen (2001), Clyde (1999), Chen \etal (2000, pp.300-303), Andrieu, Djuri\'{c}, and
Doucet (2001), or Richardson and Green (1997).
\index{authorindex}{Andrieu, C.} \index{authorindex}{Djuric, P. M.@Djuri\'{c}, P. M.} \index{authorindex}{Doucet, A.}
\index{authorindex}{Waagepetersen, R.} \index{authorindex}{Sorensen, D.} \index{authorindex}{Clyde, M.}
\index{authorindex}{Chen, M-H.} \index{authorindex}{Richardson, S.} \index{authorindex}{Green, P. J.} 
\index{authorindex}{Zhang, Z.}

Suppose we have a countable set of $K$ alternative model specifications to evaluate, denoted $\{\mathcal{M}_k, k \in K\}$.  Naturally 
these models have different numbers of parameters, which may or may not provide nesting, such model $\mathcal{M}_k$ has a $n_k$-length
($n_k \ge 1$) parameter vector $\theta^{(k)}$ measured on $\Re^{(n_k)}$.  For observed data $\y$, the joint distribution of interest is 
formed from the product of the prior on model choice, $p(k)$, the prior for the coefficient vector given model choice, $p(\theta^{(k)}|k)$, 
and the likelihood for $\y$ given model choice, $L(\y|k,\theta^{(k)})$, producing:
\begin{equation}
	p(k,\theta^{(k)},\y) = p(k)p(\theta^{(k)}|k)L(\y|k,\theta^{(k)}).
\end{equation}
Since the algorithmic focus is on moving between model specifications, start with the pair $(k,\theta^{(k)})$, and denote it with simply $x$, 
which for a specific case of $k$ must lie in the parameter space $\mathcal{C}_k = \{k\} \times \Re^{n_k}$.  Therefore in the course of the 
algorithm $x$ varies over the combined parameter space defined by $\mathcal{C} = \cup_{k \in K}\mathcal{C}_k$.  Since the pairing is 
inseparable, the posterior of interest is the joint expression:
\begin{equation}
	\pi(x|\y) = \pi(k,\theta^{(k)}|\y) = p(k|\y)p(\theta^{(k)}|k,\y).
\end{equation}

If $(k,\theta^{(k)})$ is the current position of the Markov chain at time $t$, then denote $x' = (m,\theta^{(m)}) \in \{m\} \times
\Re^{n_m}$ as a proposed destination for time $t+1$ produced by the candidate-generating distribution with probability
$q_m(x,x')$.  One useful way to generate such candidates is by the inclusion of a random component, $\bU$ in $\Re^{n_{km}}$, $km
\ge 1$, independent of $x$ with density $q_{km}(\theta^{(km)},\cdot)$.  So define the deterministic mapping $g_{mk}\range \Re^{n_k
+ n_{km}} \rightarrow \Re^{n_m}$ such that we can rewrite: $x' = g_{mk}(x,u)$.  Now $g_{mk}$ is called a \emph{bijection} since it
gives a one-to-one mapping between $(\theta^{(k)},u)$ and $(\theta^{(m)},u')$, where $u'$ is the reverse analog of $u$ from $x'$
instead of $x$.  

This is a setup to meet the \emph{dimension matching condition}, which is a necessary part of ensuring that the detailed balance equation 
holds.\index{subjectindex}{dimension matching condition}  The potential move under consideration is:\index{subjectindex}{detailed balance equation}
\begin{equation}
	(k,\theta^{(k)}) \longrightarrow (m,\theta^{(m)}) = (m,g_{mk}(x,u)),
\end{equation}
with the corresponding reverse move:
\begin{equation}
	(m,\theta^{(m)}) \longrightarrow (k,\theta^{(k)}) = (k,g_{km}(x',u')).
\end{equation}
This now makes it straightforward to match the dimensions:
\begin{equation}
	n_k + n_{km} = n_k + n_{mk}
\end{equation}
so that 
\begin{equation}
	\pi(k,\theta^{(k)},\y)g_{mk}(x,u) = \pi(m,\theta^{(m)},\y)g_{km}(x',u')
\end{equation}
holds.
This leads directly to the Metropolis-Hastings acceptance criteria:
\begin{equation}\label{rjmcmc.acceptance}
	\alpha(x,x') = \min\left\{ 1, \frac{ \pi(m,\theta^{(m)},\y) }{ \pi(k,\theta^{(k)},\y) }
				     \frac{ q_m(x,x') }{ q_k(x',x) }
				     \frac{ q_{mk}(\theta^{(mk)},u') }{ q_{km}(\theta^{(km)},u) }
			   \left|    \frac{ \partial g_{mk}(x,u) }{ \partial x \partial u } \right|
		       \right\},
\end{equation}
where the Jacobian is necessary because $x' = g_{mk}(x,u)$ is a deterministic function in the proposal process for the change in variable 
from $(x,u)$ to $x'$.  Under a wide range of applied circumstances $\alpha(x,x')$ is more simple than the general form given here.  

Finally, suppose we have a (post-convergence) MCMC sample indexed $i=1,2,\ldots,I$, and define the following indicator function:
\begin{equation}
	\mathcal{1}_{i}(i_k) =
	\begin{cases}
		1,	& \text{if}\; \mathcal{M}_i =   \mathcal{M}_k	\\
		0,	& \text{if}\; \mathcal{M}_i \ne \mathcal{M}_k.
	\end{cases}
\end{equation}
This function produces a $1$ if the $i$th model in the series is the $k$th specification in the countable set: $k \in K$.  Now the posterior
probability of model $k$ is simply:
\begin{equation}
	\pi(k|\y) = \frac{1}{I}\sum_{i=1}^{I} \mathcal{1}_{i}(i_k),
\end{equation}
where the posterior variance can be calculated according to the tools in Section~\ref{autocorrelation.section}.

The mechanics of RJMCMC are sometimes difficult to implement and must be customized for each application, although Hastie (2005) has developed
\index{authorindex}{Hastie, D.} the \texttt{AutoMix} package based on normal mixtures as a reasonably general approach.  Like all 
advanced MCMC techniques there are a number of tuning parameters that researchers need to pay close attention to.  In particular the 
candidate-generating distribution can be challenging in that proposals now have two criteria for acceptance: parameter values and model 
space.  Nonetheless, this is an active research area because the idea of assessing model quality within the context of the sampler is 
attractive.  \index{subjectindex}{Markov chain Monte Carlo (MCMC)!reversible jump|)} 

\section{Perfect Sampling}\label{Perfect.Sampling.Section}\index{subjectindex}{perfect sampling|(}
As discussed, one obvious worry is the length of the pre-convergence era of a particular Markov chain.  While we can be comforted by 
knowing that MCMC algorithms used in general practice are ergodic, an eventual decision about cessation is required.  An ingenious
solution to this problem is provided by Propp and Wilson (1996) \index{authorindex}{Propp, J. G.} \index{authorindex}{Wilson, D. B.}
who give a way to automatically obtain \emph{perfect} samples immediately from an MCMC algorithm that are guaranteed to be from the
stationary distribution of interest.  Their method, called \emph{coupling from the past},\index{subjectindex}{coupling from the past} 
(CFTP) is a particular kind of perfect sampling, although the terms are often (but incorrectly) used synonymously.  

The idea behind CFTP is that if a chain had been started at step $t=-\infty$ and run forward in time, then at time $t=0$ it must be
in its stationary distribution.  Their innovation is that they found a way to obtain this same sample without the inconvenience (or
impossibility!) of dealing with the infinite past.  Define terms as in previous chapters where the Markov chain operates on a finite
state space with the transition matrix $K(\theta,\theta')$ and the stationary (target) distribution $\pi(\theta)$.  Since this is
a finite state space we can consider all the ways to transition forward from $\theta^{[-1]}$ to $\theta^{[0]}$ using the expression
in \eqref{discrete.transition.probability}:
\begin{equation}
	p(\theta^{[0]} = j| \theta^{[-1]} = i) = K(i,j)
\end{equation}
and summing over these possible ways to produce the cumulative transition probability:
\begin{equation}
	p(\theta^{[0]} \le j| \theta^{[-1]} = i) = \sum_{k=1}^{j} K(i,k).
\end{equation}
Call this last equation $C(i,j)$ for clarity to distinguish it from $K(i,j)$.  Now draw a uniform random number ($u_0$) between 
zero and one and use the cumulative function to specify a move:
\begin{equation}
	\theta^{[0]} = j \quad \text{if} \quad C(i,j-1) < u_0 \le C(i,j).
\end{equation}
This is fairly routine MCMC so far but now perform this operation using the same $u_0$ for every state at time $t=-1$, not just 
$\theta^{[-1]} = i$.  This means we have a parallel set of Markov chains equal in number to the set of unique states at time
$t-1$.  We can apply the transition rule defined by:\index{subjectindex}{perfect sampling!transition rule}
\begin{equation}
	\theta^{(0)} = \phi(u_0,\theta^{(-1)}).
\end{equation}

Now comes the innovative part.  If
\begin{equation}
	\phi(u_0,\theta^{(-1)} =i) = j, \quad \forall i
\end{equation}
then we say that the chain has \emph{coupled} and the value given by $j$ is an exact (perfect) sample from $\pi(\theta)$.  Why is this
true?  If, hypothetically, the Markov chain had been run from $t=-\infty$ to $t=-1$, then there is no question that it is in stationarity
at time $t=-1$.  If the Markov chain is in stationarity at time $t=-1$ then it is clearly in stationarity at time $t=0$ from the condition
imposed by $\phi(u_0,\theta^{(-1)})$, and of course for any time thereafter.  All this certainty makes this result sound frequent and easy,
but the truth is that the probability that all of the parallel Markov chains couple at this one step is inconveniently low.  This is not
a difficult impediment since the selection of the zero time point was arbitrary, thus:
\begin{equation}
	\theta^{(-t)} = \phi(u_{-t},\theta^{(-t-1)}).
\end{equation}	
Applying this general idea recursively through the history of the chain gives:
\begin{equation}
	\theta^{(0)} = \phi(u_{0},\phi(u_{-1},(\phi_{-2},\ldots,\phi(u_{-T+1}(\theta^{(-T)})\cdots)))),
\end{equation}	
where $T$ is some large number of our choosing, possibly very large, and we have a series of uniform draws identified: $u_{-T},u_{-T+1},
\ldots,u_{0}$.  If we start our $M$ parallel Markov chains at time $-T$, for each
of the $M$ possible discrete locations in the state space, then at step $-T+1$ we will have a number of unique chain locations equal to or
less than $M$: a \emph{coalescence} of chains.  Since such chains remain coalesced permanently, over time the collection of $M$ chains 
will eventually coalesce to a single chain, and when this happens we will call it time $t=0$.  Note that any collection of $M$ Markov 
chains started at $t=-\infty$ will have come through the time $t=-T$ with $M$ or less unique states, then the sample at $t=0$ must be that
defined by $\theta^{(0)}$ above.\index{subjectindex}{perfect sampling!coalescence}

Various implementations of CFTP use the coupling strategy in incremental steps.  The objective is to find the time $-T$ that accomplishes
the full coalescence above, and such that $\theta^{(0)}$ is independent of $\theta^{(0)}$.  Suppose again that there are $M$ states in
the sample space and $\phi(u_{j},\theta^{(j)})$ is a transition rule at time $j$.  Then the steps proceed as follows:
\begin{enumerate}
	\item	Start $M$ parallel chains in each of the $M$ states at time $t=-1$, and generate $u_0 \sim \mathcal{U}(0,1)$.
	\item	Apply the transition rule, $\phi(u_{0},\theta^{(-1)})$, to each of the chains.  If all of the chains have
		coalesced at time $t=0$, then set $-T=-1$.  The value $\theta^{(0)}$ is a perfect draw from $\pi(\theta)$.
	\item	If all the chains have not coalesced, then go to time $t=-2$, generate $u_{-1} \sim \mathcal{U}(0,1)$, and	
		apply the transition rule, $\phi(u_{-1},\theta^{(-2)})$, to each of the chains.  If all of the chains have
                coalesced at time $t=-1$, then set $-T=-2$.  The value $\theta^{(0)}$ is a perfect draw from $\pi(\theta)$.
	\item	If all the chains have not coalesced, then go to time $t=-2$, and repeat.
	\item	Continue as necessary back in time until full coalescence.
\end{enumerate}
It is important to remember that no matter how far we go back, the first value that is a perfect draw from the stationary distribution
is still the common value at time $t=0$.  From that point on we can then collect perfect simulations without worrying about convergence.
One other reminder is warranted.  Moving backward it is critical to record the uniform draws so that we can use \emph{these exact 
values} going forward again: $u_{-T},u_{-T+1},\ldots,u_{-1},u_{0}$.

\index{subjectindex}{Fill's algorithm|(}
It should be clear that the time required, $T$, and $\theta^{(0)}$ are random variables dependent on each other such that the full
coalescence process must complete before sampling can move forward with draws from the stationary distribution.  This may be a problem
in practical applications if the initial process is interrupted.  Fill (1998) \index{authorindex}{Fill, J. A.} addresses this with a 
version of perfect sampling (perfect rejection sampling) that is independent of $T$.  It proceeds in similar fashion for the same setup:
\index{subjectindex}{perfect rejection sampling}
\begin{enumerate}
	\item	Choose a time $T$ and a state at this time $\theta^{(T)} = \theta$ at convenience.
	\item	Generate a conditional series backward to the zero point: $\theta^{(T-1)}|\theta^{(T)}$, $\theta^{(T-2)}|\theta^{(T-1)}$,
		    \ldots, $\theta^{(1)}|\theta^{(2)}$, $\theta^{(0)}|\theta^{(1)}$.
	\item	Generate a series of associated uniform draws:\\ $(u_{1}|\theta^{(0)},\theta^{(1)})$, $(u_{2}|\theta^{(1)},\theta^{(2)})$,
		    \ldots, $(u_{T-1}|\theta^{(T-2)},\theta^{(T-1)})$, $(u_{T}|\theta^{(T-1)},\theta^{(T)})$.
	\item	Run $M$ parallel chains starting at time $T=0$ and use the same uniform draws to update all chains.
	\item	If all the chains have coalesced by the time the series runs out at $T$, then accept $\theta^{(0)}$ as a draw
		    from $\pi(\theta)$.
	\item	If all the chains have not coalesced by this time then run the algorithm again, perhaps with a larger value of $T$.
\end{enumerate}
Relatively large values of $T$ in general are recommended by Fill (1998) and others.\index{authorindex}{Fill, J. A.}

Fill \etal (1999) and Casella \etal (2001) \index{authorindex}{Fill, J. A.}\index{authorindex}{Casella, G.} provide the necessary
proofs that this time reversal strategy removing the dependence between the backward path length and the sample value generated at
time $T=0$ provides samples from the stationary distribution.  Clearly the second step of conditioning given the initial path is
the most challenging part here.  However, the independence between the length of the path generation and the production of
$\theta^{(0)}$ values combined with the reuse of the uniform draws makes this process relatively efficient for users.  Notable
extensions have been developed, including the general version of this algorithm provided by Fill \etal
(1999).\index{authorindex}{Fill, J. A.} Murdoch and Rosenthal (1998) \index{authorindex}{Murdoch, D. J.}
\index{authorindex}{Rosenthal, J. S.} remove the required assumption of stochastic monotonicity of the chain.
\index{authorindex}{Moller, J.@M{\o}ller, J.} \index{authorindex}{Schladitz, K.} M{\o}ller and Schladitz (1999) are also able
remove this assumption in the context of stochastic repulsive sequences, leading new applications.
\index{subjectindex}{Fill's algorithm|)}

Several mechanical challenges remain with perfect sampling.  Unless the number of states is relatively small, then the size of the
process at each step can be awkward and tracking coalescence can be burdensome.  For non-trivial state spaces (i.e., excluding the
toy examples that dominate published introductions), it can be very difficult to show that a large number of paths have all
coalesced.  However, if the states are ordered with a monotone transition rule, then one convenient shortcut is to track
coalescence of just the maximum and minimum values at each step since these will squeeze the other values towards complete
coupling.  By monotone here we mean that $\theta^{(t)}_i \ge \theta^{(t)}_j$ implies $\theta^{(t+1)}_i \ge \theta^{(t+1)}_j$ (Fill
1998, Definition 4.2),\index{authorindex}{Fill, J. A.} i.e., paths on lower starting points remain below paths on higher starting
points until full coalescence.  Furthermore, setting up and running unbiased algorithms is labor-intensive and specific to each
application.  Therefore we are not likely to see commercial software support in the foreseeable future.  Finally, it is critical
to reuse the uniform random draws correctly for unbiased properties to hold.  \index{subjectindex}{perfect sampling!monotonicity}

Perfect sampling is one of the more exciting new research areas in MCMC.  Good reviews of perfect sampling include Casella \etal
(2001) and Craiu and Meng (2011).\index{authorindex}{Craiu, R. V.} \index{authorindex}{Meng, X-L.} \index{authorindex}{Casella, G.} 
The biggest challenge, of course, is to generalize the state space definition (see Green and Murdoch [1998] as well as Murdoch
and Green [1998]).  \index{authorindex}{Green, P. J.} \index{authorindex}{Murdoch, D. J.} Meng\index{authorindex}{Meng, X-L.}
(2000) produced an algorithm based on a multistage version of the CFTP backward coupling scheme using cluster sampling to reduce
time to coalescence.  Hobert \etal (1999)\index{authorindex}{Hobert, J. P.} are able to impose the monotonicity requirement on
cases with two and three component mixtures, but apparently with a high computational cost.  Corcoran and Tweedie (1998)
\index{authorindex}{Corcoran, J. N.} \index{authorindex}{Tweedie, R. L.} show that perfect sampling based on the independent
Metropolis-Hastings kernel has useful monotonicity properties and is therefore a good algorithmic choice.  Brooks \etal (2006)
\index{authorindex}{Brooks, S. P.} link perfect sampling with simulated tempering discussed earlier in this chapter.  Casella
\etal (2002) develop perfect slice samplers and apply them to mixtures of distributions.  Murdoch and Takahara (2006) develop
applications to queueing theory and networks.  \index{authorindex}{Murdoch, D. J.}\index{authorindex}{Takahara, G.} However, there
are more applications in statistical physics and point processes (stochastic processes with binary outcomes occurring over
continuous time) than there are in the social sciences.  \index{subjectindex}{perfect sampling|)}

\section{Hamiltonian Monte Carlo}\label{Hamiltonian.Section}\index{subjectindex}{Hamiltonian Monte Carlo|(}
Hamiltonian Monte Carlo (also called Hybrid Monte Carlo) is a variant of the Metropolis-Hastings algorithm that uses physical
system dynamics as a means of generating candidates for a Metropolis decision. When properly implemented it is faster than regular
Metropolis-Hastings because it incorporates more information about posterior topography. This idea was originally presented by Duane
\etal (1987), and further developed by Neal (1993). This section will very generally outline the method and the most detailed
description for MCMC purposes is currently Neal (2011).
\index{authorindex}{Duane, S.}
\index{authorindex}{Kennedy, A. D.}
\index{authorindex}{Pendleton, B. J.}
\index{authorindex}{Roweth, D.}
\index{authorindex}{Neal, R. M.}

\index{authorindex}{Meyer, Kenneth R}
\index{authorindex}{Hall, G. J., Jr.}
\index{subjectindex}{Hamiltonian!dynamics}
As noted by every description to date, a comprehension of Hamiltonian dynamics is a necessary qualification for understanding
hybrid Monte Carlo. This is a core topic in physics, so there exist many relevant textbooks such as that by Meyer and Hall (1992).
Hamiltonian dynamics is the model whereby physicists describe an object's trajectory within a defined system.
First define $\vT_t$ as a $k$-dimensional location vector and $\p_t$ as a $k$-dimensional momentum (mass times velocity) vector,
both recorded at time $t$. The Hamiltonian system at time $t$ with $2k$ dimensions is described by the joint Hamiltonian function:
\begin{equation}
    H(\vT_t,\p_t) = U(\vT_t) + K(\p_t)
\end{equation}
(sometimes just abbreviated $H$), where $U(\vT_t)$ is the function describing the \emph{potential energy} at the point $\vT_t$, and
$K(\p_t)$ is the function describing the \emph{kinetic energy} for momentum $\p_t$. Neal (2011) gives the simple $1$-dimensional
example:
\begin{equation}
    U(\vartheta_t) = \frac{\vartheta_t^2}{2} \qquad\qquad K(p_t) = \frac{p_t^2}{2},
\end{equation}
which is equivalent to a standard normal distribution for $\vartheta$. Commonly the kinetic energy function is defined as:
\index{subjectindex}{kinetic energy function}
\begin{equation}\label{kinetic.energy.function}
    K(\p_t) = \p_t'\SI^{-1}\p_t,
\end{equation}
where $\SI$ is a symmetric and positive-definite matrix that can be as simple as an identity matrix times some scalar that can
serve the role of a variance: $\SI = \sigma^2\I$.  This simple form is equivalent to the log PDF of the multivariate normal with
mean vector zero and variance-covariance matrix $\SI$.

\index{subjectindex}{Hamiltonian!dynamics}
Hamiltonian dynamics describe the gradient-based way that potential energy changes to kinetic energy and kinetic energy changes to
potential energy as the object moves over time throughout the system (multiple objects require equations for gravity, but that is
fortunately not our concern here). The mechanics of this process are given by Hamilton's equations, which are the set of simple
differential equations:
\begin{align}
    \frac{\partial \vT_{it}}{\partial t} &= \frac{\partial H}{\partial \p_{it}} = \frac{K(\partial \p_{it})}{\partial \p_{it})} \\
    \frac{\partial \p_{it}}{\partial t} &=-\frac{\partial H}{\partial \vT_{it}} =-\frac{U(\partial \vT_{it})}{\partial \vT_{it})} 
\end{align}
for dimension $i$ at time $t$. For continuously measured time these equations give a mapping from time $t$ to time $t+\tau$,
meaning that from some position $\vT_t$ and momentum $\p_t$ at time $t$ we can predict $\vT_{\tau}$ and $\p_{\tau}$.  Returning to
the one-dimensional standard normal case, these equations are simply $d\vartheta_t/dt=p$ and $dp/dt=-\vartheta$. 

\index{subjectindex}{Hamiltonian!dynamics} \index{authorindex}{Neal, R. M.}
There are three important properties of Hamiltonian dynamics that are actually \emph{required} if we are going to use them to
construct an MCMC algorithm (Neal 2011). First, Hamiltonian dynamics is \emph{reversible}, meaning that the mapping from
$(\vT_t,\p_t)$ to $(\vT_{t+\tau},\p_{t+\tau})$ is one-to-one and therefore also defines the reverse mapping from
$(\vT_{t+\tau},\p_{t+\tau})$ to $(\vT_t,\p_t)$. Second, \emph{total} energy is conserved over time $t$ and dimension $k$, and the
Hamiltonian is invariant, as shown by:
\begin{equation}
    \frac{\partial H}{\partial t} 
        = \sum_{i=1}^{k}\left[ \frac{\partial \vT_i}{\partial t} \frac{\partial H}{\partial \vT_i} 
                             + \frac{\partial \p_i}{\partial t} \frac{\partial H}{\partial \p_i} \right]
        = \sum_{i=1}^{k}\left[ \frac{\partial H}{\partial \p_i} \frac{\partial H}{\partial \vT_i} 
                             - \frac{\partial H}{\partial \vT_i} \frac{\partial H}{\partial \p_i} \right]
        = 0.
\end{equation}
\index{subjectindex}{Hamiltonian!dynamics}
This provides detailed balance (reversibility) for the MCMC algorithm. Second, Hamiltonian dynamics preserve volume in the $2k$
dimensional space. In other words, elongating some region in a direction requires withdrawing another region as the process
continues over time. This ensures that there is no change in the scale of Metropolis-Hastings acceptance probability. Finally,
Hamiltonian dynamics provides a \emph{symplectic mapping} in $\mathcal{R}^{2k}$ space. Define first the smooth mapping $\PSI:
\mathcal{R}^{2k} \;\rightarrow\; \mathcal{R}^{2k}$ with respect to some constant and invertible matrix $\J$ with $\J' = -\J$ and
$\det(\J) \ne 0$, along with having Jacobian $\PSI(z)$ for some $z \in \mathcal{R}^{2k}$. The mapping $\PSI$ is symplectic if:
\begin{equation}
    \PSI(z)'\J^{-1}\PSI(z) = \J^{-1}.
\end{equation}
Leimkuhler and Reich (2005, p.53) give the following mapping in $2$-dimensional space $z=(\vartheta,p)$:
\index{authorindex}{Leimkuhler, B.} \index{authorindex}{Reich, S.}
\begin{equation}
    \PSI(\vartheta,p) = \left[ \begin{array}{c} p \\ 1 + b\vartheta + ap^2 \end{array} \right],
\end{equation}
with constants $a,b \ne 0$. The Jacobian of $\PSI(\vartheta,p)$ is calculated by:
\begin{equation}
    \frac{\partial}{\partial \vartheta} \frac{\partial}{\partial p}\PSI(\vartheta,p) 
        = \left[ \begin{array}{cc} 0 & 1 \\ b & 2ap \end{array} \right]. 
\end{equation}
We check symplecticness by:
\begin{equation}
    \left[ \frac{\partial}{\partial \vartheta}\frac{\partial}{\partial p}\PSI(\vartheta,p) \right]' \J^{-1}
    \left[ \frac{\partial}{\partial \vartheta}\frac{\partial}{\partial p}\PSI(\vartheta,p) \right] = 
    \left[ \begin{array}{cc} 0 & 1 \\ b & 2ap \end{array} \right]'
    \left[ \begin{array}{cc} 0 & -1 \\ 1 & 0 \end{array} \right]
    \left[ \begin{array}{cc} 0 & 1 \\ b & 2ap \end{array} \right] = -b\J^{-1}.
\end{equation}
Thus we say that $\PSI(\vartheta,p)$ is symplectic for $b=-1$ and any $a \ne 0$.

Everything discussed so far assumed continuous time, but obviously for a computer implementation in a Markov chain Monte Carlo
context we need to discretize time. So we will grid $t+\tau$ time into intervals of size $\upsilon$: $\upsilon, 2\upsilon,
3\upsilon,\ldots,m\upsilon$. We need a way to obtain this discretization while preserving volume, and so we use a tool called the
\emph{leapfrog methods}.\index{subjectindex}{leapfrog method} The notation is more clear if we now move $t$ from the subscript
to functional notation: $\vartheta(t)$ and $\p(t)$, which is also a reminder that time is now discrete rather than continuous.  To
complete a single step starting at time $t$, first update each of the momentum dimensions by $\upsilon/2$ with the following:
\begin{equation}
    \p_i\left(t+\frac{\upsilon}{2})\right) = p_i(t) - \frac{\upsilon}{2} \frac{\partial U(\vT_t)}{\partial \vT_i(t)}.
\end{equation}
Now take a full $\upsilon$-length step to update each of the position dimensions to leapfrog over the momentum:
\begin{equation}
    \vT_i(t+\upsilon) = \vartheta_i(t) + \upsilon \frac{\partial K(\p_t)}{\partial \p_i(t+\frac{\upsilon}{2})},
\end{equation}
and finish with the momentum catching up in time the step:
\begin{equation}
    \p(t+\upsilon) = \p_i\left(t+\frac{\upsilon}{2})\right) - \frac{\upsilon}{2} \frac{U(\vT_t)}{\partial \vT_i(t+\upsilon)}.
\end{equation}
\index{subjectindex}{leapfrog method}
Notice that the leapfrog method is reversible since it is a one-to-one mapping from $t$ to $t+\upsilon$.  Obviously, running these
steps $M$ times completes the Hamiltonian dynamics for $M \times \upsilon$ period of total time.  The determination of $\upsilon$
is a key tuning parameter in the algorithm since smaller values give a closer estimation to continuous time but also add more
steps to the algorithm.
\index{subjectindex}{Hamiltonian!dynamics}

A Metropolis-Hastings algorithm is configured such that the Hamiltonian function serves as the candidate-generating distribution.
This requires connecting the regular posterior density function, $\pi(\theta)$, to a potential energy function, $U(\vT_t)$, where 
a kinetic energy function, $K(\p_t)$, serves as a (multidimensional and necessary) auxiliary variable in the manner discussed in
Section~\ref{sec:auxiliary.variables} starting on \pageref{sec:auxiliary.variables}. This connection is done via the
\emph{distribution!canonical }\index{subjectindex}{distribution!canonical} commonly used in physics:
\begin{equation}\label{canonical.distribution}
    p(x) = \frac{1}{Z}\exp\left[ -\frac{E(x)}{T} \right],
\end{equation}
where $E(x)$ is the energy function of some system at state $x$, $T$ is the temperature of the system (which can simply be set at
$1$), and $Z$ is just a normalizing constant so that $p(x)$ is a regular density function. In the Hamiltonian context
\eqref{canonical.distribution} is:
\begin{align}\label{hamiltonian.canonical.distribution}
    p(\vT,\p) &= \frac{1}{Z}\exp\left[ -\frac{H(\vT,\p)}{T} \right] \nonumber\9
              &= \frac{1}{Z}\exp\left[ -\frac{U(\vT_t) + K(\p_t)}{T} \right] \nonumber\9
              &= \frac{1}{Z}\exp\left[ -\frac{U(\vT_t)}{T} \right] \exp\left[ -\frac{K(\p_t)}{T} \right],
\end{align}
demonstrating that $\vT$ and $\p$ are independent. Finally we connect the energy function metric with the regular posterior density
metric with the function:
\begin{equation}
    E(\vT) = -\log(\pi(\T)),
\end{equation}
thus completing the connection. Notice that the $\T$ variables must all be continuous in the model, although Hamiltonian Monte
Carlo can be combined with other MCMC strategies in a hybrid algorithm.

The Hamiltonian Monte Carlo algorithm uses two general steps at time $t$:
\begin{bayeslist}
    \item   generate, independent of the current $\vT_t$, the momentum $\p_t$ from the multivariate normal distribution implied 
            by $K(\p_t) = \p_t'\SI^{-1}\p_t$ with mean vector zero and variance-covariance matrix $\sigma^2\I$ (or some other
            desired symmetric and positive-definite form).
    \item   run the leapfrog method $M$ times with $\upsilon$ steps to produce the candidate $(\tilde{\vT},\tilde{\p})$.
    \item   accept this new location or accept the current location as the $t+1$ step with a standard Metropolis decision using
            the $H$ function:
            \begin{equation}
                \min\left[1, \exp(-H((\tilde{\vT},\tilde{\p})) + H(\vT,\p) \right].
            \end{equation}
\end{bayeslist}
While this process looks simple, there are several complications to consider. We must be able to take the partial derivatives of
the log-posterior distribution, which might be hard. Also the chosen values of the leapfrog parameters, $M$ and $\upsilon$ are
critical. If $\upsilon$ is too small then exploration of the posterior density will be very gradual with small steps, and if
$\upsilon$ is too big then many candidates will be rejected. Choosing $M$ is important because this parameter allows the
Hamiltonian process to explore strategically with respect to gradients. Excessively large values of $M$ increase compute time, 
but excessively small values of $M$ also lead to many rejected candidates. In both cases where the parameters are too small we
lose the advantages of the gradient calculations and produce an inefficient random walk.  Finally, $\sigma^2$ affects efficiency 
of the algorithm in the conventional sense of appropriating tuning the variance of the multivariate normal for the momentum.
These can be difficult parameter decisions and Neal (2011) gives specific guidance on trial runs and analysis of the results.
\index{authorindex}{Neal, R. M.}
\index{subjectindex}{Hamiltonian Monte Carlo|)}

\begin{examplelist}
    \item   {\bf An Illustrative Simple Simulated Hamiltonian Setup.}\index{subjectindex}{Hamiltonian!example} This is a
            basic illustration of Hamiltonian Monte Carlo applied to the same target distribution as the Hit-and-Run sampler in
            Example~\ref{hit.run.example} (page~\pageref{hit.run.example}).  The target distribution is a bivariate normal PDF
            with a correlation of 0.95. Similar to the data structures in the Hit-and-Run code and the basic Metropolis-Hastings
            code in Example~\ref{metropolis.normal.example.code} (starting on page~\pageref{metropolis.normal.example.code}), a 
            matrix is created with all \texttt{NA} values to fill in. Here there are four stored values on each row: $x$, $y$,
            $\vartheta_1$, and $\vartheta_2$. Consider the following \R\ code: 

            \begin{R.Code}
ham.norm <- function(theta.matrix,reps,I.mat,upsilon,M,Sigma)  \{
    for (i in 2:reps)  \{
        vartheta <- theta.matrix[(i-1),3:4]
        p <- rnorm(length(vartheta),0,1)
        p.half <- p - (upsilon/2)*vartheta
        for (j in 1:M)  \{
            vartheta <- vartheta + upsilon*p.half
            p.half <- p.half - upsilon*vartheta
        \}
        p.full <- -(p.half + (upsilon/2)*vartheta)
        new.U <- t(vartheta) %*% solve(I.mat) %*% vartheta/2
        new.K <- t(p.full) %*% solve(Sigma) %*% p.full
        old.U <- t(theta.matrix[(i-1),1:2]) %*% solve(I.mat) 
                    %*% theta.matrix[(i-1),1:2]/2
        old.K <- t(-p) %*% solve(Sigma) %*% -p
        a <- exp(old.U - new.U - new.K + old.K)
        if (a > runif(1)) theta.matrix[i,1:2] <- vartheta
        else theta.matrix[i,1:2] <- theta.matrix[(i-1),1:2]
        theta.matrix[i,3:4] <- vartheta
    \}
    theta.matrix
\}
            \end{R.Code}
            This code is also provided in the \texttt{BaM} package in \R. % NEED TO ADD TO BaM
            The following lines code give the setup and the function call for 10,000 iterations. The last 5,000 iterations are
            displayed in Figure~\ref{hamiltonian.travel.figure}. Notice the values $\upsilon = 0.001$ and $M = 1000$. These were
            set up by trial-and-error, and even a model as simple as this requires some tuning of these parameters.
            \begin{R.Code}
num.sims <- 10000
Sig.mat <- matrix(c(1.0,0.95,0.95,1.0),2,2)
upsilon.in <- 0.001
M.in <- 1000
Sigma.in <- matrix(c(3,0,0,3),2,2)
walks<-rbind(c(-3,-3,1,1),matrix(NA,nrow=(num.sims-1),ncol=4))
walks <- ham.norm(walks,num.sims,Sig.mat,upsilon.in,M.in,Sigma.in)
            \end{R.Code}
            This is intended to be an illustrative example only since a bivariate normal does not require estimation with MCMC.
            For a fully developed package that implements Hamiltonian Monte Carlo see the \texttt{Stan} package developed by
            Andrew Gelman and his colleagues at \texttt{http://mc-stan.org}.  

\begin{center}
\begin{figure}[h!]
\parbox[c]{\linewidth}{ \hspace{0.4in}
    \epsfig{file=Images/mcmc_advanced.figure04.ps,clip=,angle=270,width=4.65in}
}
\vspace{11pt}
\caption{\textsc{Samples from Hamiltonian Monte Carlo}} \label{hamiltonian.travel.figure}
\end{figure}
\end{center}
\end{examplelist}

\section{Exercises}
\begin{exercises}
	\item   Plot in the same graph a time sequence of 0 to 100 versus the annealing cooling schedules: logarithmic, geometric, 
            semi-quadratic, and linear.  What are the trade-offs associated with each with regard to convergence and processing 
            time?

            % NEW
    \item   Plot a bivariate normal distribution at 4 different temperature levels with enough variation that the subplots are
            distinct.

	\item   Replicate the contrived simulated annealing example from Section~\ref{annealing.section} using each of the cooling 
            schedules from Exercise~1, checking for convergence before moving on to the next temperature.  What 
            differences do you observe?

            % NEW
    \item   Using the HMO data in Example~\ref{example:florida.hmo} on page~\pageref{example:florida.hmo} (available in the
            \texttt{BaM} package) implement a Metropolis-coupling algorithm (MCMCMC) at 10 different temperature levels.  The
            lowest temperature defines the regular joint posterior and the highest temperature defines a joint uniform density.
            Run this in \R\ to produce a posterior estimate from the cold chain values to compare with the \bugs\ code in
            Chapter~\ref{Software.Chapter}.  \index{subjectindex}{Metropolis-Coupling}

    \item 	\label{witch's.hat.exercise} The so-called \emph{witch's hat}\index{subjectindex}{witch's hat distribution}
			 distribution is given this name because it looks like a conical spike in the middle of a wide flat plane.  This 
             distribution can be disastrous for the Gibbs sampler because all but one coordinate must be lined up with the
			 peak before a subchain step can move to the peak and the probability that this happens reduces exponentially with 
             increasing dimensions.  

			 Matthews\index{authorindex}{Matthews, P.} (1993) gives the following multivariate form based on a normal/uniform 
             mixture, defined over the $d$-dimensional unit cube:
			 \begin{equation}
		    		     p(\theta|\x) = (1-\delta)[2\pi\sigma^2]^{-d/2}
						    \exp\left[ -\sum_{i=1}^d \frac{1}{2\sigma^2}
							(x_i - \theta_i)^2 \right]
						    +\delta I_{(0,1)}(x_i),	\nonumber
			 \end{equation}
			 where $I_{(0,1)}(x_i)$ is an indicator function equal to one when $x_i$ is in the interval $(0,1)$ and zero otherwise.
			 Cui\index{authorindex}{Cui, L.} \etal (1992) sample this distribution setting: $\delta=10^{-11}$, $\sigma^2 = 0.0009$, 
             and the nine-dimensional peak at $(0.9,0.9,0.9,0.9,0.9,0.9,0.9,0.9,0.9)$.  Run a Gibbs sampler for this problem starting 
             at 10 different points in the unit cube, and test convergence with the Gelman\index{authorindex}{Gelman, A.} and 
			 Rubin\index{authorindex}{Rubin, D. B.} diagnostic.  What do you conclude about convergence?
				 \index{subjectindex}{convergence diagnostic!Gelman and Rubin}

            % NEW
    \item   Using again the witch's hat distribution from the last exercise, write a Metropolis-Hastings variant that fully
            explores the ``brim'' of the hat efficiently (in reasonable time).\index{subjectindex}{witch's hat distribution}

    \item   (Bohachevsky,\index{authorindex}{Bohachevsky, I. O.} Johnson,\index{authorindex}{Johnson, M. E.} and
            Stein\index{authorindex}{Stein, M. L.} 1986).  Use simulated annealing to find the global minima of the function 
            \index{subjectindex}{simulated annealing}
            \begin{equation}
                f(x,y) = x^2 + 2y^2 - 0.3\cos(3\pi x) - 0.4\cos(4\pi y) + 0.7, \nonumber
            \end{equation}
            on the support $-1 \le x,y \le 1$.  Using the same function from Bohachevsky,\index{authorindex}{Bohachevsky, I. O.}
            Johnson,\index{authorindex}{Johnson, M. E.} and Stein\index{authorindex}{Stein, M. L.} (1986), write a simple 
            Metropolis-Hastings algorithm in \R\ to explore the sample space.  Show the problems associated with this approach by
            graphing the acceptance rate over time.

            % NEW
    \item   {\bf Simulated Tunneling.}
            \index{subjectindex}{simulated tunneling}\index{authorindex}{Wenzel, W.}\index{authorindex}{Hamacher, K.}
            Wenzel and Hamacher (1999) design a stochastic search related to simulated annealing that randomly ``hops'' from point
            to point to accommodate efficient search of complex multimodal functions without being trapped for long periods of
            time at local minima.  Replace the target function $f(x)$ with $f_{\text{STUN}} = 1 - \exp[-\zeta(f(x) - f_{\min})]$,
            where $f_{\min}$ is the lowest minimum yet encountered and $\zeta >0$ is a tunable ``tunneling parameter'' that
            regulates the steepness of descents.  Write and run a Metropolis-Hastings implementation using the same function as
            the last exercise.  

    \item 	One of the difficulties with slice sampling\index{subjectindex}{slice sampler} is that in high dimensions, the Markov 
            chain may be restricted to a set of unusual subspaces of the parameter space: non-linear and varying dramatically in size.
			This can obviously lead to poor convergence and mixing properties.  Develop a simulated annealing algorithm that 
            alleviates this problem in at least one application.

            % NEW
    \item   Consider the surface (3D function) created by the following \R\ code:
            \begin{R.Code}
f.xyz <- function(x, y)  \{
     0.75*exp( -((10*x-1)^2 + (10*y-1)^2)/5 ) +
     0.50*exp( -((10*x-7)^2 + (10*y-5)^2)/5 ) -
     0.25*exp( -((10*x-4)^2 + (10*y-7)^2)/5 )
\}
set.seed(pi);   n <- 15;
x2 <- x1 <- seq(0,1,length=n)
y <- outer(x1, x2, f.xyz) 
y <- y + rnorm(n^2,0,0.05*max(abs(y)))
            \end{R.Code}
            Write a simulated tunneling application for this surface that accommodates maxima instead of minima.  Graph the
            function and 200 post-convergence visits on the graph.

	\item	It is possible that not all full conditional probability statements can be identified to set up a Gibbs sampler.
		    One way to solve this problem is to embed a Metropolis-Hastings algorithm within the Gibbs sampler, called 
            ``Metropolis-within-Gibbs,'' \index{subjectindex}{Metropolis-within-Gibbs} for the parameter or parameters that 
            are causing the difficulty.  Write out the full statement of this algorithm and derive the detailed balance equation.  
            \index{subjectindex}{detailed balance equation}

            % NEW
    \item   Using the terrorism in Great Britain example (Example~\ref{tweed.example}, starting on page~\pageref{tweed.example})
            implement a Metropolis-with-Gibbs example where $\lambda$ and $\phi$ get a Gibbs draw as done before but $k$ is
            sampled using Metropolis-Hastings.
            \index{subjectindex}{example!terrorist events in Britain}\index{subjectindex}{changepoint problem}

	\item	In his original article Green (1995)\index{authorindex}{Green, P. J.} analyzes the well-known coal mining data used in 
		    many papers and texts, except that he compares models with one, two, and three possible 
            changepoints using reversible jump MCMC and uses days instead of years.  Green's analysis supports the notion of two 
            changepoints (his Figure~2).  Replicate that analysis using \R\ and make a decision about which model should eventually 
            be preferred with a Bayes Factor calculation.  \index{subjectindex}{Markov chain Monte Carlo (MCMC)!reversible jump}
            This is the data vector:
            \begin{R.Code}
coal.mining.disasters <- c(4,5,4,0,1,4,3,4,0,6,3,3,4,0,2,6,
                           3,3,5,4,5,3,1,4,4,1,5,5,3,4,2,5,
                           2,2,3,4,2,1,3,2,2,1,1,1,1,3,0,0,
                           1,0,1,1,0,0,3,1,0,3,2,2,0,1,1,1,
                           0,1,0,1,0,0,0,2,1,0,0,0,1,1,0,2,
                           3,3,1,1,2,1,1,1,1,2,4,2,0,0,1,4,
                           0,0,0,1,0,0,0,0,0,1,0,0,1,0,1)
            \end{R.Code}
            (also included in the \texttt{BaM} package).

            % NEW
    \item   Using the death penalty data in Example~\ref{death.penalty.example} on page~\pageref{death.penalty.example}, write a
            RJMCMC procedure in \R\ for the Tobit model.
            \index{subjectindex}{Bayesian Tobit model}\index{subjectindex}{death penalty}

	\item	Show that the acceptance criteria given in \eqref{rjmcmc.acceptance} satisfies the detailed balance equation.
		    \index{subjectindex}{Markov chain Monte Carlo (MCMC)!reversible jump}
		    \index{subjectindex}{detailed balance equation}

            % NEW
    \item   {\bf Umbrella Sampling.} \index{subjectindex}{umbrella sampling} Torrie and Valleau (1977) 
            \index{authorindex}{Torrie, G. M.}\index{authorindex}{Valleau, R.  L.} suggested a generalization of importance
            sampling,\index{subjectindex}{importance sampling}  solidified by Geyer (2011).\index{authorindex}{Geyer, C. J.}
            Here the approximation distribution is replaced with a mixture distribution: $g(\theta) \equiv \sum_{j=1}^J
            g(\theta_j)c_j$ for a $J$-component mixture where $c_j$ is a set of normalizing weights (using the language of
            Section~\ref{SIR.Section} starting on page~\pageref{SIR.Section}, rather than that of Geyer).  This makes the
            importance weight $\omega_i = f(\theta_i)/\sum_{j=1}^J g(\theta_j)c_j$.  The choice of mixture distribution is
            problem-dependent and must be made carefully.  Apply this sampler to the function given in \eqref{ugly.function}.

	\item	\label{cftp.exercise} Using \R\ run a coupling from the past algorithm with the following transition matrix:
		    \begin{equation*}
			    \K = \left[ \begin{array}{rr} 0.9 & 0.1 \\ 0.5 & 0.5 \\ \end{array} \right].
		    \end{equation*}
		    Print out the reached values as part of the code, indicate when coalescence occurs, and determine stationary distribution.
            \index{subjectindex}{coupling from the past}

            % NEW
    \item   A state space has four ordered states, $\{A,B,C,D\}$, where $D$ wraps around back to $A$.  For any given state the
            probability of moving ``forward'' is $2/3$ and the probability of moving ``backward'' is $1/3$.  Implement a CFTP
            algorithm in \R\ to obtain the stationary distribution.

	\item	Rerun the CFTP algorithm from Exercise~\ref{MCMC.Extensions.Chapter}.\ref{cftp.exercise} but instead of re-using 
		    the uniform draws, generate new ones at each step.  What difference do you see?  Why did this go wrong?
		    \index{subjectindex}{coupling from the past}

            % NEW
    \item   The Hoff (2005, 2009) \index{authorindex}{Hoff, P.}social networking model starts with the $n \times n$ symmetric
            matrix $\Y$ of links between $n$ individuals, where $y_{ij}=1$ indicates a known link between node $i$ and node $j$,
            and $y_{ij}=0$ indicates the absence of evidence of a link.  The $n \times n \times K$ array $\X$ defines for each
            $n\times n$ relationship between individual $i$ and individual $j$ with a $K$-length vector of covariate information.
            Relate $\X$ and $\Y$ with the random effects logistic regression specification:
            \begin{equation*}
                p(\Y|\T_{ij}) = \prod_{i \ne j} \frac{ \exp(\T_{ij}) }{ 1 + \exp(\T_{ij}) }  \qquad
                \T_{ij}       = \B^{\prime}\x_{ij} + z_{ij}                      \qquad
                z_{ij}        = \vu_i^{\prime}\G\vv_j + \epsilon_{ij}
            \end{equation*}
            where $\B$ is a $K$-length vector of coefficients to estimate, and $z_{ij}$ is a random effects term to account for 
            dependencies between attribute relationships. The $z_{ij}$ term has two components: a $\vu_i^{\prime}$
            vector of sender-specific latent factors, a $\vv_j$ vector of receiver-specific latent factors, a $\G$ diagonal matrix
            of unknown coefficients, plus a $\epsilon_{ij}$ scalar error specific to the $ij$ edge.  Assign priors and write a
            Metropolis-Hastings variant using the Roethlisberger and Dickson (1939) Hawthorne Plant wiring room dataset in the \R\ 
            package \texttt{BaM}. \index{authorindex}{Roethlisberger, F.}\index{authorindex}{Dickson, W.}
\end{exercises}

\thispagestyle{empty}
\chapter{The Bayesian Linear Model}\label{Linear.Chapter}
\setcounter{examplecounter}{1}

\section{The Basic Regression Model}\label{linear.regression.model.section}
\index{subjectindex}{Bayesian linear model}
This chapter develops the Bayesian linear regression model with differing priors and assumptions.  We will 
consider both informed and uninformed prior specifications as well as look at the common problem of 
heteroscedasticity.  Detailed technical expositions of the Bayesian linear regression model are found in the 
classic article by Lindley and Smith (1972) with discussion, the follow-up article with generalizations by Smith 
	\index{authorindex}{Lindley, D. V.}%
	\index{authorindex}{Smith, A. F. M.}%
	\index{authorindex}{Geweke, J.}%
	\index{authorindex}{Tiao, G. C.}%
	\index{authorindex}{Zellner, A.}%
	\index{authorindex}{Davis, W. W.}%
	\index{authorindex}{Pollard, W. E.}%
(1973), Geweke's (1993) exposition on t-distributed errors, and the early work by Tiao and Zellner (1964b).  
Elsewhere Zellner and Tiao (1964) provide estimation techniques for general error models, Davis (1978) considers 
the Bayesian general linear model with inequality constraints, and Pollard (1986) gives a helpful chapter on 
Bayesian linear forms.  A modern and very comprehensive volume on linear model theory is given by Ravishanker
and Dey (2002).                  
\index{authorindex}{Ravishanker, N.}
\index{authorindex}{Dey, D. K.}
%\cite{Taagepera1960}

The first treatment presented here assumes homoscedasticity.  A discussion of unequal variances for the Bayesian linear 
model follows, and additional discussions can be found in Leonard (1975) and Boscardin and Gelman (1996).
	\index{subjectindex}{homoscedasticity!linear model}
	\index{authorindex}{Leonard, T.}%
	\index{authorindex}{Boscardin, J. W.}%
	\index{authorindex}{Gelman, A.}%
Le Cam (1986, Chapter~13) gives a detailed asymptotic analysis, and more elementary treatments of 
	\index{authorindex}{Le Cam, L.}%
	\index{subjectindex}{heteroscedasticity!linear model}%
heteroscedastic linear models can be found in Goldberger (1964, p.235), Huang (1970, p.147), and Rao and Toutenburg 
(1995, p.101).  A detailed theoretical discussion of linear model 
	\index{authorindex}{Goldberger, A. S.}%
	\index{authorindex}{Huang, D. S.}%
	\index{authorindex}{Rao, C. R.}%
	\index{authorindex}{Toutenburg, H.}%
heteroscedasticity is given in Amemiya (1985, Section~6.5), and the corresponding application of the jackknife and 
bootstrap are outlined in Shao and Tu (1995, Chapter~7).  See also Fomby, Hill, and Johnson (1980).
        \index{authorindex}{Fomby, T. B.}%
        \index{authorindex}{Hill, R. C.}%
        \index{authorindex}{Johnson, S. R.}%
	\index{authorindex}{Shao, J.}%
	\index{authorindex}{Tu, D.}%

Start with the well-known basic multiple linear regression model, described in Appendix~\ref{GLM.Chapter}, conforming to the
Gauss-Markov assumptions.  Define the terms conventionally: \index{subjectindex}{linear model!regression}
\begin{equation}
	\y = \X\B + \EP, 
\end{equation}
where $\X$ is an $n \times k$, rank $k$ matrix of explanatory variables with a leading vector of ones for the constant, $\B$ is a
$k \times 1$ vector of coefficients to be estimated, $\y$ is an $n \times 1$ vector of outcome variable values, and $\EP$ is a $n
\times 1$ vector of errors distributed $\mathcal{N}(0,\sigma^2 I)$ for a constant $\sigma^2$.  The likelihood function for a
sample of size $n$ is:
\begin{equation}\label{linear.likelihood.equation}
    L(\B,\sigma^2|\X,\y) = (2\pi\sigma^2)^{-\frac{n}{2}}
    \exp\left[
        -\frac{1}{2\sigma^2}(\y-\X\B)'(\y-\X\B)
    \right].
\end{equation}
So far we have the standard non-Bayesian approach to linear modeling, and once the data, $[\X,\y]$, are observed, this 
likelihood function (or rather its log) is maximized relative to the unknown parameter vector $\B$ and the unknown 
scalar $\sigma$.  We know the values for which (\ref{linear.likelihood.equation}) is at its maximum from standard 
likelihood theory (bias corrected for $\sigma^2$) and ordinary least squares principles:
\begin{equation}\label{basic.linear.estimates}
    \bh = (\X'\X)^{-1}\X'\y,
    \qquad \qquad
    \hat{\sigma}^2 = \frac{ (\y-\X\bh)'(\y-\X\bh) }{ (n-k) },
\end{equation}
and we can therefore plug these values into (\ref{linear.likelihood.equation}) and process according to:
\begin{align}\label{linear.likelihood.max}
    L(\B,&\mathbf{\sigma}^2|\X,\y)\propto \sigma^{-n} \exp\left[ -\frac{1}{2\sigma^2}
        (\y'\y - 2\B'\X'\y + \B'\X'\X\B) \right]       \nonumber \\
    &=       \sigma^{-n} \exp\biggl[ -\frac{1}{2\sigma^2}
         (\y'\y - 2\B'\X'\y + \B'\X'\X\B \nonumber\\
    &\qquad    \underbrace{-2((\X'\X)^{-1}\X'\y)'\X'\y +2((\X'\X)^{-1}\X'\y)'\X'\X((\X'\X)^{-1}\X'\y))}_{\text{
            sums to zero}} \biggr]       \nonumber \\
    &= \sigma^{-n} \exp\Biggl[ -\frac{1}{2\sigma^2}((\y-\X\bh)'(\y-\X\bh) \nonumber \\
    &\qquad\qquad\qquad\qquad\qquad    + \bh'\X'\X\bh + \B'\X'\X\B - 2\B'\X'\X\bh) \Biggr]    \nonumber \\
    &= \sigma^{-n} \exp\left[ -\frac{1}{2\sigma^2}
        (\hat{\sigma}^2(n-k) + (\B-\bh)'\X'\X(\B-\bh)) \right].
\end{align}
The ``trick'' used here is completing the square after inserting a quantity that is simultaneously subtracted
and added (therefore adding zero), and using the property $\X'\X(\X'\X)^{-1} = I$ to rearrange terms.  The
other mildly subtle point is the replacement of $-2\B'\X'\y$ with $- 2\B'\X'\X\bh$ from the normal equation
($\X'\X\bh = \X'\y$).

Thus the result is a likelihood function expressed in terms of unknown coefficients to be estimated and observable 
data (Zellner\index{authorindex}{Zellner, A.} 1976, p.401).  This is the starting point for the Bayesian analysis of the standard linear
model and we now begin to quantify existing knowledge about the mean vector and the variance parameter (a
scalar due to the homoscedasticity assumption).\index{subjectindex}{homoscedasticity}  The Bayesian complement to maximum 
likelihood estimation not
only provides for prior information, but also easily incorporates: linear additive specifications of
nonlinear functions (Bernardo and Smith 1994, pp.221-222; Leonard and Hsu 1999, Section~5.2), linear inequality
	\index{authorindex}{Bernardo, J. M.}%
	\index{authorindex}{Smith, A. F. M.}%
	\index{authorindex}{Leonard, T.}%
	\index{authorindex}{Hsu, J. S. J.}%
	\index{authorindex}{Davis, W. W.}%
	\index{authorindex}{Halpern, E. F.}%
	\index{authorindex}{Hartigan, J. A.}%
	\index{authorindex}{Lempers, F. B.}%
	\index{authorindex}{Datta, G. S.}%
	\index{authorindex}{Ghosh, M.}%
	\index{authorindex}{Ferreira, M. A. R.}%
	\index{authorindex}{Gamerman, D.}%
	\index{authorindex}{Kitagawa, G.}%
	\index{authorindex}{Gersch, W.}%
	\index{authorindex}{Mar\'{\i}n, J. M.}%
	\index{authorindex}{Pole, A.}%
	\index{authorindex}{West, M.}%
	\index{authorindex}{Harrison, J.}%
constraints (Davis 1978), analysis of variance of means (Hartigan 1983, Chapter~9), polynomial regression
(Halpern 1973; Lempers 1971), small area estimation (Datta and Ghosh 1991), and time-series models (Ferreira
and Gamerman 2000; Kitagawa and Gersch 1996; Mar\'{\i}n 2000; Pole, West, and Harrison 1994), among other 
extensions.

\subsection{Uninformative Priors for the Linear Model}\label{uninformed.prior.linear.regression}
Define the improper uninformed priors $p(\B) \propto c$ and $p(\sigma^{2}) = \frac{1}{\sigma}$ over the support $[-\infty\range
\infty]$ and $[0\range \infty]$, respectively (Tiao and Zellner 1964a, p.220).  
	\index{authorindex}{Tiao, G. C.}%
	\index{authorindex}{Zellner, A.}%
	\index{subjectindex}{Bayesian linear model!uninformative priors}%
Note that we are assuming independence between $\B$ and $\sigma^2$.
Therefore the joint posterior from the likelihood function (\ref{linear.likelihood.max}) is provided by:
\begin{align}\label{joint.posterior.uninformed.linear}
    \pi(\B,\sigma^2&|\X,\y)  \propto L(\B,\sigma^2|\X,\y)p(\B)p(\sigma^{2}) 	\nonumber \9
    			   &\propto \sigma^{-n-1} \exp\left[ -\frac{1}{2\sigma^2}
        			(\hat{\sigma}^2(n-k) + (\B-\bh)'\X'\X(\B-\bh)) \right].
\end{align}
Note that the constant $c$ drops out with proportionality.

To obtain the desired marginal distribution of $\B$, first make the transformation: $s=\sigma^{-2}$, with a required
Jacobian: $J=|\frac{d}{d s}\sigma| = |\frac{d}{d s}s^{-\frac{1}{2}}| =
\frac{1}{2}s^{-\frac{3}{2}}$. Reexpressing (\ref{joint.posterior.uninformed.linear}) in terms of $s$ gives:
\begin{align}\label{joint.posterior.uninformed.linear.s}
    \pi(\B,s|&\X,\y)    \nonumber \\
        &\propto (s^{-\frac{1}{2}})^{-n-1} \exp\left[ -\frac{1}{2}s
             (\hat{\sigma}^2(n-k) + (\B-\bh)'\X'\X(\B-\bh)) \right]
             \left( \frac{1}{2}s^{-\frac{3}{2}} \right)     \nonumber \\
        &\propto s^{\frac{n}{2}-1} \exp\left[ -\frac{1}{2}s
             (\hat{\sigma}^2(n-k) + (\B-\bh)'\X'\X(\B-\bh)) \right].
\end{align}
Now integrate with respect to $s$ to get the marginal for $\B$:
\begin{equation}
    \pi(\B|\X,\y) = \int_0^\infty s^{\frac{n}{2}-1} \exp\left[ -\frac{1}{2}s
                         (\hat{\sigma}^2(n-k) + (\B-\bh)'\X'\X(\B-\bh)) \right] ds.
        \nonumber
\end{equation}
This integral is quite easy to calculate when one notices that inside the integral is a
gamma PDF kernel, with the integration performed over the appropriate support.  So use the
following substitution from a gamma PDF:
\begin{equation}
    1 = \int_0^\infty \frac{q^{p+1}}{\Gamma(p+1)}s^p e^{-qs} ds,
    \qquad \qquad
    \frac{\Gamma(p+1)}{q^{p+1}} = \int_0^\infty s^p e^{-qs} ds.
    \nonumber
\end{equation}
Setting $p=\frac{n}{2}-1$ and $q=\frac{1}{2} (\hat{\sigma}^2(n-k) + (\B-\bh)'\X'\X(\B-\bh))$, and defining the
degrees of freedom as $\nu = n-k$ means that:
\begin{equation}\label{gamma.trick}
    \pi(\B|p,q) \propto q^{-(p+1)} = q^{-\frac{n}{2}},
\end{equation}
which can then be resubstituted back to get:
\begin{equation}\label{beta.posterior.linear.uninformed}
    \pi(\B|\X,\y) \propto \left[ (n-k)+(\B-\bh)'\hat{\sigma}^{-2}\X'\X(\B-\bh) \right]^{-\frac{n}{2}}.
\end{equation}
It is easy to recognize $\pi(\B|\X,\y)$ as the kernel of a multivariate-$t$ distribution for $\B-\bh$,
provided that the covariance matrix, $\mathbf{R} = \frac{ (n-k)\hat{\sigma}^{2}(\X'\X)^{-1} }{ n-k-2 }$,
is positive definite (Box and Tiao 1973, p.440; Press 1989, p.135; Tong 1990, Chapter 9).  Thus
	\index{authorindex}{Box, G. E. P.}%
	\index{authorindex}{Tiao, G. C.}%
	\index{authorindex}{Press, S. J.}%
	\index{authorindex}{Tong, Y. L.}%
	\index{subjectindex}{distribution!multivariate-$t$}%
$E[\bh] = \B$, and the covariance between any two
coefficients is given by the elements of the $\mathbf{R}$ matrix: $\Cov[t_i,t_j]=R_{ij}$,
where $\hat{\sigma}^{2}$ is defined by \eqref{basic.linear.estimates}.

Obtaining the marginal distribution for $\sigma^2$ is considerably less involved due to an
obvious shortcut.  Start with the defining integral and separate terms in the exponent:
\begin{align}
    \pi(\sigma|\X,\y)
        &\propto \int_{-\infty}^{\infty} \sigma^{-n-1} \exp\left[ -\frac{1}{2\sigma^2}
             (\hat{\sigma}^2(n-k) + (\B-\bh)'\X'\X(\B-\bh)) \right]d\B.
                \nonumber \\
        &= \sigma^{-n-1}       \exp\left[ -\frac{1}{2\sigma^2}\hat{\sigma}^2(n-k) \right]
		\nonumber\\
        &\qquad\qquad\qquad\qquad\quad  \times 
		\int_{-\infty}^{\infty} \exp\left[ -\frac{1}{2\sigma^2}(\B-\bh)'\X'\X(\B-\bh) \right]d\B.
                \nonumber
\end{align}
The second exponential term is a k-dimensional kernel of a multivariate normal distribution providing the
following substitution and simplification:\index{subjectindex}{distribution!multivariate normal}
\begin{align}
     \pi(\sigma|\X,\y)
                &\propto \sigma^{-n-1} \exp\left[ -\frac{1}{2\sigma^2} \hat{\sigma}^2(n-k) \right]
             (2\pi\sigma^2)^{\frac{k}{2}}
             \nonumber \\
                &\propto (\sigma^2)^{-\frac{1}{2}(n-k)-\frac{1}{2}}
                \exp\left[ -\frac{1}{2\sigma^2} \hat{\sigma}^2(n-k) \right].
            \label{sigma.uninformed.posterior.distribution}
\end{align}
It is not immediately obvious, but the posterior distribution of $\sigma$ is the kernel of an inverse-gamma distribution
(Appendix~\ref{distribution.appendix}), and can also be parameterized as an inverse Wishart distribution (Tiao and Zellner 1964b).
\index{authorindex}{Zellner, A.}\index{authorindex}{Tiao, G. C.} To see how this is really an inverse gamma form, apply the
simple transformations: $\alpha=\frac{1}{2}(n-k-1)$, $\beta=\frac{1}{2}\hat{\sigma}^2(n-k)$, so that
\ref{sigma.uninformed.posterior.distribution} is now:
\begin{equation}\label{gamma.transform.result}
    \pi(\sigma^2|\alpha,\beta) \propto (\sigma^2)^{-(\alpha+1)}\exp[-\beta/\sigma^2].
\end{equation}
So far \eqref{gamma.transform.result} is exactly the same inferential result that we would expect from a standard likelihood
analysis of the linear model The maximum likelihood solution, which is also equivalent to minimizing the summed squared errors, is
equivalent to a Bayesian solution in which improper uniform priors over the entire support of the unknown parameters are
specified.  Thus, the omnipresent non-Bayesian approach to linear modeling is a special case of a Bayesian model.

\begin{examplelist}
    \item {\bf The 2000 U.S. Election in Palm Beach County.}\label{pbc.example} \index{subjectindex}{example!2000 presidential election} 
    The 2000 U.S. election for president was marked by considerable controversy concerning the casting of ballots in the
    state of Florida.  Because the election was so tightly contested in Florida and the state's 25 electoral college delegates
    were the final determining factor for electing the president in the contest between Al Gore and George W.  Bush, various
    problems and aberrations in the voting process became magnified in importance.  There was considerable evidence that the final
    certified outcome declaring Bush the winner by 537 votes (out of approximately 6 million) was shaped by technical problems
    with the voting apparatus, ballot confusion by voters, and outright discrimination against minority voters.  

    At the nexus of this controversy is Palm Beach County, a liberal-leaning, upper-middle class \index{subjectindex}{Palm Beach County, FL} 
    area with a considerable number of northeastern retirees where the far-right conservative candidate Pat Buchanan
    did suspiciously well.  The data here (available in the \texttt{BaM} package associated with this text) consist of all 516
    reporting precincts in Palm Beach County collected by the \emph{Palm Beach Post} from state and federal reporting sources.
    The variables include party affiliation percentages, racial demographics, registration status, voting technology, and the
    outcome variable of interest: the number of spoiled ballots.  Ballots are spoiled if the voter designates more than one
    presidential selection or marks the ballot in some other inappropriate way.  Therefore this variable does not count so-called
    under-votes wherein the voter does not select any presidential candidate.  For the purpose of this example, the variables
    described in Table~\ref{PBC.Variable.Table} are included in the linear specification.
	
	\begin{table}[h]
	\centering
	\tabletitle{\textsc{PBC Data Variable Definitions}}\label{PBC.Variable.Table}
	\vspace{9pt}
	\begin{small} 
	\begin{tabular}{l|r}
			& \\[-8pt]
	{\texttt{Bad Ballots}}$\quad$ & $\quad${\bf Total Number of Spoiled Ballots}  \\
	\hline
			            & \\[-7pt]
	\texttt{Technology} & 0 for a datapunch machine (butterfly ballot), \\
            	        & 1 for votomatic \\
	\texttt{New}        & Number of ``new'' voters \\
            	        & (have not voted in the precinct for previous 6 years) \\
	\texttt{Size}       & Total number of precinct voters \\
	\texttt{Republican} & Number of voters registered as Republican \\
	\texttt{White}      & Number of white (nonminority) voters    
	\end{tabular} 
	\end{small} 
	\end{table}
	
    First we specify an uninformed joint prior, $p(\B,\sigma^2) = 1/\sigma$, and calculate the resulting posterior density
    according to Section~\ref{uninformed.prior.linear.regression}.  This joint prior comes from the simultaneous determination of
    the improper forms: $p(\B,\sigma^2) = p(\B|\sigma^2)p(\sigma^2)$.  The resulting posterior is summarized in
    Table~\ref{PBC.Uninformed.Table} where the posterior summaries, posterior moments, and quantiles for the coefficient estimates
    were produced directly from the analytical $t$-distribution result.  Note that the mean and median are identical for the
    coefficient posteriors indicating symmetry.

    Section~\ref{uninformed.prior.linear.regression} also demonstrated that the posterior distribution of $\sigma^2$ is an
    inverse-gamma specification as given by \eqref{sigma.uninformed.posterior.distribution}, and we can analytically determine the
    quantiles of interest with $\alpha=(n-k-1)/2$, $\beta=\frac{1}{2}\hat{\sigma}^2(n-k)$ inserted into the inverse-gamma PDF (see
    Appendix~\ref{distribution.appendix}).  However, it turns out to be much easier, and nearly equally accurate, to generate a
    large number of values of $\sigma^2$ and use the empirical quantiles.  The \R\index{subjectindex}{R@\R!simulation} code for
    all estimates in Table~\ref{PBC.Uninformed.Table} is included in the {\bf Computational Addendum } for this chapter.

    Some comments are necessary regarding the presentation of Table~\ref{PBC.Uninformed.Table}.  A strictly canonical Bayesian may
    object to the traditional non-Bayesian reporting of the mean and standard error in the left-hand side and want more than three
    quantile summaries on the right-hand side.  In a conventional linear regression table we would expect to see $t$-statistics,
    p-values, and ``stars'' instead of these quantiles.  Most Bayesians publishing in the social sciences see the usefulness of
    giving means and standard errors to comfort reviewers and readers who may not have had substantial exposure to Bayesian
    results.  But, these same researchers would certainly object to $t$-statistics since this implies a test that is not
    undertaken, to p-values since there is no null distribution with which to define them, and to ``stars'' since this is an
    idiotic practice.  Thus Table~\ref{PBC.Uninformed.Table} represents a compromise between accessibility and the Bayesian
    preference for purely distributional summaries.
	
	\begin{table}[h]
	\begin{center}
	\tabletitle{\textsc{Posterior: Linear Model, Uninformative Priors}}\label{PBC.Uninformed.Table}
	\vspace{9pt}
	\begin{small}
	\begin{tabular}{l|ccccc}
	             & \multicolumn{5}{c}{$\quad$}\\[-5pt]
	$\B$ Covariate$\;$ & Mean & Std. Error & 0.025 Q.    & Median    & 0.975 Q.  \\
	\hline
	             & \multicolumn{5}{c}{$\quad$}\\[-5pt]
	\texttt{Intercept}    &   107.351  &  7.759  &    ~92.108  &  107.351  &    122.593  \\
	\texttt{Technology}   &   -50.529  &  3.492  &    -57.389  &  -50.529  &    -43.670  \\
	\texttt{New}          &   ~-0.353  &  0.038  &    ~-0.427  &  ~-0.353  &    ~-0.278  \\
	\texttt{Size}         &   ~~0.149  &  0.006  &    ~~0.137  &  ~~0.149  &    ~~0.161  \\
	\texttt{Republican}   &   ~-0.084  &  0.007  &    ~-0.098  &  ~-0.084  &    ~-0.069  \\
	\texttt{White}        &   ~-0.048  &  0.006  &    ~-0.059  &  ~-0.048  &    ~-0.037  \\
	\hline
	             & \multicolumn{5}{c}{$\quad$}\\[-5pt]
	$\sigma$     &   ~22.003  &  0.698  &    ~20.695  &  ~21.983  &    ~23.434  
	\end{tabular}
	\end{small}
	\end{center}
	\end{table}
	
	What we see from the analysis in Table~\ref{PBC.Uninformed.Table} is that the linear model with
	an uninformed prior for $\B$ and $\sigma^2$ provides an apparently very good fit to the data.
	Each of the marginal posterior distributions is narrow and easily statistically distinguishable
	from zero.  Since this is still a linear model, the interpretation of these coefficients is very
	direct:\index{subjectindex}{Bayesian linear model!interpretation of coefficients}%
	
	\begin{footnotesize} \begin{bayeslist}
    	\item   Precincts with votomatic technology have an expected 50 less spoiled ballots.
	
    	\item   For every 2.5 new voters, there is an expected 1 less spoiled ballot.
	
    	\item   For each additional voter that turns out, there is an additional 0.15
            	expected increase in spoiled ballots: in other words, a turnout
            	addition of 100 provides an expected increase in spoiled ballots of 15.
            	This includes the new voters above; so in fact, this effect is
            	really greater than this coefficient indicates since the new voters
            	are actually suppressing this effect somewhat.
	
    	\item   For each additional Republican voter there is about a 9\% decrease in
            	expected spoiled ballots.  That is, we expect about 1 less
            	spoiled ballot for every 11 increased Republican voters.
	
    	\item   For each additional white voter, there is a 5\% decrease in expected
            	spoiled ballots.  That means about 1 less spoiled ballot
            	for every 20 increased white voters.
	\end{bayeslist} \end{footnotesize}
    
    It is also important to remember that while the reported mean and standard error can be identical between those produced with
    Bayesian methods and ordinary least squares (equivalently maximum likelihood here), they come from fundamentally different
    processes and assumptions (the OECD example on page~\pageref{OECD.Example} gives identical coefficient estimates even though
    one was produced with MCMC tools and the other with minimizing squared residuals).  A posterior mean is a point estimate that
    comes from an underlying distributional assumption about posterior quantities and the OLS point estimate comes from an
    assumption of a fixed underlying quantity set by nature.  Furthermore, a posterior standard error also arises from the belief
    that unknowns should be described with distributions, whereas a maximum likelihood estimate-based standard error comes from
    the curvature around the mode of the likelihood function under the assumption that the mode is the optimal estimator.
    Obviously these values basically agree in the presence of uninfluential prior distributions, or large samples, but they do not
    have to.  
\end{examplelist}

\subsection{Conjugate Priors for the Linear Model}\label{conjugate.prior.linear.regression}
\index{subjectindex}{Bayesian linear model!conjugate priors}%
We can also stipulate conjugate priors according to the principles outlined in Chapter~\ref{Normal.Model.Chapter}.  The linear 
regression model developed in this section is actually just a multivariate generalization of the normal model in that 
chapter, where the maximum likelihood estimates from the linear parametric likelihood specification are used.  The 
well-known conjugate prior distributions are: multivariate normal\index{subjectindex}{distribution!multivariate normal} 
for the mean vector $\B$ and inverse gamma for $\sigma^2$.  We can therefore proceed in essentially the same manner as in 
the basic normal model, but incorporating the likelihood function from the linear regression setup.

In this case of conjugacy we also have a dependency of $\B$ on $\sigma^2$ as in the simple normal-normal model in
Chapter~\ref{Normal.Model.Chapter}:
\begin{align}\label{linear.conjugate.priors}
    p(\B|\sigma^2) &= (2\pi)^{-\frac{k}{2}}|\SI|^{-\frac{1}{2}}
              \exp\left[ -\frac{1}{2} (\B - \mathbb{B})'\SI^{-1}(\B - \mathbb{B}) \right],
              \nonumber \\
               &\text{and:}     \nonumber \\
    p(\sigma^2)    &\propto \sigma^{-(a-k)}\exp\left[ -\frac{b}{\sigma^2} \right].
\end{align}
This affects the joint posterior as a simple joint prior since:
\begin{equation}
    p(\B,\sigma^2) = p(\B|\sigma^2)p(\sigma^2).
\end{equation}
However, the dependency in \eqref{linear.conjugate.priors} will flow through to the posterior of $\B$.
Here $\SI = \sigma^2 \mathbf{I}$ by assumption (this form is adaptable but not only makes the operations conformable, it allows us
to easily transition to the general linear model later).  Thus the prior for $\B$ conditional on $\sigma^2$ is a multivariate
normal \index{subjectindex}{distribution!multivariate normal} with mean $\mathbb{B}$ and variance $\sigma^2$.  The prior for
$\sigma^2$ is an inverse gamma kernel with parameters $a$ and $b$ (employing the form from page~\pageref{inverse.gamma.pdf.form},
see also Appendix~\ref{distribution.appendix}).  Using such notation, assign $\alpha-1=a-k$, and $\beta=b$ since these are free
prior parameters.  The inverse gamma prior for $\sigma^2$ is not only the conjugate prior, but also the marginal posterior from
the uninformed prior model.  So to summarize the notation, we now have:\label{zellner.regression.terms}
\begin{align*}
	n\times k	& \qquad \text{size of the $\X$ matrix}			\\
	\B   		& \qquad \text{the unknown linear model coefficient vector}	\\
	\mathbb{B}	& \qquad \text{prior mean vector for $\B$}			\\
	\sigma^2	& \qquad \text{prior variance for $\B$, collected in the diagonal matrix $\SI$}			\\
	\bh		& \qquad (\X'\X)^{-1}\X'\y					\\
    	\hat{\sigma}^2 	& \qquad \frac{ (\y-\X\bh)'(\y-\X\bh) }{ (n-k) }		\\
	a-k		& \qquad \text{assigned first parameter for $\sigma^2$ prior}	\\
	b  		& \qquad \text{assigned second parameter for $\sigma^2$ prior}.	
\end{align*}

Combining the data likelihood from (\ref{linear.likelihood.max}) with the prior specifications
from (\ref{linear.conjugate.priors}) and applying Bayes' Law gives the joint posterior:
\begin{align}\label{first.conjugate.posterior}
     \pi(\B,&\mathbf{\sigma}^2|\X,\y)        \nonumber\\
     		&\propto \sigma^{-n} \exp\left[ -\frac{1}{2\sigma^2} (\hat{\sigma}^2(n-k) + (\B-\bh)'\X'\X(\B-\bh)) \right] 	  \nonumber \\
    		&\times (2\pi)^{-\frac{k}{2}}|\SI|^{-\frac{1}{2}} \exp\biggl[ -\frac{1}{2}
           		(\B - \mathbb{B})'\SI^{-1}(\B - \mathbb{B}) \biggr] \sigma^{-(a-k)}\exp\left[ -\frac{b}{\sigma^2} \right] \nonumber \\
     	    &\propto \sigma^{-n-a} \exp\biggl[ -\frac{1}{2\sigma^2} \biggl( \hat{\sigma}^2(n-k) + (\B-\bh)'\X'\X(\B-\bh)  
     	        + 2b + (\B - \mathbb{B})'\SI^{-1}(\B - \mathbb{B}) \bigg) \bigg],	
\end{align}
where $\sigma^2$ moves out of $|\SI|$ from the determinant operation.
This form can be simplified by coweighting the precisions and re-expressing as a Gaussian kernel.  First
define (Zellner 1971, Chapter 3):\index{authorindex}{Zellner, A.}
\begin{align}
    \tilde{\B} &= (\SI^{-1} + \X'\X)^{-1}(\SI^{-1}\mathbb{B} +  \X'\X\bh)
            		\label{conjugate.substitution.for.btilde}\\
    \tilde{s}  &= 2b + \hat{\sigma}^2(n-k) + (\mathbb{B}-\tilde{\B})'\SI^{-1}\mathbb{B}
                        + (\bh - \tilde{\B})'\X'\X\bh.\label{conjugate.substitution.for.stilde}
\end{align}
The initial form of the joint posterior can now be re-expressed (Exercise~\ref{Linear.Chapter}.\ref{normal.derivation.exercise}) as:
\begin{equation}\label{second.conjugate.linear.posterior}
    \pi(\B,\mathbf{\sigma}^2|\X,\y) \propto
        (\sigma^2)^{-\frac{n+a}{2}}\exp\left[ -\frac{1}{2\sigma^2} \left( \tilde{s}
        + (\B-\tilde{\B})'(\SI^{-1}+\X'\X)(\B-\tilde{\B}) \right) \right].
\end{equation}
The advantage of this new form is that it immediately allows us to use the same marginalization
trick as we did with the posterior from the uninformed priors to get the distribution of $\B|\X,\y$.
This produces:
\begin{equation}\label{conjugate.posterior.for.beta.from.lm}
    \pi(\B|\X,\y) \propto \left[ \tilde{s} + (\B-\tilde{\B})'(\SI^{-1}+\X'\X)(\B-\tilde{\B})
                  \right]^{-\frac{n+a}{2}+1},
\end{equation}	
which is the kernel of a multivariate-$t$ distribution with $\nu = n+a-k-2$ degrees of freedom (Bauwens, 
Lubrano, and Richard 1999, Section~2.7, Zellner 1971, Section~3.2, Box and Tiao 1973, Section~8.4).  
	\index{subjectindex}{distribution!multivariate-$t$}%
	\index{authorindex}{Bauwens, L.}%
	\index{authorindex}{Lubrano, M.}%
	\index{authorindex}{Richard, J-F.}%
	\index{authorindex}{Box, G. E. P.}%
	\index{authorindex}{Tiao, G. C.}%
	\index{authorindex}{Zellner, A.}%
Therefore the mean and variance of the parameter estimates are given by:
\begin{equation}
    E(\B|\X,\y) = \tilde{\B},   \qquad\qquad
    \Cov(\B|\X,\y)= \frac{ \tilde{s}(\SI^{-1}+\X'\X)^{-1} }{ n+a-k-3 }.
\end{equation}
The marginal distribution of $\sigma^2$ is also produced in similar fashion to the uninformed prior derivation from before:
$\pi(\sigma^2|\X,\y) \propto \sigma^{-n-a+k-1}\exp\left[-\frac{1}{2\sigma^2}\hat{\sigma}^2(n+a-k)\right]$.  This is the kernel of
an $\mathcal{IG}(n+a-k,\frac{1}{2}\hat{\sigma}^2(n+a-k))$ distribution.  For details, see Note~B of Zellner
\index{authorindex}{Zellner, A.} (1971).

We can now compare the informed conjugate model with the uninformed model developed previously.  There are some interesting 
similarities as well as differences in Table~\ref{linear.model.comparison}.  Note first that the parametric form for the
marginal posteriors comes from the same family but with important differences in the parameters.  Specifically, conjugacy
provides $a$ in each of the possible places.  Since the researcher controls $a \in [0\range\infty)$, then there is 
temptation to say that this is manipulable to provide customary levels of statistical reliability.  This is most easily
seen in the posterior distribution of $\beta|\sigma^2$ where larger values of $a$ shrink the tails of the Student's-$t$ 
\index{subjectindex}{posterior distribution!Student's-$t$} toward that of the normal producing smaller posterior coefficient 
variance.  What prevents abuse of this term is the convention that authors overtly state their parameter values and the 
substantive reasons behind them.  \index{subjectindex}{Student's-$t$}

\begin{table}[h]
%\begin{center}
\begin{small}
\tabletitle{\textsc{Linear Regression Model, Prior Comparison}}\label{linear.model.comparison}
\vspace{0.07in}
\begin{tabular}{lll} 
					&		& \\[-6pt]
Setup                 			& Prior         & Posterior \\
\hline\\
{\bf \footnotesize Conjugate}         	& $\B|\sigma^2 \sim \mathcal{N}(\mathbb{B},\sigma^2)$          
					& $\B|\X \sim t(n+a-k-2)$ \9
                        		& $\sigma^2 \sim \mathcal{IG}(a,b)$     
					& $\sigma^2|\X \sim \mathcal{IG}(n+a-k,\frac{1}{2}\hat{\sigma}^2(n+a-k))$ \9
\hline\\
{\bf \footnotesize Uninformative}     	& $\B \propto c$ over $[-\infty\range \infty]$                 
					& $\B|\X \sim t(n-k)$ \9
                        		& $\sigma^{2} = \frac{1}{\sigma}$ over $[0\range \infty]$
                        		& $\sigma^2|\X \sim \mathcal{IG}(\frac{1}{2}(n-k-1),\frac{1}{2}\hat{\sigma}^2(n-k))$    \9
\end{tabular}
\end{small}
%\end{center}
\end{table}

The role of sample size is also highlighted in Table~\ref{linear.model.comparison}.  For the $t$-distribution posteriors, increases
in $n$ will obviously push this distribution towards the normal.  Since most models in the social sciences have a relatively
modest number of covariates (compared to statistical genetics or some fields in engineering), $k$ will have little effect for
large sample sizes in this transition.  The role of $n$ in the inverse gamma forms is also clear from the form of $p(\sigma^2)$ in
\eqref{linear.conjugate.priors} since both are negative exponents.  So asymptotically the conjugate and the uninformative linear
regression models both converge to conventional large sample results with normally distributed coefficients and fixed variance.

	\index{authorindex}{Berger, J. O.}
	\index{authorindex}{Diaconis, P.}
	\index{authorindex}{Ylvisaker, D.}
	\index{authorindex}{Ghosh, J. K.}
	\index{authorindex}{Goel, P. K.}
	\index{authorindex}{DeGroot, M. H.}
	\index{authorindex}{West, M.}
	\index{authorindex}{Box, G. E. P.}
	\index{authorindex}{Tiao, G. C.}
\subsection{Conjugate Caveats for the Cautious and Careful}
Some caveats are warranted about the use of conjugate priors in the Bayesian linear regression model as
described here.   Conjugate priors with exponential family distributions typically provide \emph{linear}
posterior expectations (Berger 1984, Diaconis and Ylvisaker 1979, Ghosh 1969, Goel and DeGroot 1980) which are highly
nonrobust to influential outliers (West 1984), although these outliers can be explicitly accounted for in a
Bayesian context (Box and Tiao 1968).  In the absence of credible normal assumptions about the prior,
due to small samples or less well-behaved error structures, linear expectations and linear variance estimates
(estimating the mean squared error) can be substituted (Hartigan\index{authorindex}{Hartigan, J. A.} 1969), but only by averaging over both
the data and the parameters.  Because the posterior distribution inherits its tail structure from the
prior in conjugate specifications, robustness is often difficult to incorporate (Anscombe 1963; Dickey
1974; Hill 1974; Lindley 1968; Rubin 1977).  Geweke (1993) shows that when the model residuals are shown to
be noticeably non-normal, more complex models are required (mixtures, distributional assignment
for the degrees of freedom parameter) and these sometimes require Bayesian stochastic simulation
techniques (see later chapters).
	\index{authorindex}{Anscombe, F. J.}
	\index{authorindex}{Dickey, J. M.}
	\index{authorindex}{Hill, B.}
	\index{authorindex}{Lindley, D. V.}
	\index{authorindex}{Rubin, D. B.}
        
The normal-normal conjugacy setup in Chapter~\ref{Normal.Model.Chapter} required that the prior and posterior for the mean be
conditional on the variance.  This was a mathematical requirement for the parameter form to flow through the likelihood function,
but it seems to water down the spirit of conjugacy.  For this reason it is sometimes called ``pseudo-conjugacy'' (or sometimes
``semi-conjugacy''), which is a term that some authors dislike.  This requirement on the mean also applied to the linear
regression models in this chapter, and was no weaker in this case.  Here, we have actually relaxed pure conjugacy even further
since the normal prior for the mean produces a Student's-$t$ posterior.  However, unless the sample size, $n$, and the set
parameter, $a$, are set relatively low compared to the number of covariates, $k$, the distribution is likely to approach normality
anyway.  \index{subjectindex}{posterior distribution!Student's-$t$}

\begin{examplelist}
	\item {\bf The 2000 U.S. Election in Palm Beach County, Continued.}\label{pbc.example.part.2}
	\index{subjectindex}{example!2000 presidential election}
	Now we can impose a conjugate prior as explained in Section~\ref{conjugate.prior.linear.regression}
	on these data and re-perform this analysis.  Suppose we now specify a ``pessimistic'' prior on the
	$\B$ coefficients with the following multivariate normal distribution and diffuse inverse-gamma
	distribution:\index{subjectindex}{distribution!multivariate normal}
	\begin{align}
    	\B \sim & \mathcal{N}(\mathbb{B},\SI)  \nonumber \\
       	& \text{where:}\qquad \mathbb{B} = [0,0,0,0,0,0], \SI = \text{diagonal}_{6\times 6}(2)  \nonumber \\
    	\sigma^2 \sim & \mathcal{IG}(A,B)   \nonumber \\
       	& \text{where:}\qquad A = 3, \; B = 9.  \nonumber
	\end{align}
	and the notation ``$\text{diagonal}_{6\times 6}(2)$'' indicates a six-by-six square matrix with 2's on the diagonal and 
    zeros elsewhere.  The hyperparameters $A = 3, B = 9$ on $\sigma^2$ were chosen to provide a diffuse and therefore
    relatively uninfluential form of the prior distribution.  Later we will use better prior distributions for $\sigma^2$ based
	on folded distributions (i.e., normals, or Student's-$t$, excluding negative values), but these forms are not
	conjugate and require more elaborate estimation procedures.  The specification above produces the model results 
	summarized in Table~\ref{PBC.Conjugate.Table} with posterior moments and quantiles.
	\index{subjectindex}{posterior distribution!Student's-$t$}
	
	\begin{table}[h]
    \parbox[c]{\linewidth}{
	    \begin{center}
        \hspace{-33pt}
	    \begin{small}
	    \tabletitle{\textsc{Posterior: Linear Model, Conjugate Priors}}\label{PBC.Conjugate.Table}
	    \vspace{0.07in}
	    \begin{tabular}{l|ccccc}
		        & \multicolumn{5}{c}{}\\[-5pt]
	    $\B$ Covariate$\;$  &      Mean & Std. Error & 0.025 Q. &       Median & 0.975 Q.  \\
	    \hline
		        & \multicolumn{5}{c}{}\\[-7pt]
	    \texttt{Intercept}    &  ~96.337 &   7.347  &    ~81.904  &  ~96.337  &    110.770  \\
	    \texttt{Technology}   &  -46.635 &   3.327  &    -53.171  &  -46.635  &    -40.099  \\
	    \texttt{New}          &  ~-0.378 &   0.040  &    ~-0.456  &  ~-0.378  &    ~-0.300  \\
	    \texttt{Size}         &  ~~0.155 &   0.006  &    ~~0.143  &  ~~0.155  &    ~~0.167  \\
	    \texttt{Republican}   &  ~-0.085 &   0.007  &    ~-0.099  &  ~-0.085  &    ~-0.070  \\
	    \texttt{White}        &  ~-0.049 &   0.006  &    ~-0.060  &  ~-0.049  &    ~-0.038  \\
	    \hline
		     & \multicolumn{5}{c}{}\\[-7pt]
	    $\sigma$     &  ~15.577 &   0.343  &    ~14.925  &  ~15.569  &    ~16.273  \\
	    \end{tabular}
	    \end{small}
	    \end{center}
    }
	\end{table}
	
	From this reanalysis, we can see that it does not make a dramatic difference whether an 
	uninformed or a conjugate prior is specified (at least with these assigned parameters).  
	While there are slight differences, it seems to be of little substantive concern.  This 
	is partly due to the sample size of course ($n=516$ precincts), meaning that the 
	likelihood dominates our choice of prior here.
\end{examplelist}	

\section{Posterior Predictive Distribution for the Data}\label{section:poster.predictive}
\index{subjectindex}{posterior predictive distribution|(}
We will explore the topic in greater detail in Chapter~\ref{Testing.Chapter}, but it is interesting at this point to derive the marginal 
distribution of future draws of the data from the Bayesian linear model.  This is the \emph{predictive} distribution of the data assumed 
to be generated by the model implied by the posterior distribution of the parameters.  Thus we can compare the distribution of the data 
from this prediction with the actual data where large observed differences may indicate poor model fit.

Consider predictive data, a vector $\ty$ of length $k<n-2$, generated from inserting actual or hypothetical covariate values into
the $q \times k$ matrix $\tX$.  Instead of a dataset of size $n$, we create our own set of explanatory values of size $q$.  The $q
\times 1$ vector of actual predictive data is assumed to be produced from the linear model,
\begin{equation}
	\ty = \tX\B + \EP,
\end{equation}
but interest lies instead in the posterior distribution of the $\ty$ unconditional on parameters: $\pi(\ty|\y,\tX,\X)$.  
This means that we are interested in linear predictions that result from constructing a design matrix of covariate 
values and multiplying this by the estimated coefficients from the original model with this linear form, but we want 
a distribution for this prediction that incorporates all sources of variance, including that of the coefficient 
posteriors.  To get this unconditional distribution, start with the relation: 
\begin{equation}\label{predictive.joint}
	\pi(\ty,\B,\sigma^2|\tX,\X,\y) = \pi(\ty|\B,\sigma^2,\tX) \pi(\B,\sigma^2|\X,\y),
\end{equation}
where the first term on the right-hand side is just the normal PDF of new data, given parameterization,
\begin{equation}
	\pi(\ty|\B,\sigma^2,\tX) \propto \frac{1}{(\sigma^2)^{q/2}} 
					\exp\left[ -\frac{1}{2\sigma^2} (\ty-\tX\B)'(\ty-\tX\B) \right]
\end{equation}
and the second term on the right-hand side is joint posterior of the parameters from the linear model,
\begin{equation}
	\pi(\B,\sigma^2|\X,\y) \propto (\sigma^2)^{-\frac{n+1}{2}} 
				       \exp\left[ -\frac{1}{2\sigma^2} (\hat{\sigma}^2(n-k) + 
				       (\B-\bh)'\X'\X(\B-\bh)) \right]
\end{equation}
($\bh = (\X'\X)^{-1}\X'\y$), equation \eqref{joint.posterior.uninformed.linear} from 
the uninformed priors $p(\B) \propto c$ and $p(\sigma^{2}) = \frac{1}{\sigma}$.  We want to unwind the square in the exponent according 
to: $(\B-\bh)\X'\X(\B-\bh) = (\B-(\X'\X)^{-1}\X'\y)'\X'\X(\B-(\X'\X)^{-1}\X'\y) = \B'\X'\X\B - 2\B'\X'\y + \y'\y = (\y-\X\B)'(\y-\X\B)$.  
Putting these last two together gives:
\begin{multline}
	\pi(\ty,\B,\sigma^2|\tX,\X,\y) \propto (\sigma^2)^{-\frac{n+q+1}{2}} \\
                            \times \exp\left[ -\frac{1}{2\sigma^2} 
				       		\left( (\y-\X\B)'(\y-\X\B) + (\ty-\tX\B)'(\ty-\tX\B) \right) \right].
\end{multline}

The quantity of interest is obtained by integrating out $\B$ and $\sigma^2$ individually, starting with $\sigma^2$:
\begin{align}
	\pi(\ty,\B|\tX,\X,\y) 	&= \int_0^\infty \pi(\ty,\B,\sigma^2|\tX,\X,\y) d\sigma^2	
					\nonumber \9	
				&\propto \int_0^\infty (\sigma^2)^{-\frac{n+q+1}{2}} \exp\left[ -\frac{1}{2\sigma^2}
                                       \left( (\y-\X\B)'(\y-\X\B) \right.\right. \nonumber \9
				&\left.\left. \qquad\qquad\qquad\qquad\qquad\qquad 
					+ (\ty-\tX\B)'(\ty-\tX\B) \right) \right] d\sigma^2. 
					\nonumber \9
				&\propto \left[ (\y-\X\B)'(\y-\X\B) + (\ty-\tX\B)'(\ty-\tX\B) \right]^{-\frac{n+q}{2}},
\end{align}
where we used the same gamma PDF trick as before in \eqref{gamma.trick}.  To integrate out $\B$ we need to collect terms in a more useful 
way, starting with breaking out the squares:
\begin{equation}
	\pi(\ty,\B|\tX,\X,\y) 	= \left[\y'\y - 2\B\X'\y + \B'\X'\X\B + \ty'\ty - 2\B\tX'\ty + \B'\tX'\tX\B 
					\right]^{-\frac{n+q}{2}}.
\end{equation}
Defining $\LL=\X'\y + \tX'\ty$ and $\MM=\X'\X + \tX'\tX$, this last expression can be expressed as:
\begin{align}
	\pi(\ty,\B|&\tX,\X,\y) 	 \propto \left[ \y'\y + \ty'\ty + \B'\MM\B - 2\B'\LL\right]^{-\frac{n+q}{2}} \nonumber\9
				&= \left[ \y'\y + \ty'\ty -\LL'\MMI\LL \right. \nonumber\9	
				&\qquad\left. + (\B'\MM\B - \B'\MM\MMI\LL - \LL'\MMI\MM\B + \LL'\MMI\MM\MMI\LL)
				   \right]^{-\frac{n+q}{2}}	\nonumber\9
				&= \left[ \y'\y + \ty'\ty -\LL'\MMI\LL + (\B-\MMI\LL)'\MM(\B-\MMI\LL) \right]^{-\frac{n+q}{2}}.
\end{align}
Zellner (1971, p.73)\index{authorindex}{Zellner, A.} integrates this form over the $k$ length $\B$ vector to produce the 
posterior predictive distribution of interest:
\begin{equation}
	\pi(\ty|\tX,\X,\y) 	\propto \left[ \y'\y + \ty'\ty + \B'\MM\B - 2\B'\LL \right]^{-\frac{n+q-k}{2}}.
\end{equation}
He then defines the substitution $\HH = (\I - \tX\MMI\tX')/\hat{\sigma}^2$ and reintroduces $\bh$ to produce the form:
\begin{equation}\label{posterior.predictive.dist.y}
	\pi(\ty|\tX,\X,\y) 	\propto \left[ (n-k) + (\ty-\tX\bh)'\HH(\ty-\tX\bh) \right]^{-\frac{n+q-k}{2}},
\end{equation}
which makes it easier to see that this is a multivariate $t$-distribution with $\nu = n-k$ degrees of freedom.  Recalling the properties of the 
multivariate-$t$ (Appendix~\ref{distribution.appendix}), this result means that:
\begin{equation}
	E[\ty] = \tX\bh,	\qquad\qquad\qquad 	\Cov[\ty] = \frac{n-k}{n-k-2}\HH^{-1},
\end{equation}
with the obvious restriction on the size of $n-k$ from the denominator.

This result is actually quite intuitive.  While the data themselves are assumed to be normally distributed, future claims about the data 
conditional on the model are t-distributed reflecting added uncertainty.  \index{subjectindex}{posterior predictive distribution|)}

% NEED: ADD THIS DATASET (AT DATAVERSE) TO BaM [added 5/13/15]
\index{authorindex}{Meier, K. J.} \index{authorindex}{Polinard, J. L.} \index{authorindex}{Wrinkle, R.}
\begin{examplelist}
	\item	\label{example:education.effects} {\bf A Model of Educational Effects.}	\index{subjectindex}{example!education policy}
		The Bayesian linear model is further illustrated through a partial replication of a Meier, Polinard, and Wrinkle 
		(2000) study of bureaucratic effects on education outcomes in public schools (see also the reanalysis in Wagner and 
		Gill [2005], and extensions in Meier and Gill [2000]).\index{authorindex}{Wagner, K.}\index{authorindex}{Gill, J.} 
		These authors are concerned with whether the education bureaucracy is the product or cause of poor student performance.  
		The issue is controversial, and Chubb and Moe (1988, 1990) argue otherwise that the institutional structure of the schools, 
        especially the overhead democratic control, resulted in the schools being ineffective. The institutional structure and the 
        bureaucracy created a process that leads to poor performance by the public schools.  \index{authorindex}{Chubb, J. E.} 
        \index{authorindex}{Moe, T.} This conclusion is challenged by Meier and Smith (1994), as well as in Smith and Meier (1995), 
        \index{authorindex}{Meier, K. J.} \index{authorindex}{Smith, K. B.} who contend that bureaucracy is an adaptation to poor 
        performance and not the cause.

		\index{authorindex}{Meier, K. J.} \index{authorindex}{Polinard, J. L.} \index{authorindex}{Wrinkle, R.} 
		Meier, Polinard, and Wrinkle develop a linear model based on panel dataset from more than 1,000 school districts 
		for a seven-year period to test organizational theory and educational policy, producing an impressively large dataset
        here with $n=7301$ cases.  The question asked is whether there is a causal relationship between bureaucracy and poor 
        performance by public schools.  The central issue in this literature is one of causality through a ``production function'' 
        that maps inputs to outputs in essentially an economic construct.  Therefore their outcome variable is the percent of students
        in district/year that pass the Texas Assessment of Academic Skills (TAAS), which measures mastery of basic skills.
        In addition to bureaucratic causes, student and school performance can be influenced by a number of variables, 
        some of which are causally related, including class size, state funding, teacher salary, and teacher experience.  The data
        measure bureaucracy as the total number of full-time administrators per 100 students and lag the variable so as to create
        a more likely causal relationship.  Other variables include three measures of financial capital, which consist of the
        average teacher salary, per pupil expenditures for instruction and the percentage of money each district receives from
        state funds.  A measure of human capital was included based on teacher experience, and two policy indicators were used by
        measuring the average class size in the district and the percent of students in gifted classes.

		\index{authorindex}{Meier, K. J.} 
		The linear regression model proposed by Meier \etal is affected by both serial correlation and heteroscedasticity.  
		Meier \etal address these concerns through a set of six dummy variables 
		for each year as well as through the use of weighted least squares.  The Meier \etal \index{authorindex}{Meier, K. J.}  
		results are obtained by specifying diffuse normal priors on the unknown parameters.  These are Gaussian normal specifications 
		centered at zero with small precision.  The model is summarized in ``stacked'' notation that shows the distributional 
		assumptions (priors and likelihood):
		\begin{align}\label{model.specification.1}
    		Y[i]        &\sim \mathcal{N}(\lambda[i],\sigma^2), 					\nonumber\9
    		\lambda[i]  &= \beta_0 + \beta_1 x_1[i] + \ldots + \beta_k x_k[i] + \epsilon[i] 	\nonumber\9
    		\beta[i]    &\sim \mathcal{N}(0.0,10)							\nonumber\9
    		\epsilon[i] &\sim \mathcal{N}(0.0, \tau) 						\nonumber\9
			\tau	    &\sim \mathcal{IG}(16,6)
		\end{align}
        Note the hierarchical expression of these distributional and modeling assumptions here, which is the conventional way
        to notate models with dependent distributional features (Chapter~\ref{Hierarchical.Chapter}).  This specification above 
        allows a close Bayesian replication of the original model since all of the coefficient prior distribution forms are 
        highly diffuse, and the precision prior distribution is tuned to resemble $1/\sigma^2$ from Meier \etal  This model is
        estimated using \bugs\ software, although with more agony it could be directly computed.  The results are provided in 
        Table~\ref{replication.table}.

		\blstable
		\begin{table}[h]
			\begin{center}
			\tabletitle{\textsc{Posterior: Meier Replication Model}\label{replication.table}}
			\vspace{0.02in}
			\begin{tabular}{lcccr@{:}l}
        		Explanatory Variables & \multicolumn{1}{c}{Mean} & \multicolumn{1}{c}{Std. Error} 
                        & \multicolumn{3}{c}{\quad 95\% HPD Region}\\
        		\hline
            	\texttt{Intercept}                           &  9.172  & 1.358  &\;\qquad& [~6.510 & 11.840] \\
        		\texttt{Low Income Students}                 & -0.108  & 0.006  &\;\qquad& [-0.119 & -0.097] \\
        		\texttt{Teacher Salaries}                    &  0.073  & 0.053  &\;\qquad& [-0.035 & ~0.181] \\
        		\texttt{Teacher Experience}                  & -0.009  & 0.046  &\;\qquad& [-0.099 & ~0.082] \\
        		\texttt{Gifted Classes}                      &  0.097  & 0.023  &\;\qquad& [~0.054 & ~0.139] \\
        		\texttt{Class Size}                          & -0.220  & 0.052  &\;\qquad& [-0.322 & -0.118] \\
        		\texttt{State Aid Percentage}                & -0.002  & 0.004  &\;\qquad& [-0.010 & ~0.006] \\
        		\texttt{Funding Per Student} ($\times 1000$) &  0.065  & 0.174  &\;\qquad& [-0.276 & ~0.406] \\
        		\texttt{Lag of Student Pass Rate}            &  0.677  & 0.008  &\;\qquad& [~0.661 & ~0.693]\\
        		\texttt{Lag of Bureaucrats}                  & -0.081  & 0.262  &\;\qquad& [-0.595 & ~0.431] \\
			\hline
    			\multicolumn{6}{c}{Posterior standard error of $\tau = 0.00072$}\\
			\end{tabular}
			\end{center}
		\end{table}
		\bls

        For this model we can calculate the posterior predictive distribution of the data as given by
        \eqref{posterior.predictive.dist.y} in the previous section.  While there are certainly many interesting cases that a
        scholar of education policy might insert into the rows of the design matrix, $\tX$, we will limit the analysis here to the
        predicted outcome for all of the explanatory variables set at their mean.  Producing other cases to reveal important
        effects is much like a first difference calculation for evaluating the effect of coefficients in GLM models.  The
        posterior predictive value for $\ty$ where $\tX = \bar{\X}$ is 48.18 (with a standard error of 0.0007202 due to the huge
        size of the dataset), yet the mean of the $y$ vector is 63.84.  This is interesting because it shows that the mean model
        outcome predicts a much poorer outcome than that observed suggesting that the model is missing some important features of
        the data-generating process.  The other obvious suggestion that the data are right-skewed is not true.

		\index{authorindex}{Meier, K. J.} \index{authorindex}{Smith, K. B.}
        To expand on the Meier \etal model, it is possible to include non-sample information for the creation of the Bayesian
        prior drawn from Meier's previous work on school bureaucracy and school performance with Kevin Smith (1994).  Clearly,
        Meier \etal were not uninformed when specifying the model above and Bayesian inference allows for the incorporation of
        that knowledge. The Smith and Meier work includes data and inference on the impact of funding and other institutional
        variables on student achievement in Florida.  These include district level data for all of the public schools in Florida.
        Smith and Meier note also that the Florida data provides a diverse group of students with constant measures over time. The
        Florida data represents both rural and urban districts as well as different ethnic and socioeconomic compositions.  The
        prior distributions remain normal, but are now centered around values drawn from the 1995 Smith and Meier findings:
        \index{authorindex}{Meier, K. J.} \index{authorindex}{Smith, K. B.}
		\begin{align*}\label{model.specification.2}
     			\beta[0]  &\sim \mathcal{N}(0.0,10)      &
     			\beta[1]  &\sim \mathcal{N}(-0.025,10)   &
     			\beta[2]  &\sim \mathcal{N}(0.0,10)      \\
     			\beta[3]  &\sim \mathcal{N}(0.23,10)     &
     			\beta[4]  &\sim \mathcal{N}(0.615,10)    &
     			\beta[5]  &\sim \mathcal{N}(-0.068,10)   \\
     			\beta[6]  &\sim \mathcal{N}(0.0,10)      &
     			\beta[7]  &\sim \mathcal{N}(-0.033,10)   &
     			\beta[8]  &\sim \mathcal{N}(0.299,10)    \\
     			\beta[9]  &\sim \mathcal{N}(0.0,10)      &
     			\beta[10] &\sim \mathcal{N}(0.0,10)     &
     			\beta[11] &\sim \mathcal{N}(0.0,10)     \\
     			\beta[12] &\sim \mathcal{N}(0.0,10)     &
     			\beta[13] &\sim \mathcal{N}(0.0,10)     &
     			\beta[14] &\sim \mathcal{N}(0.0,10)     \\
     			\beta[15] &\sim \mathcal{N}(0.0,10)     &
		\end{align*}
        The numbered $\B$ terms represent the prior information assigned to each explanatory variable (such \bugs\ statements are
        more elegant vectorized but the prior assignments are then less obvious to a reader).  The terms are ordered as in
        Table~\ref{replication.table}, with the additional $\beta[10:15]$ values representing years 1993-97.  Some of these are
        left relatively uninformed since the data from the Smith and Meier research were insufficient to address all of the
        current terms.  \index{authorindex}{Meier, K. J.} \index{authorindex}{Smith, K. B.}

		\index{authorindex}{Meier, K. J.} 
		Interestingly, Meier \etal expected to find a positive relationship between teacher salaries and student performance, 
		but did not find one.  The model that includes stipulated priors generated results that were closer to the expectation 
		of the researchers since it incorporated knowledge to which the researchers already had access.  Meier \etal noted
		that economic theory expects higher salaries attract better teachers.

		\blstable
		\begin{table}[h]
		\begin{center}
		\tabletitle{\textsc{Posterior: Interaction Model}\label{new.model2.table}}
		\vspace{0.02in}
		\renewcommand{\arraystretch}{1.1}
		\begin{tabular}{lcccr@{:}l}
        		Explanatory Variables & \multicolumn{1}{r}{Mean} & \multicolumn{1}{r}{Std. Error} 
                    & \multicolumn{3}{l}{95\% HPD Region}\\
        		\hline
    			\texttt{Intercept}                       &  4.799  & 2.373  &\;\qquad& [~0.165 &~9.516] \\
        		\texttt{Low Income Students}             & -0.105  & 0.006  &\;\qquad& [-0.117 &-0.094] \\
        		\texttt{Teacher Salaries}                &  0.382  & 0.099  &\;\qquad& [ 0.189 &~0.575] \\
        		\texttt{Teacher Experience}              & -0.066  & 0.046  &\;\qquad& [-0.156 &~0.025] \\
        		\texttt{Gifted Classes}                  &  0.096  & 0.021  &\;\qquad& [ 0.054 &~0.138] \\
        		\texttt{Class Size}                      &  0.196  & 0.191  &\;\qquad& [-0.180 &~0.569] \\
        		\texttt{State Aid Percentage}            &  0.002  & 0.004  &\;\qquad& [-0.006 &~0.010] \\
        		\texttt{Funding Per Student} ($\times 1000$)
                    		&  0.049  & 0.175  &\;\qquad& [-0.294 &~0.392] \\
        		\texttt{Lag of Student Pass Rate}        &  0.684  & 0.008  &\;\qquad& [~0.667 &~0.699]\\
        		\texttt{Lag of Bureaucrats}              & -0.042  & 0.261  &\;\qquad& [-0.557 &~0.469] \\
        		\texttt{Class Size} $\times$ \texttt{Teacher Salaries}
                    		& -0.015  & 0.007  &\;\qquad& [-0.029 &-0.002] \\
		\hline
    		\multicolumn{6}{c}{Posterior standard error of $\tau = 0.00071$}\\
		\end{tabular}
		\end{center}
		\end{table}
		\bls
		
		In addition, the new model adds a multiplicative interaction between class size and teacher salary as the 
		variables are claimed to be related in this way.  The interaction coefficient was found to have a negative 
		sign with 95\% credible interval bounded away from zero, as provided in Table~\ref{new.model2.table}.  So larger class 
		sizes have an apparent dampening effect on the positive impact of increasing teacher salaries.  Also, the posterior 
		distribution for class size now shows a much less reliable effect in the interaction model (the 95\% credible interval is 
		almost centered at zero).  Interaction effects can sometimes ``steal'' explanatory power and reliability from main effects.  
		This finding says that the effects of class size are now only reliable in this model in the context of specified teachers'
		salary levels. 

        If this model is qualitatively different than the Meier Replication Model, then we would expect the posterior predictive
        distribution to show some change reflecting better fit to the data.  From the mean model, applying the data mean as input
        $\tX$, the posterior predictive value of $y$ is 51.77 (with a standard error of 0.00071022), compared to the $y$ data mean
        of 63.84.  So the enhanced model predicts slightly better but not as much as we would have hoped.  The point from this 
        comparison is that one way to compare model fit in model development is to compare the predicted data to the actual.  We 
        will be much more systematic about this process in Chapter~\ref{Testing.Chapter}, but it easy to see here that posterior 
        quantities form the basis of any such comparison.

\begin{comment} % NEW CODE TO ADD TO R PACKAGE
source("Article.IJPA/Data7301imp.txt")
n <- ijpa$N
# REPLICATION MODEL
X <- matrix(cbind("Intercept"=rep(1,n), "Low Income Students"=ijpa$X2, "Teacher Salaries"=ijpa$X3,
        "Teacher Experience"=ijpa$X4, "Gifted Classes"=ijpa$X5, "Class Size"=ijpa$X6,
        "State Aid Percentage"=ijpa$X7, "Funding Per Student times 1000"=ijpa$X7,
        "Lag of Student Pass Rate"=ijpa$X9, "Lag of Bureaucrats"=ijpa$X10),nrow=n)
        #"Class Size times Teacher Salaries"=ijpa$X6*ijpa$X3),nrow=n)
k <- ncol(X)
beta <- c(9.172, -0.108, 0.073, -0.009, 0.097, -0.220, -0.002, 0.065, 0.677, -0.081)
sig.hat.sq <- 0.00072
mean.vec <- apply(X,2,mean)
DX <- X; for(i in 1:n) DX[i,] <- jitter(X[i,],10000); DX[1,] <- mean.vec
M <- t(X)%*%X + t(DX)%*%DX
H <- (diag(nrow(X)) - DX%*%solve(M)%*%t(DX))/sig.hat.sq
( E.Y <- (DX%*%beta)[1] )
#( Cov.Y <- ((n-k)/(n-k-2))*solve(H)[1,1] )
( Cov.Y <- ((n-k)/(n-k-2))*H[1,1]^(-1) )

# INTERACTION WITH INFORMED PRIORS MODEL  
X <- matrix(cbind("Intercept"=rep(1,n), "Low Income Students"=ijpa$X2, "Teacher Salaries"=ijpa$X3,
        "Teacher Experience"=ijpa$X4, "Gifted Classes"=ijpa$X5, "Class Size"=ijpa$X6,
        "State Aid Percentage"=ijpa$X7, "Funding Per Student times 1000"=ijpa$X7,
        "Lag of Student Pass Rate"=ijpa$X9, "Lag of Bureaucrats"=ijpa$X10,
        "Class Size times Teacher Salaries"=ijpa$X6*ijpa$X3),nrow=n)
k <- ncol(X)
beta <- c(4.799, -0.105, 0.382, -0.066, 0.096, 0.196, 0.002, 0.049, 0.684, -0.042, -0.015)
sig.hat.sq <- 0.00071
mean.vec <- apply(X,2,mean)
DX <- X; for(i in 1:n) DX[i,] <- jitter(X[i,],10000); DX[1,] <- mean.vec
M <- t(X)%*%X + t(DX)%*%DX
H <- (diag(nrow(X)) - DX%*%solve(M)%*%t(DX))/sig.hat.sq
( E.Y <- (DX%*%beta)[1] )
#( Cov.Y <- ((n-k)/(n-k-2))*solve(H)[1,1] )
( Cov.Y <- ((n-k)/(n-k-2))*H[1,1]^(-1) )
\end{comment}

\end{examplelist}

\section{Linear Regression with Heteroscedasticity}\index{subjectindex}{linear model!heteroscedastic}
It is not uncommon to encounter the linear regression problem of non-constant error variance.  The typical means of 
dealing with this problem in the non-Bayesian setting is through a more general form of least squares inserting a 
weighting matrix.  The Bayesian approach is similar but leads to some difficult analytical issues, which we will 
overcome here.  The definitive citation is Geweke (1993) \index{authorindex}{Geweke, J.} and useful discussions can 
also be found in Mouchart and Simar (1984), who show that the Bayesian model can be worked out with least squares 
theory, and Leonard (1975), who shows the importance of exchangeability in this context.
\index{authorindex}{Mouchart, M.} \index{authorindex}{Simar, L.} \index{authorindex}{Leonard, T.} 

Instead of the usual assumption about the distribution of $\y$ given $\X$, we now assert that $\y_i|\X_i \sim
\mathcal{N}(\X_i\B,\sigma^2\omega_i)$, where $\omega=(\omega_1,\omega_2,\ldots,\omega_n)$ is a vector of unknown
regression weights (parameters), which we can also organize along the diagonal of a $n \times n$ matrix $\OM$
for convenience.  The linear model is now defined as:
\begin{equation}
	\y = \X\B + \EP,	\qquad\qquad \Var[\EP] = \sigma^2\OM,
\end{equation}
which implies from the conditional distribution of the $\y_i$ that $\EP_i \sim \mathcal{N}(0,\sigma^2\omega_i)$.  
Thus the likelihood function from \eqref{linear.likelihood.max} becomes:
\begin{equation}
    L(\B,\mathbf{\sigma}^2|\X,\y) \propto \sigma^{-n} |\OM|^{-\half}\exp\left[ -\frac{1}{2\sigma^2}
        				     (\hat{\sigma}^2(n-k) + 
					     (\B-\bh)'\X'\OM^{-1}\X(\B-\bh)) \right].
\end{equation}
We will again use the uninformed priors $p(\B) \propto c$ and $p(\sigma^{2}) = \frac{1}{\sigma}$ over the support 
$[-\infty\range \infty]$ and $[0\range \infty]$.  Geweke (1993)\index{authorindex}{Geweke, J.} suggests using 
independent chi-square distributions, $\omega|\nu \sim \chi^2(df=\nu)$, which is also expressible as the gamma 
distribution $\mathcal{G}(\nu/2,1/2)$.  This $\nu$ parameter is flexible and can be fixed or estimated The resulting 
joint posterior distribution is:	\index{subjectindex}{distribution!chi-square@$\chi^2$}
\begin{align}\label{joint.posterior.uninformed.hetero}
    \pi(\B,\sigma^2,\OM|\X,\y) & \propto L(\B,\sigma^2|\X,\y)p(\B)p(\sigma^{2})p(\OM)		\nonumber\9
    			               & \propto \sigma^{-n-1} |\OM|^{-\frac{\nu+3}{2}} 
			 		                \exp\biggl[ -\frac{1}{2\sigma^2} \biggl( (\hat{\sigma}^2(n-k) 	\nonumber\9
			                   &\qquad\qquad + (\B-\bh)'\X'\OM^{-1}\X(\B-\bh) + \nu\tr(\OM)^{-1} \biggr) \biggr]. 
\end{align}
This posterior turns out to be quite difficult to integrate for marginals, although $\pi(\B,\OM|\X,\y)$ can be 
obtained with methods given previously in this chapter.  Geweke\index{authorindex}{Geweke, J.} instead finds three 
\emph{conditional} distributions according to the following (suppressing the data in the conditional for emphasis).
\begin{bayeslist}
	\item	The conditional posterior distribution of $\B$:
		    \begin{equation}
			    \pi(\B|\sigma^2,\OM) \propto \exp\left[ -\frac{1}{2\sigma^2} (\B-\bh^*)'\X'\OM^{-1}\X(\B-\bh^*) \right],
		    \end{equation}
		    where $\bh^* = (\X'\OM\X)^{-1}\X'\OM\y$.  Thus the conditional distribution of the $\B$ vector is 
            multivariate normal according to $\mathcal{N}(\bh^*,\sigma^2(\X'\OM^{-1}\X)^{-1})$.

	\item	The conditional posterior distribution of $\sigma^2$:
		    \begin{equation}
			    \pi(\sigma^2|\OM) \propto (\sigma^2)^{-\frac{n+1}{2}} \exp \left[-\frac{1}{2\sigma^2} \hat{\sigma}^{2*} \right],
		    \end{equation}
		where $\hat{\sigma}^{2*} = (\y-\X\bh)'\OM^{-1}(\y-\X\bh)$.  This is clearly an inverse gamma distribution 
        according to $\mathcal{IG}(\sigma^2|(n-1)/2,\hat{\sigma}^{2*}/2)$, and Geweke\index{authorindex}{Geweke, J.} 
        also expresses it in $\chi^2$ terms.  \index{subjectindex}{distribution!chi-square@$\chi^2$}
	
	\item	The conditional posterior distribution of $\OM$:
		    \begin{equation}
			    \pi(\OM|\B,\sigma^2) \propto |\OM|^{-\frac{\nu+3}{2}} 
                    \exp\left[-\frac{1}{2}\left( \hat{\sigma}^{2*}/\sigma^2 + \nu \tr(\OM)^{-1} \right) \right]
		    \end{equation}
		    where $\hat{\sigma}^{2*}$ is defined as before.  So an individual diagonal element of $\OM$ is 
            conditionally distributed $\omega_i|\B,\sigma^2 \propto \omega_i^{-\frac{\nu+3}{2}} \exp\left[ 
                -(u_i^2/2\sigma^2 + \nu/2)/\omega_i \right]$, where $u_i = y_i - \x_i\B$.  This gives an inverse 
            gamma PDF with parameters $\frac{\nu+1}{2}$ and $(u_i^2/\sigma^2 + \nu)/2$.
\end{bayeslist}
Another interesting feature of this model is that the posterior distribution of the residuals are no longer normal 
according to $\EP_i \sim \mathcal{N}(0,\sigma^2\omega_i)$.  In fact, they end up being Student's-$t$ distributed with $\nu = n-k$ 
degrees of freedom.  Geweke\index{authorindex}{Geweke, J.} also points out that the model developed in this fashion 
produces residuals that are \emph{independent} Student's-$t$ (an independent $\chi^2$ denominator of a normal), 
whereas previous versions produced residuals with a \emph{joint} Student's-$t$ (a common $\chi^2$ denominator of a 
normal). \index{subjectindex}{posterior distribution!Student's-$t$}\index{subjectindex}{distribution!chi-square@$\chi^2$}

While the results above may not seem as useful as the unconditional marginal posteriors we derived earlier in this 
chapter, it does set us up perfectly for the Gibbs sampler as a way to get desired unconditionals.  Recall from 
Chapter~\ref{Intro.Chapter} that the Gibbs sampler uses iterated samples from full conditional distributions for the
parameters of interest to obtain empirical estimates of marginals (page~\pageref{first.gibbs}).  In the present case
we sample iteratively at the $j+1$ step according to:\label{Geweke.Gibbs.Steps}
\begin{align*}
    \B^{[j+1]}              &\sim \pi(\B|{\sigma^2}^{[j]},\OM^{[j]})           \9
    {\sigma^2}^{[j+1]}      &\sim \pi(\sigma^2|\OM^{[j]})                   \9
       \OM^{[j+1]}          &\sim \pi(\OM|\B^{[j+1]},{\sigma^2}^{[j+1]})
\end{align*}
where the $\B$ vector and the $\OM$ matrix (which is really a vector's worth of information) can be sampled individually
($\B_1^{[j]},\ldots,\B_k^{[j]},\omega_1^{[j]},\ldots,\omega_n^{[j]}$) or as a block.  Miraculously this iterative process
eventually produces sample values that behave as if they were generated from the marginal distributions rather than the
conditional distributions.  Details on this MCMC procedure are given starting in Chapter~\ref{MCMC.Chapter} and essentially
constitute the second half of this text.  
	
\begin{examplelist}
	\item	\label{ancient.china.war.example}
            {\bf War in Ancient China.}	\index{subjectindex}{example!war in ancient China}
	        To demonstrate the described heteroscedastic linear model with Bayesian priors we use conflict data from 
            West Asia for events taking place between 2700 BCE to 722 BCE.  Cioffi-Revilla and Lai (1995, 2001)
            \index{authorindex}{Cioffi-Revilla, C.} \index{authorindex}{Lai, D.} coded documents from multiple 
            epigraphic and archaeological sources on war and politics in ancient China covering the Xia (Hsia), Shang, 
            and Western Zhou (Chou) periods.  These data ($n=104$ conflicts) are available via the Murray Archive.  
            This is the only quantitative dataset covering Chinese war during this period, but Cioffi-Revilla and Lai 
            use the modern \emph{Long-Range Analysis of War} definitions.

            The outcome variable of interest here is an additive combination of two of the coded variables: \emph{Political Level}
            (1 for internal war, 2 for interstate war) and \emph{Political Complexity} (governmental level of the warring
            parties), where the first variable is multiplied by ten for scale purposes.  Thus we are looking to explain the
            \emph{political scope} of conflicts in terms of governmental units affected.  Explanatory variables of interest are
            \texttt{EXTENT} (number of belligerents), \texttt{DIVERSE} (number of ethnic groups participating as belligerents),
            \texttt{ALLIANCE} (total number of alliances among belligerents), \texttt{DYADS} (number of alliance pairs),
            \texttt{TEMPOR} (type of war: protracted rivalry, integrative conquest, disintegrative/fracturing conflict, sporadic
            event), \texttt{DURATION} of conflict, measured in years, \texttt{BALANCE} (the difference in military capabilities:
            minor-minor, minor-major, major-major), \texttt{ETHNIC} (intra-group or inter-group), and \texttt{POLAR} (number of
            relatively major or great powers at the time of onset).  See Cioffi-Revilla and Lai (2001) or the associated codebook
            for further details.  Graphical investigation indicates the presence of heteroscedastic effects of concern with a
            homoscedastic linear model.  See Figure~\ref{cumsum.wars.fig}.  
            \index{authorindex}{Cioffi-Revilla, C.}\index{authorindex}{Lai, D.} 

            \begin{figure}[h]
            \parbox[c]{\linewidth}{
                \begin{center} 
                    \epsfig{file=Images/linear.figure01.ps,height=3.92in,width=5.5in,clip=,angle=0}
                    \caption{\textsc{MCMC Running Means, Chinese Conflicts}}\label{cumsum.wars.fig}
                \end{center} 
            }
            \end{figure}
    
            The Bayesian heteroscedastic linear model is fit without an intercept (zero levels do not make sense here), using
            uninformed priors $p(\B) \propto c$ and $p(\sigma^{2}) = \frac{1}{\sigma}$, as discussed above.  Furthermore, fix $\nu
            = 50$ to elongate the distribution of weights for this example.  The Gibbs sampler is run for $100,000$ iterations,
            throwing away the first $50,000$ (this early ``burn-in'' period will discussed at length starting on
            page~\pageref{burn-in.section}). Table~\ref{chinese.model2.table} summarizes the resulting marginal posteriors from
            this estimation.  The \R\ code for running this model is provided in the {\bf Computational Addendum} to this chapter.
            Details about running Gibbs samplers are provided starting in Chapter~\ref{MCMC.Chapter} and diagnostics for MCMC
            convergence are discussed particularly in Chapter~\ref{MCMC.Utilitarian.Chapter}.  For now note that the Markov chain
            iterations become stable in the cumulative mean plots given in Figure~\ref{cumsum.wars.fig}.
            \index{subjectindex}{linear model!heteroscedastic}

	        \begin{table}[h]
            \parbox[c]{\linewidth}{
	            \blstable
	            \begin{center}
                \hspace{-55pt}
	            \tabletitle{\textsc{Heteroscedastic Model, Ancient Chinese Conflicts}\label{chinese.model2.table}}
	            \vspace{0.02in}
	            \renewcommand{\arraystretch}{1.1}
	            \begin{tabular}{lccr@{:}l}
        	        Explanatory Variables\;\qquad & Mean & Std. Error & \multicolumn{2}{c}{95\% HPD Region}\\
		            \hline
                    \texttt{EXTENT}   &  1.0145 & 0.1077 & [~0.8034 & ~1.2256] \\ 
                    \texttt{ALLIANCE} & -0.2840 & 0.0756 & [-0.4321 & -0.1359] \\ 
                    \texttt{DYADS}    & -0.6540 & 0.0739 & [-0.7988 & -0.5092] \\ 
                    \texttt{TEMPOR}   &  0.1391 & 0.0302 & [~0.0799 & ~0.1984] \\ 
                    \texttt{DURATION} & -0.0779 & 0.0353 & [-0.1471 & -0.0087] \\ 
                    \texttt{BALANCE}  &  0.2810 & 0.0692 & [~0.1454 & ~0.4165] \\ 
                    \texttt{ETHNIC}   &  0.3210 & 0.0574 & [~0.2086 & ~0.4335] \\ 
                    \texttt{POLAR}    &  0.0189 & 0.0078 & [~0.0035 & ~0.0343] \\ 
        	        \hline
		            \multicolumn{5}{c}{Mean of $\sigma^2 = 0.0454$}
	            \end{tabular}
	            \end{center}
	            \bls
            }
	        \end{table}

	        Observe from the resulting HPD regions for the coefficients that the model fits very well with no coefficient
            95\% HPD regions crossing zero.  We see here that the number of belligerents, the number of ethnic groups,
            imbalance in capabilities, ethnic composition, and number of major groups all have accelerating effects on 
            the political scope of the conflict.  The type of war does as well but its interpretation is not as clear.
            Conversely, the number of alliances, the number of dyads, and the duration give the opposite effect.  These 
            are all as expected, despite using the most diffuse priors possible.
\end{examplelist}
%\vspace{1in}

\section{Exercises}
\begin{exercises}
    \item 	Derive the posterior marginal for $\B$ in \eqref{conjugate.posterior.for.beta.from.lm} 
		    from the joint distribution given by \eqref{second.conjugate.linear.posterior}.

            % NEW
    \item   Write an \R\ function that calculates $R^2$ and the $F$-statistic for the two models given in 
            Table~\ref{linear.model.comparison}.  Defend your choice of point estimate from the posterior distributions
            used for these calculations.  Does the Bayesian interpretation of these values differ?  Is it reasonable to
            use these in applied Bayesian work?

	\item	For uninformed priors, the joint posterior distribution of the regression coefficients was shown to be 
            multivariate-$t$ (page~\pageref{beta.posterior.linear.uninformed}), with covariance matrix:
		    $\mathbf{R} = \frac{ (n-k)\hat{\sigma}^{2}(\X'\X)^{-1} }{ n-k-2 }$.  Under what conditions is this matrix 
            positive definite (a requirement for valid inferences here).

            % NEW
    \item   Clogg, Petkova, and Haritou (1995) 
            \index{authorindex}{Clogg, C. C.}  \index{authorindex}{Petkova, E.} \index{authorindex}{Haritou, A.}
            give detailed guidance for deciding between different linear regression models using the same data.  In this work 
            they define the matrices $\X$, which is $n \times (p+1)$ rank $p+1$, and $\Z$, which is $n \times (q+1)$ rank $q+1$, 
            with $p<q$.  They calculate the matrix $A=\left[\X'\X-\X'\Z(\Z'\Z)^{-1}\Z'\X\right]^{-1}$.  Find the dimension and
            rank of $A$.
            % First, the dimension:
            % \begin{multline*}
            %          \left[\underset{(p+1)\times n}{\X'}
            %               \underset{n\times (p+1)}{\X}-
            %               \underset{(p+1)\times n}{\X'}
            %               \underset{n\times (q+1)}{\Z}(
            %               \underset{(q+1)\times n}{\Z'}
            %               \underset{n\times q+1)}{\Z})^{-1}         \right. \\ \left.
            %               \times \underset{(q+1)\times n}{\Z'}
            %               \underset{n\times (p+1)}{\X}\right]^{-1}
            % \end{multline*}
            % Therefore the dimension of $A$ is $(p+1)\times (p+1)$.  The rank of $\X'\X$ is $\min(p+1,n) = p+1$, and
            % the rank of $\X'\Z(\Z'\Z)^{-1}\Z'\X$ is $\min(p+1,q+1,n) = p+1$ (i.e. matrix multiplication cannot increase
            % rank).  So the rank of the difference is obvious $p+1$ and the matrix is not invertible unless it is full rank,
            % so $A$ is rank $p+1$.

	\item	Under standard analysis of linear models, the hat matrix is given by $\hat{y} = \HH y$ where $\HH$ is $\X(\X'\X)^{-1}\X'$ 
            where the diagonal values of this matrix indicate leverage,  which is obviously a function of $X$ only.  Can the 
            stipulation of 	strongly informed priors change data-point leverage?  Influence in linear model theory depends on 
            both hat matrix diagonal values and $y_i$.  Calculate the influence on the \texttt{Technology} variable of each datapoint 
            in the Palm Beach County model with uninformed priors by jackknifing out these values one at a time and re-running 
            the analysis.  Which precinct has the greatest influence?

            % NEW
    \item   Prove that the $\MM$ matrix ($\I - \HH$) from linear regression is symmetric and idempotent.

    \item 	\label{meier.keiser.exercise} \index{authorindex}{Meier, K. J.} \index{authorindex}{Keiser, L. R.}
		    Meier and Keiser (1996) used a linear pooled model to examine the \index{subjectindex}{pooling!data}
			impact of several federal laws on state-level child-support collection policies.  Calculate a Bayesian linear model 
            and plot the posterior distribution of the parameter vector, $\B$, as well as $\sigma^2$, specifying an inverse 
            gamma prior with your selection of prior parameters, and the uninformative uniform prior: $f(\sigma^2)=\frac{1}{\sigma}$.
			Use a diffuse normal prior for the $\B$, and identify outliers using the hat matrix method (try the {\normalfont \R} 
            command {\normalfont \texttt{hat}}).

                The data are collected for the 50 states over the period 1982 to 1991, 
				where the outcome variable, {\small \texttt{\normalfont{SCCOLL}}}, is the 
				change in child-support collections.  The explanatory variables are: 
				chapters per population ({\small \texttt{\normalfont{ACES}}}), policy 
				instability ({\small \texttt{\normalfont{INSTABIL}}}), policy ambiguity 
				({\small \texttt{\normalfont{AAMBIG}}}), the change in agency staffing 
				({\small \texttt{\normalfont{CSTAFF}}}), state divorce rate 
				({\small \texttt{\normalfont{ARD}}}), organizational slack 
				({\small \texttt{\normalfont{ASLACK}}}), and state-level expenditures 
				({\small \texttt{\normalfont{AEXPEND}}}).  These data can be downloaded on
				the webpage for this book  or from the \texttt{BaM} package. 
                Additional description can be found in their original article or Meier\index{authorindex}{Meier, K. J.} and 
				Gill\index{authorindex}{Gill, J.} (2000, Chapter~2).
                % NEED TO MAKE SURE THESE DATA ARE IN BaM [yes]

                		%\begin{center}
				        %\begin{normalfont}\blsref
                		%\begin{tiny} \begin{tabular}{r|rrrrrrr}
                		%\texttt{SCCOLL}  &  \texttt{ACES}  &  \texttt{INSTABIL}  &  \texttt{AAMBIG} &
                    		%\texttt{CSTAFF}  &  \texttt{ARD}  &  \texttt{ASLACK}  &  \texttt{AEXPEND}  \\
                		%\hline
                		%1141.4 & 1.467351 &  70431.41 & 23.57 & 115.5502 & 26.2 & 3.470122 & 3440.159\\
                 		%2667.0 & 1.754386 &   6407.35 & 32.10 &  39.5922 & 27.8 & 4.105816 & 8554.186\\
                		%307.3  & 0.421585 &  63411.55 & 35.14 & 106.0642 & 29.4 & 5.610657 & 2544.263\\
                		%840.7  & 0.000000 &  16396.95 & 23.46 &  50.5777 & 30.8 & 3.848957 & 2366.732\\
                		%482.5  & 1.184990 &  78906.07 & 31.95 &  -1.1440 & 20.3 & 3.700461 & 4954.157\\
                		%550.0  & 2.072846 &  28103.63 & 17.81 &  25.7648 & 22.8 & 2.742410 & 2976.427\\
                		%514.1  & 0.607718 &  22717.82 & 22.23 &  18.3592 & 14.9 & 5.063129 & 4905.780\\
                		%1352.5 & 1.470588 &   6602.02 & 65.03 &-232.2549 & 19.6 & 9.178106 & 5970.520\\
                		%1136.5 & 0.828500 & 123240.60 & 21.44 &  60.6078 & 30.8 & 2.495392 & 2567.212\\
                		%1575.4 & 1.660879 &  87061.77 & 31.26 &  86.8236 & 22.2 & 2.100618 & 2589.531\\
                		%1170.6 & 0.000000 &  13042.99 & 37.68 &  31.7781 & 19.1 & 4.491279 & 4141.247\\
                		%1679.6 & 0.962464 &   7197.37 & 21.72 &  79.9475 & 27.0 & 3.074239 & 3216.067\\
                		%882.3  & 1.212856 & 181247.60 & 16.97 &  45.6272 & 17.4 & 1.902308 & 2469.215\\
                		%1347.5 & 1.782531 &  78822.32 & 11.84 &  33.7674 & 26.6 & 1.762038 & 1742.745\\
                		%1353.4 & 1.431127 &  11549.32 & 31.10 &  57.3484 & 16.6 & 3.434238 & 2659.697\\
                		%1363.4 & 2.404810 &  13084.74 & 18.11 &  72.4447 & 22.5 & 2.563138 & 3296.465\\
                		%1087.9 & 0.538648 &  26327.75 & 23.06 & 166.6439 & 21.9 & 2.649425 & 3330.793\\
                		%712.8  & 0.470367 &  27160.63 & 25.46 &  35.4368 & 15.6 & 4.030189 & 3416.769\\
                		%1865.4 & 2.429150 &   9965.54 & 18.14 &  90.4288 & 20.9 & 3.209765 & 4109.007\\
                		%1088.4 & 1.234568 &  38821.03 & 40.84 &  24.8976 & 14.2 & 3.488844 & 5275.824\\
                		%1308.1 & 0.667111 &  76348.86 & 26.11 &  82.5835 & 12.7 & 3.641929 & 4926.775\\
                		%3485.4 & 1.174210 & 197802.90 & 25.89 &  80.5744 & 17.7 & 1.753225 & 5419.784\\
                		%1946.4 & 1.128159 &  35488.20 & 26.85 &  70.3593 & 14.9 & 4.764180 & 5208.218\\
                		%1116.8 & 0.385803 &  76166.11 & 16.39 & 225.7210 & 21.8 & 4.611794 & 2539.369\\
                		%2047.5 & 0.580495 &  54306.30 & 21.42 & 121.5814 & 22.0 & 2.799651 & 2754.369\\
                		%893.3  & 2.475247 &   7206.16 & 12.36 & 103.7574 & 22.9 & 3.882953 & 2083.009\\
                		%1779.2 & 0.000000 &  26995.98 & 45.80 &  78.2787 & 17.6 & 3.511719 & 3982.897\\
                		%703.4  & 1.557632 &   3533.64 & 49.19 & -32.1768 & 54.9 & 6.609568 & 4082.064\\
                		%695.1  & 2.714932 &   6912.47 & 43.87 &  32.3424 & 19.7 & 6.407141 & 2998.161\\
                		%1287.0 & 0.257732 &  62029.57 & 36.51 &  17.6954 & 15.0 & 4.470681 & 6310.163\\
                		%573.0  & 0.000000 &  13153.38 & 28.29 &  48.0668 & 26.0 & 2.264973 & 2759.142\\
                		%955.6  & 0.332263 & 151116.20 & 32.31 &  50.3237 & 15.0 & 3.284161 & 5634.305\\
                		%1296.1 & 1.187472 &  70689.08 & 23.75 &  31.7058 & 20.6 & 3.450963 & 2992.531\\
                		%1194.5 & 1.574803 &   5132.25 & 15.61 &  54.0354 & 15.4 & 2.777402 & 2679.454\\
                		%4299.2 & 3.565225 & 169930.40 & 22.62 & 136.6210 & 20.3 & 2.841627 & 3072.191\\
                		%896.4  & 1.259843 &  66218.98 & 23.09 &  35.2008 & 32.9 & 3.420866 & 2381.565\\
                		%815.4  & 4.106776 &  53441.92 & 37.42 &-176.8579 & 23.8 & 6.527769 & 4303.525\\
                		%2262.9 & 0.752445 & 121275.40 & 51.96 &  18.3963 & 14.8 & 2.932871 & 4130.657\\
                		%1108.8 & 0.996016 &  10880.08 & 38.61 &  22.0054 & 16.1 & 2.421065 & 3339.350\\
                		%1314.2 & 1.404494 &  27217.83 & 25.78 &  36.9890 & 17.4 & 1.432419 & 2533.139\\
                		%1268.9 & 4.207574 &  2172.79  & 17.05 &  23.4408 & 16.9 & 2.988671 & 2366.595\\
                		%952.8  & 0.807591 &  94001.98 & 41.90 &  33.3545 & 26.6 & 1.970768 & 2031.763\\
                		%763.2  & 0.403481 & 121752.90 & 51.39 &  53.8607 & 24.4 & 2.710884 & 1620.140\\
                		%1228.6 & 0.000000 &  11480.06 & 21.33 &  73.4535 & 21.8 & 4.641821 & 5069.072\\
                		%1023.5 & 3.527337 &   3049.59 & 20.86 &  84.8460 & 18.9 & 4.317273 & 2983.782\\
                		%1646.3 & 1.272669 &  58245.25 & 19.83 & 105.2593 & 17.7 & 2.980112 & 3097.782\\
                		%2483.3 & 1.195695 &  80936.93 & 30.97 & 160.9784 & 24.7 & 6.063038 & 5996.734\\
                		%1045.3 & 0.555247 &  15810.82 & 15.11 &  78.1052 & 22.3 & 3.786695 & 2222.728\\
                		%3866.9 & 1.210898 &  70813.30 & 24.03 &  60.6050 & 15.2 & 3.335582 & 4972.425\\
                		%1450.2 & 0.000000 &   7217.25 & 10.33 & 163.1716 & 29.0 & 2.623226 & 1860.076\\
                		%\end{tabular} \end{tiny} \end{normalfont} \bls\vspace{11pt}\end{center}

        \item   % NEW
                Replicate the Meier Interaction Model in Table~\ref{new.model2.table} on page~\pageref{new.model2.table}, using 
                the dataset \texttt{student.score} in \texttt{BaM}.  Modify the prior variances for the coefficient from 10 to other
                numbers.  Does this change the prior predictive distribution of the data (at means for $\tX$)?  Why or why not?

        \item   \label{normal.derivation.exercise}
				Returning to the discussion of conjugate priors for the Bayesian linear regression model starting 
                on page~\pageref{conjugate.prior.linear.regression}, show that substituting 
                \eqref{conjugate.substitution.for.btilde} and \eqref{conjugate.substitution.for.stilde} into
                \eqref{first.conjugate.posterior} produces \eqref{second.conjugate.linear.posterior}.

                % NEW
        \item   Consider a linear regression setting where the matrix $\X'\X$ is singular ($\X$ is $n\times k$).  Clearly non-Bayesian 
                solutions are limited, but careful stipulation of priors can lead to workable results.  Using the setup of Zellner 
                (1971, p.75) we start with the usual joint conjugate prior $p(\B,\sigma) = p(\B|\sigma)p(\sigma)$, which is essentially
                \eqref{conjugate.prior.linear.regression}, and
                \begin{align*}
                    p(\sigma) &\propto \frac{1}{\sigma^{\nu_0 + 1}}\exp\left[-\frac{\nu_0c_0^2}{2\sigma^2}\right] \\
                    p(\B|\sigma) &\propto \frac{|\bA|^{-\half}}{\sigma^k} \exp\left[-\frac{1}{2\sigma^2}(\B-\mathbb{B})'\bA(\B-\mathbb{B}) \right]
                \end{align*}
                where $\mathbb{B}$ is the prior mean for $\B$, the prior covariance matrix, $\sigma^2\bA^{-1}$, is nonsingular.  Using
                the likelihood function in \eqref{linear.likelihood.equation}, we get the joint posterior distribution:
                \begin{equation*}
                    p(\B,\sigma|\X,\y) \propto \sigma^{-n' -k -1}
                        \exp\left[-\frac{1}{2\sigma^2}(n'c^2 + (\B-\tilde{\B})(\bA+\X'X)(\B-\tilde{\B})\right],
                \end{equation*}
                where $n' = n + \nu_0$, and $n'c^2 = \nu_0c_0^2 +\y'\y + \mathbb{B}'\bA\mathbb{B} - \tilde{\B}'(\bA + \X'\X)\tilde{\B}$
                (notice that there is no need to invert $\X'\X$).  Show that the marginal posterior distribution for $\B$ is
                \begin{equation*}
                    \pi(\B|\X,\y) \propto \left[ n'c^2 + (\B-\tilde{\B}'(\bA + \X'\X)(\B-\tilde{\B}) \right]^{-\frac{n' + k}{2}},
                \end{equation*}
                and give the distributional form with parameters identified.  % multivariate t with mean \tilde{\B}

	    \item	For the Bayesian linear regression model, prove that the posterior that results from conjugate
		        priors is asymptotically equivalent to the posterior that results from uninformative priors.
		        See Table~\ref{linear.model.comparison} on page~\pageref{linear.model.comparison}.

    \item   % NEW, ADD DATA TO BaM AS ehcps
            The following data come from the 1998 European Household Community Panel Survey (given in the \R\ package
            \texttt{BaM} as \texttt{ehcps}).  The two variables are: (1)  the median (EU standardized) income of individuals
            age 65 and older as a percentage of the population age 0--64, and (2) the percentage of all age groups with income
            below 60\% of the median (EU standardized) income of the national population.  Regress the second variable on the
            first with a linear model, constructing conjugate and uninformative prior distributions for Over 65 Relative
            Income.  Compare these results.  Is it possible to specify highly influential priors in the conjugate case that
            differ markedly from the uninformative case?\\
            \parbox[c]{\linewidth}{
                    \begin{center} 
                    \hspace{-0.85in}
                    \begin{normalfont} \begin{tabular}{lrr}
                        Nation          & Over 65 Relative Income   & Total Poverty Rate \\[-3pt]
                        \hline
                        Netherlands & 93.00 & 7.00 \\[-3pt]
                        Luxembourg & 99.00 & 8.00 \\[-3pt]
                        Sweden & 83.00 & 8.00 \\[-3pt]
                        Germany & 97.00 & 11.00 \\[-3pt]
                        Italy & 96.00 & 14.00 \\[-3pt]
                        Spain & 91.00 & 16.00 \\[-3pt]
                        Finland & 78.00 & 17.00 \\[-3pt]
                        France & 90.00 & 19.00 \\[-3pt]
                        United.Kingdom & 78.00 & 21.00 \\[-3pt]
                        Belgium & 76.00 & 22.00 \\[-3pt]
                        Austria & 84.00 & 24.00 \\[-3pt]
                        Denmark & 68.00 & 31.00 \\[-3pt]
                        Portugal & 76.00 & 33.00 \\[-3pt]
                        Greece & 74.00 & 33.00 \\[-3pt]
                        Ireland & 69.00 & 34.00 \\[3pt]
                    \end{tabular} \end{normalfont} \end{center}
            }

                \index{authorindex}{Moore, D. S.} \index{authorindex}{McCabe, G. P.}
	    \item 	Develop a Bayesian linear model for the following data that describe the average weekly household spending
		        on tobacco and alcohol (in pounds) for the eleven regions of the United Kingdom (Moore and McCabe 1989, originally
		        from \emph{Family Expenditure Survey, Department of Employment, 1981}, British Official Statistics).  Specify both 
		        an informed conjugate and uninformed prior using the level for alcoholic beverages as the outcome variable and
		        the level for tobacco products as the explanatory variable.  Do you notice a substantial difference in the 
		        resulting posteriors?  Describe.\\
                \parbox[r]{\linewidth}{
                    \begin{center}
                    \hspace{-1.5in}
		            \begin{tabular}{lrr}
			            Region			    & Alcohol	& Tobacco \\
			            \hline
			            Northern Ireland	& 4.02		& 4.56 \\
			            East Anglia		    & 4.52		& 2.92 \\
			            Southwest		    & 4.79		& 2.71 \\
			            East Midlands		& 4.89		& 3.34 \\
			            Wales			    & 5.27		& 3.53 \\
			            West Midlands		& 5.63		& 3.47 \\
			            Southeast		    & 5.89		& 3.20 \\
			            Scotland		    & 6.08		& 4.51 \\
			            Yorkshire		    & 6.13		& 3.76 \\
			            Northeast		    & 6.19		& 3.77 \\
			            North			    & 6.47		& 4.03 \\
		            \end{tabular}
                    \end{center}
                }
    
            % NEW
    \item   Using your two model results from the \emph{Family Expenditure Survey} model above, create two graphs of predicted
            outcome values using the following steps for each:
            \begin{enumerate}
                \item   Create a matrix of $\X$ values over the range of the data, $\tilde{\X}$, of size greater than the data.
                \item   Draw $m$ (large) values from the posterior distributions of $\B$: $\tilde{\B}$.
                \item   Create a vector of predicted outcomes according to: $\tilde{\y} = \tilde{\X}\tilde{\B}$.
                \item   Plot these values with a histogram and a random sample of them against the actual data with qqplot.
            \end{enumerate}
            Observe any differences occurring from the use of different priors.

	\item	The standard econometric approach to testing parameter restrictions in the linear model is to compare
		    $H_0\range \mathbf{R}\B - \q = 0$ with $H_1\range \mathbf{R}\B - \q \ne 0$ (e.g., Greene 2011), 
		    \index{authorindex}{Greene, W.} where $\mathbf{R}$ is
		    a matrix of linear restrictions and $\q$ is a vector of values (typically zeros).  Thus for example
		    $\mathbf{R} = \left[ \begin{smallmatrix} 1 & 0 & 0 & 0 \\ 0 & 1 & 1 & 0 \\ 0 & 0 & 1 & 1 \end{smallmatrix} \right]$
		    and $\q = [0,1,2]'$ indicate $\beta_1 = 0$, $\beta_2 + \beta_3 = 1$, and $\beta_4 = 2$.  
		    Derive expressions for the posteriors $\pi(\B|\X,\y)$ and $\pi(\sigma|\X,\y)$ using both conjugate and uninformative priors.

            % NEW
    \item   The inverse of a matrix $\UP^{-1}$ of $\UP$ is defined as meeting five conditions: (1) $\HH\UP^{-1}\UP=\HH$, 
            (2) $\UP^{-1}\UP\UP^{-1}=\UP^{-1}$, (3) $(\UP\UP^{-1})'=\UP^{-1}\UP$, (4) $(\UP^{-1}\UP)=\UP\UP^{-1}$, and 
            (5) $\UP^{-1}\UP=\mathbf{I}$ (1--4 implied by 5, $\HH$ a symmetric, full rank matrix).
            The Moore-Penrose generalized inverse matrix $\UP^-$ of $\UP$ meets only the first four conditions.  
            A pseudo-variance matrix is calculated as $\mathbf{V}'\mathbf{V}$, where $\mathbf{V}=\text{GCHOL}(\HH^-)$, 
            $\text{GCHOL}(\cdot)$ is the generalized Cholesky, and $\HH^-$ is the generalized inverse of the Hessian 
            matrix.  The result is a pseudo-variance matrix that is in most cases well conditioned (not nearly singular).  
            Show that if the Hessian is invertible, the pseudo-variance matrix is the usual inverse of the negative Hessian.

	\item	Using the Palm Beach County electoral data (page~\pageref{pbc.example}, also available in the \R\ package \texttt{BaM}
            using \texttt{data(pbc)}), calculate the posterior predictive distribution of the data using a skeptical conjugate prior 
            (i.e., centered at zero for hypothesized effects and having large variance).

    \item   % NEW
            Returning to the 1998 European Household Community Panel Survey data, run a heteroscedastic specification 
            using the Gibbs steps given on page~\pageref{Geweke.Gibbs.Steps} written in \R.  Do you see a model improvement
            over a regular non-Bayesian model assuming homoscedasticity?  Provide evidence.

	\item	Rerun the Ancient China Conflict Model using the code in this chapter's {\bf Computational Addendum} using
		    informed prior specifications of your choice (you should be willing to defend these decisions though).
		    Calculate the posterior mean for each marginal distribution of the parameters and create a set of $104$ predicted 
		    outcome data values as customarily done in linear models analysis.  Graph the $y_i$ against $\hat{y}_i$ and make a 
		    statement about the quality of fit for your model.  Can you improve this fit by dramatically changing the
		    prior specification?

            % NEW
    \item   The ``Grenander Conditions'' for establishing asymptotic properties of the linear model are given by:
            [G1:]  for each column of $\X$: $\X_k'\X_k \longrightarrow +\infty$ (sums of squares grow as $n$ grows, 
             no columns of all zeros), [G2:]  no single observation dominates each explanatory variable $k$: 
            $\underset{n \longrightarrow \infty}{\lim} \frac{ \X_{ik}^2 }{\X_k'\X_k} = 0, i=1,\ldots,n$, [G3:]  
            $\X'\X$ has rank $k$ by Gauss-Markov assumption, define $\X_{-0}$ as the explanatory 
            variable matrix minus the leading column of 1s, then $\underset{n \longrightarrow \infty}{\lim} 
            \X_{-0'}\X_{-0} = C$, $C$ a positive definite matrix.  Prove that for a Bayesian linear model 
            with conjugate priors these provide: 
            $\hat{\beta} \;\underset{\sim}{\text{\tiny asym.}}\; N\left[ \B, \frac{\hat{\sigma}^2}{n}\Q^{-1} 
            \right]$, where $\Q = \underset{n \rightarrow \infty}{\lim} \frac{1}{n}\X'\X$, $\hat{\beta}$ is the 
            posterior mean vector for the coefficients, and $\hat{\sigma}$ is the posterior mean of $\sigma$. 
\end{exercises}


\section{Computational Addendum}
\subsection{Palm Beach County Normal Model}\label{pbc.model.code}
% NEED TO CLEANUP PBC.VOTE IN BaM [okay]
The \R\ code here was used to develop the linear model example with the Palm Beach County voting data.
\index{subjectindex}{R@\R!Bayesian linear model}
\index{subjectindex}{Palm Beach County, FL}
\index{subjectindex}{Bayesian linear model!conjugate priors}
\index{subjectindex}{Bayesian linear model!uninformative priors}

\begin{R.Code}
# RETURNS A REGRESSION TABLE WITH CREDIBLE INTERVALS
t.ci.table <- function(coefs,cov.mat,level=0.95,degrees=Inf,
        quantiles=c(0.025,0.500,0.975))  \{
    quantile.mat <- cbind( coefs, sqrt(diag(cov.mat)),
        t(qt(quantiles,degrees) %o% sqrt(diag(cov.mat)))
        + matrix(rep(coefs,length(quantiles)),
        ncol=length(quantiles)) )
    quantile.names <- c("Mean","Std. Error")
    for (i in 1:length(quantiles))
        quantile.names <- c(quantile.names,paste(quantiles[i],
        "Quantile"))
    dimnames(quantile.mat)[2] <- list(quantile.names)
    return(list(title="Posterior Quantities",round(quantile.mat,4)))
\}

# READ IN THE DATA AND USE MULTIPLE IMPUTATION ON MISSING
lapply(c("BaM","mice","nnet"),library,character.only=TRUE)
data(pbc.vote)
attach(pbc.vote)
X <- cbind(tech, new, turnout, rep, whi)
Y <- badballots
detach(pbc.vote)
imp.X <- mice(X)
X <- as.matrix(cbind(rep(1,nrow(X)), complete(imp.X)))
dimnames(X)[[2]] <- c("tech", "new", "turnout", "rep", "whi")

# UNINFORMED PRIOR ANALYSIS
bhat <- solve(t(X)%*%X)%*%t(X)%*%Y
s2 <- t(Y- X%*%bhat)%*%(Y- X%*%bhat)/(nrow(X)-ncol(X))
R <- solve(t(X)%*%X)*((nrow(X)-ncol(X))*
        s2/(nrow(X)-ncol(X)-2))[1,1]
uninformed.table <- t.ci.table(bhat,R,
        degrees=nrow(X)-ncol(X))[[2]]
alpha <- (nrow(X)-ncol(X)-1)/2
beta <- 0.5*s2*(nrow(X)-ncol(X))
sort.inv.gamma.sample <- sort(1/rgamma(10000,alpha,beta))
sqrt.sort.inv.gamma.sample <- sqrt(sort.inv.gamma.sample)
uninformed.table <- rbind(uninformed.table,
       c( mean(sqrt.sort.inv.gamma.sample),
      sqrt(var(sqrt.sort.inv.gamma.sample)),
               sqrt.sort.inv.gamma.sample[250],
               sqrt.sort.inv.gamma.sample[5000],
               sqrt.sort.inv.gamma.sample[9750] ))

# CONJUGATE PRIOR ANALYSIS
A <- 3; B <- 9
BBeta <- rep(0,6); Sigma <- diag(c(2,2,2,2,2,2))
tB <- solve(solve(Sigma)
    + t(X)%*%X)%*%(solve(Sigma)%*%BBeta+t(X)%*%X%*%bhat)
ts <- 2*B + s2*(nrow(X)-ncol(X)) + (t(BBeta)-t(tB))%*%
     solve(Sigma)%*%BBeta + t(bhat-tB)%*%t(X)%*%X%*%bhat
R <- diag(ts/(nrow(X)+A-ncol(X)-3))*
     solve(solve(Sigma)+t(X)%*%X)
alpha <- nrow(x)+A-ncol(X); beta <- 0.5*S2*(nrow(X)+A-ncol(X))
conjugate.table<-t.ci.table(tB,R,
    degrees=nrow(X)+A-ncol(X)-2)[[2]]
sort.inv.gamma.sample <- sort(1/rgamma(10000,alpha,beta))
sqrt.sort.inv.gamma.sample <- sqrt(sort.inv.gamma.sample)
conjugate.table<- rbind(conjugate.table,
    c( mean(sqrt.sort.inv.gamma.sample),
        sqrt(var(sqrt.sort.inv.gamma.sample)),
                 sqrt.sort.inv.gamma.sample[250],
                 sqrt.sort.inv.gamma.sample[5000],
                 sqrt.sort.inv.gamma.sample[9750] ))
\end{R.Code}

\subsection{Educational Outcomes Model}
\index{authorindex}{Meier, K. J.}
While programming in \bugs\ is not explained until later chapters, the code for the Meier \etal model and the informed extension is 
provided here for reference.  Notice, for the time being, that \bugs\ code shares many features with \R\ in terms of specifying models and 
distributions.  The first model could be coded in a more efficient manner by making \texttt{theta} a vector, but leaving it as a set of 
scalars makes modifying the prior with substantive prior information easier to specify and evaluate.  The \texttt{X[]} variables are
out of order in the linear specification because the data matrix orders them differently than the model listing in the original article.
Also, we would normally specify a \texttt{theta} vector below instead of a series of scalars, but the individual notation is retained to make
it easier to contrast the two model specifications.

\begin{Bugs.Code}
# MEIER, ET AL. REPLICATION MODEL
Model \{
    theta0~dnorm(0.0,0.001);  theta1~dnorm(0,0.001)
    theta2~dnorm(0.0,0.001);  theta3~dnorm(0.0,0.001)
    theta4~dnorm(0.0,0.001);  theta5~dnorm(0.0,0.001)
    theta6~dnorm(0.0,0.001);  theta7~dnorm(0.0,0.001)
    theta8~dnorm(0.0,0.001);  theta9~dnorm(0.0,0.001)
    theta10~dnorm(0.0,0.001); theta11~dnorm(0.0,0.001)
    theta12~dnorm(0.0,0.001); theta13~dnorm(0.0,0.001)
    theta14~dnorm(0.0,0.001); theta15~dnorm(0.0,0.001)
    tau~dgamma(16,6); sigma.sq <- 1

    for (i in 1 : N)  \{
        epsilon[i]~dnorm(0.0, sigma.sq)
        lambda[i] <- theta0 + theta1*X9[i]   + theta2*X10[i] + 
            theta3*X2[i]    + theta4*X3[i]   + theta5*X4[i] + 
            theta6*X5[i]    + theta7*X6[i]   + theta8*X7[i] + 
            theta9*X8[i]    + theta10*X11[i] + theta11*X12[i] + 
            theta12*X13[i]  + theta13*X14[i] + theta14*X15[i] + 
            theta15*X16[i]  + epsilon[i]
        Y[i] ~ dnorm(lambda[i], tau)
    \}
\}

# MODEL WITH INFORMED PRIORS AND INTERACTION
Model  \{
    theta0~dnorm(0.0,0.1);   theta1~dnorm(-.025,0.1)
    theta2~dnorm(0.0,0.1);   theta3~dnorm(0.23,0.1)
    theta4~dnorm(0.615,0.1); theta5~dnorm(-0.068,0.1)
    theta6~dnorm(0.0,0.1);   theta7~dnorm(-.033,0.1)
    theta8~dnorm(0.299,0.1); theta9~dnorm(0.0,0.1)
    theta10~dnorm(0.0,0.1);  theta11~dnorm(0.0,0.1)
    theta12~dnorm(0.0,0.1);  theta13~dnorm(0.0,0.1)
    theta14~dnorm(0.0,0.1);  theta15~dnorm(0.0,0.1)
    tau~dgamma(16,6); sigma.sq <- 1

    for (i in 1 : N)  \{
        epsilon[i]~dnorm(0.0, sigma.sq)
        lambda[i] <- theta0 + theta1*X9[i]   + theta2*X10[i] + 
            theta3*X2[i]    + theta4*X3[i]   + theta5*X4[i] + 
            theta6*X5[i]    + theta7*X6[i]   + theta8*X7[i] + 
            theta9*X8[i]    + theta10*X11[i] + theta11*X12[i] + 
            theta12*X13[i]  + theta13*X14[i] + theta14*X15[i] + 
            theta15*X16[i]  + theta16*X6*X3  + epsilon[i]
        Y[i] ~ dnorm(lambda[i], tau)
    \}
\}
\end{Bugs.Code}

\subsection{Ancient China Conflict Model}
This section provides the \R\ code for running the Gibbs sampler in the Chinese wars example.

\begin{R.Code}
data(wars)
attach(wars)
X <- cbind(EXTENT,DIVERSE,ALLIANCE,DYADS,TEMPOR,DURATION)
y <- SCOPE
detach(wars)
n <- nrow(X); k <- ncol(X)
nu <- 5
num.sims <- 10000
war.samples <- matrix(NA,nrow=num.sims,(ncol=k+n+1))
beta <- rep(1,ncol(X));sigma.sq <- 3;Omega <- 3*diag(n)
b <- solve(t(X) %*% X) %*% t(X) %*% y 
yXb <- y-X%*%b

for (i in 1:num.sims)  \{
    Omega.inv <- solve(Omega)
    X2.Om  <- solve(t(X) %*% Omega %*% X) 
    b.star <- X2.Om %*% t(X) %*% Omega %*% y
    s.sq.star <- t(yXb) %*% Omega.inv %*% (yXb)
    u <- y - X %*% beta
    beta <- as.vector( rmultinorm(1, b.star, sigma.sq *solve(t(X) 
        %*% Omega.inv %*% X) ) )
    sigma.sq <- 1/rgamma(1,shape=(n-1)/2,rate=s.sq.star/2) 
    for (j in 1:n) Omega[j,j] <- 1/rgamma(1, shape=(nu+1)/2, 
        rate=((sigma.sq^(-1))*u^2 + nu)/2 )
    war.samples[i,] <- c(beta,sigma.sq,diag(Omega))
\}
\end{R.Code}

\chapter{Common Probability Distributions}\label{distribution.appendix}
%\renewcommand{\thechapter}{\Alph{chapter}}

This appendix serves as a reference for the parametric forms used in the text.  Considerably more detail can be found
in the standard references: Johnson \etal (2005) for univariate forms on the counting measure; Johnson, Kotz, and
Balakrishnan (1997) for multivariate forms on the counting measure; Johnson, Kotz, and Balakrishnan (1994, 1995) for
univariate forms on the Lebesgue measure; Johnson, Kotz, and Balakrishnan (2000) for multivariate forms on the Lesbegue
measure; Fang, Kotz, and Ng (1990) concentrating on symmetric forms; Kotz and Nadarajah (2000) for extreme value distributions;
and more generally, Evans, Hastings, and Peacock (2000), Balakrishnan and Nevzorov (2003), and Krishnamoorthy (2006).
                \index{authorindex}{Johnson, N. L.}
                \index{authorindex}{Kotz, S.}
                \index{authorindex}{Balakrishnan, N.}
                \index{authorindex}{Fang, K.-T.}
                \index{authorindex}{Ng, K. W.}
                \index{authorindex}{Nadarajah, S.}
                \index{authorindex}{Evans, M.}
                \index{authorindex}{Hastings, N.}
                \index{authorindex}{Peacock, B.}
                \index{authorindex}{Nevzorov, V. B.}
                \index{authorindex}{Krishnamoorthy, K.}

\begin{small}
\begin{bayeslist}
    \item {\bf Bernoulli}\index{subjectindex}{distribution!Bernoulli}
        \begin{bayeslist2}
            \item   PMF: $\mathcal{BR}(x|p) = p^x(1-p)^{1-x}, x=0,1,\quad 0 < p < 1$.
            \item   $E[X] = p$.
            \item   $\Var[X] = p(1-p)$.
        \end{bayeslist2}

    \item {\bf Beta}\index{subjectindex}{distribution!beta}
        \begin{bayeslist2}
            \item   PDF: $\mathcal{BE}(x|\alpha,\beta) = \frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)}
                    x^{\alpha-1}(1-x)^{\beta-1},
                    \quad 0 < x < 1, 0 < \alpha,\beta$.
            \item   $E[X] = \frac{\alpha}{\alpha+\beta}$.
            \item   $\Var[X] = \frac{\alpha\beta}{(\alpha+\beta)^2(\alpha+\beta+1)}$.
        \end{bayeslist2}

    \item {\bf Binomial}\index{subjectindex}{distribution!binomial}
        \begin{bayeslist2}
            \item   PMF: $\mathcal{BN}(x|n,p) = \binom{n}{x} p^x (1-p)^{n-x},
                x=0,1,\ldots,n,\quad 0 < p < 1$.
            \item   $E[X] = np$.
            \item   $\Var[X] = np(1-p)$.
        \end{bayeslist2}

    \item {\bf Cauchy}\index{subjectindex}{distribution!Cauchy}
        \begin{bayeslist2}
            \item   PDF: $\mathcal{C}(x|\theta,\sigma) = \frac{1}{\pi\sigma}
                    \frac{1}{ 1+\left(\frac{x-\theta}{\sigma}\right)^2 },
                    \quad -\infty < x,\theta < \infty, 0 < \sigma$.
            \item   $E[X] = \text{Does not exist}$.
            \item   $\Var[X] = \text{Does not exist}$.
            \item   Note: sometimes $\theta$ and $\sigma$ are labeled location and scale, respectively.
        \end{bayeslist2}

    \item {\bf Dirichlet}\index{subjectindex}{distribution!Dirichlet}
        \begin{bayeslist2}
            \item   PDF: $\mathcal{D}(\x|\alpha_1,\ldots,\alpha_k) = 
			\frac{\Gamma(\alpha_1+\ldots+\alpha_k)}{\Gamma(\alpha_1)\cdots\Gamma(\alpha_k)}
                    x_1^{\alpha_1-1}\cdots x_k^{\alpha_k-1}
                    \quad 0 \le x_i \le 1, \sum_{i=1}^{k} x_i = 1, 0 < \alpha_i, \forall i \in [1,2,\ldots,k]$.		%FIXED DIRECTION OF INEQ
            \item   $E[X_i] = \frac{\alpha_i}{\alpha_0}$, where $\alpha_0=\sum_{j=1}^k \alpha_j$.
            \item   $\Var[X_i] = \frac{\alpha_i(\alpha_0-\alpha_i)}{\alpha_0^2(\alpha_0+1)}$.
            \item   $\Cov[X_h,X_i] = -\frac{\alpha_h\alpha_i}{\alpha_0^2(\alpha_0+1)}$.
        \end{bayeslist2}

    \item {\bf Double Exponential}\index{subjectindex}{distribution!double exponential}
        \begin{bayeslist2}
            \item   PDF: $\mathcal{DE}(x|\mu,\ldots,\tau) = 
			\frac{1}{2\tau} \exp[-|x-\mu|/\tau]
                                    \quad -\infty < \mu,x < \infty, 0<\tau$.
            \item   $E[X] = \mu$.
            \item   $\Var[X] = 2\tau^2$.
        \end{bayeslist2}

    \item {\bf F}\index{subjectindex}{distribution!F}
        \begin{bayeslist2}
            \item   PDF: $\mathcal{F}(x|\nu_1,\nu_2) = \frac{ \Gamma\left( \frac{\nu_1+\nu_2}{2} \right) }
                                { \Gamma\left(\frac{\nu_1}{2}\right)\Gamma\left(\frac{\nu_2}{2}\right) }
                           \left( \frac{\nu_1}{\nu_2} \right)^{\nu_1 / 2}
                           \frac{ x^{(\nu_1-2)/2} }
                            { \left( 1+\frac{\nu_1}{\nu_2}x \right)^{\frac{\nu_1+\nu_2}{2}} }, \\
                    \quad 0 \le x < \infty, \nu_1, \nu_2 \in \mathbb{I}^+$.
            \item   $E[X] = \frac{\nu_2}{\nu_2-2}, \nu_2>2$.
            \item   $\Var[X] = 2\left( \frac{\nu_2}{\nu_2-2} \right)^2 \frac{\nu_1+\nu_2-2}{\nu_1(\nu_2-4)},
                    \quad \nu_2 > 4$.
        \end{bayeslist2}

    \item {\bf Gamma}\index{subjectindex}{distribution!gamma}\label{gamma.distribution.form}
        \begin{bayeslist2}
            \item   PDF rate version: $\mathcal{G}(x|\alpha,\beta) = \frac{\beta^\alpha}{\Gamma(\alpha)}x^{\alpha-1}\exp[-x\beta],
                    \quad 0 \le x < \infty, \quad 0 \le \alpha,\beta$.
            \item   PDF scale version: $\mathcal{G}(x|\alpha,\beta) = \frac{\beta^{-\alpha}}{\Gamma(\alpha)}x^{\alpha-1}\exp[-x/\beta],
                    \quad 0 \le x < \infty, \quad 0<\alpha,\beta$.
            \item   $E[X] = \frac{\alpha}{\beta}$, rate version.
            \item   $\Var[X] = \frac{\alpha}{\beta^2}$, rate version.
            \item   $E[X] = \alpha\beta$, scale version.
            \item   $\Var[X] = \alpha\beta^2$, scale version.
            \item   Note: the $\chi^2$ distribution is $\mathcal{G}\left(\frac{\nu}{2},\frac{1}{2}\right)$ ($\nu$ is the degrees
                    of freedom parameter), and the exponential distribution comes from setting the shape parameter to one:
                    $\mathcal{EX}(\beta)$ is $\mathcal{G}(1,\beta)$ (rate version).
        \end{bayeslist2}

    \item {\bf Geometric}\index{subjectindex}{distribution!geometric}
        \begin{bayeslist2}
            \item   PMF: $\mathcal{GEO}(x|p) = p(1-p)^{x-1}, \quad x=1,2,\ldots, \quad 0 \le p \le 1$.
            \item   $E[X] = \frac{1}{p}$.
            \item   $\Var[X] = \frac{1-p}{p^2}$.
        \end{bayeslist2}

    \item {\bf Hypergeometric}\index{subjectindex}{distribution!hypergeometric}
        \begin{bayeslist2}
            \item   PMF: $\mathcal{HG}(x|n,m,k) = \frac{\binom{m}{x}\binom{n-m}{k-x}}{\binom{n}{k}},
                    \quad m-n+k \le x \le m, \quad n,m,k \ge 0$.
            \item   $E[X] = \frac{km}{n}$.
            \item   $\Var[X] = \frac{km(n-m)(n-k)}{n^2(n-1)}$.
        \end{bayeslist2}

    \item {\bf Inverse Gamma}\index{subjectindex}{distribution!inverse gamma}
        \begin{bayeslist2}
            \item   PDF: $\mathcal{IG}(x|\alpha,\beta)
                   = \frac{\beta^\alpha}{\Gamma(\alpha)}(x)^{-(\alpha+1)}
                                   \exp[-\beta/x],
                                   \quad 0< x,\alpha,\beta$.
            \item   $E[X] =  \frac{\beta}{\alpha-1}, \alpha > 1$.
            \item   $\Var[X] = \frac{\beta^2}{(\alpha-1)^2(\alpha-2)}, \alpha > 2$.
        \end{bayeslist2}

    \item {\bf Lognormal}\index{subjectindex}{distribution!lognormal}
        \begin{bayeslist2}
            \item   PDF: $\mathcal{LN}(x|\mu,\sigma) = (2\pi\sigma^2)^{-\frac{1}{2}}x^{-1}
                    \exp[-(\log(x) - \mu)^2/2\sigma^2],$\\
                    $-\infty < \mu,x < \infty, 0<\sigma^2$
            \item   $E[X] = \exp[\mu+\sigma^2/2]$
            \item   $\Var[X] = \exp[2(\mu+\sigma^2)] - \exp[2\mu+\sigma^2]$.
        \end{bayeslist2}

    \item {\bf Multinomial}\index{subjectindex}{distribution!multinomial}
        \begin{bayeslist2}
            \item   PMF: $\mathcal{MN}(x|n,p_1,\ldots,p_k) = \frac{n!}{x_1!\cdots x_k!}p_1^{x_1}\cdots p_k^{x_k},
                \quad x_i=0,1,\ldots,n,$\\
                $0 < p_i < 1, \quad \sum_{i=1}^{k}p_i = 1$.
            \item   $E[X_i] = np_i$.
            \item   $\Var[X_i] = np_i(1-p_i)$.
	    \item   $\Cov[X_i,X_j] = -np_ip_j$.
        \end{bayeslist2}

    \item {\bf Negative Binomial}\index{subjectindex}{distribution!negative binomial}
        \begin{bayeslist2}
            \item   PMF: $\mathcal{NB}(x|r,p) = \binom{x-1}{r-1}p^r(1-p)^{x-r},
                    \quad x=0,1,\ldots, \; 0 < p < 1$, $r \in \mathcal{I}^+$.
            \item   $E[X] = \frac{r(1-p)}{p}$.
            \item   $\Var[X] = \frac{r(1-p)}{p^2}$.
        \end{bayeslist2}

    \item {\bf Normal}\index{subjectindex}{distribution!normal}
        \begin{bayeslist2}
                        \item   PDF: $\mathcal{N}(x|\mu,\sigma^2) =(2\pi\sigma^2)^{-\frac{1}{2}}
                    \exp\left[ -\frac{1}{2\sigma^2}(x - \mu)^2 \right],$\\
                                    $-\infty < \mu,x < \infty, 0<\sigma$.
                        \item   $E[X] = \mu$.
                        \item   $\Var[X] = \sigma^2$.
        \end{bayeslist2}
        Multivariate case: $\mathcal{N}_k(\x|\M,\SI^2)=(2\pi)^{-k/2} |\SI|^{-1/2}
                    \exp\left[ -\frac{1}{2}(\x-\M)'\SIinv(\x-\M) \right]$

    \item {\bf Pareto}\index{subjectindex}{distribution!Pareto}
        \begin{bayeslist2}
            \item   PDF: $\mathcal{PA}(x|\alpha,\beta) = \alpha\beta^\alpha x^{-(\alpha+1)},
                \quad \beta < x, 0 < \alpha,\beta$.
            \item   $E[X] = \frac{\beta\alpha}{\alpha-1}$, exists provided $\alpha > 1$.
            \item   $\Var[X] = \frac{\beta^2\alpha}{(\alpha-1)^2(\alpha-2)}$, exists provided $\alpha > 2$.
        \end{bayeslist2}

    \item {\bf Poisson}\index{subjectindex}{distribution!Poisson}
        \begin{bayeslist2}
            \item   PMF: $\mathcal{P}(x|\lambda) = \frac{\lambda^x e^{-\lambda}}{x!},
                    \quad x=0,1,\ldots, \quad 0 \le \lambda < \infty$.
            \item   $E[X] = \lambda$.
            \item   $\Var[X] = \lambda$.
        \end{bayeslist2}

    \item {\bf Student's-$t$}\index{subjectindex}{distribution!Student's-$t$}
        \begin{bayeslist2}
            \item   PDF: $\mathcal{T}(x|\nu) = \frac{ \Gamma\left(\frac{\nu+1}{2}\right) }
                                   { \Gamma\left(\frac{\nu}{2}\right) }
                    \frac{1}{ (\pi\nu)^\frac{1}{2} (1+x^2/\nu)^{(\nu+1)/2} },
                    \quad -\infty < \mu,x < \infty, \nu\in \mathbb{I}^+$.
            \item   $E[X] = 0, 1 < \nu$.
            \item   $\Var[X] = \frac{\nu}{\nu-2}, 2 < \nu$.
        \end{bayeslist2}

    \item {\bf Student's-$t$, Multivariate}\index{subjectindex}{distribution!Student's-$t$}
	\begin{bayeslist2}
		\item	PDF: $\mathcal{MVT}(\x|\MM,\nu) = |\MM|^{-\half} (\pi\nu)^{-k/2} 
						             \frac{\Gamma\left(\frac{\nu+k}{2}\right)}{\Gamma\left(\frac{\nu}{2}\right)}
							     \left( 1 + \frac{(\x-\M)'\MM(\x-\M)}{\nu} \right)^{-\frac{\nu+k}{2}}$,
			where $\x$ is a $k$-length vector, $\MM$ is a $k \times k$ positive definite matrix, and $\nu$ is a positive scalar.
		\item	$E[\X] = \M$.
		\item	$\Var[\X] = \frac{\nu}{\nu-2}\MMI$.
	\end{bayeslist2}

    \item {\bf Uniform}\index{subjectindex}{distribution!uniform} \\[3pt]
        $k$-Category Discrete Case PMF:
        \begin{bayeslist2}
                \item   $\mathcal{U}(x) = p(X=x) =
                        \begin{cases}
                        \frac{1}{k},    & \text{for}\; x = 1, 2,\ldots, k \\
                        0,              & \text{otherwise}
                        \end{cases}$
            \item   $E[X] = \frac{k+1}{2}$.
            \item   $\Var[X] = \frac{(k+1)(k-1)}{12}$.
        \end{bayeslist2}

        Continuous Case PDF:
        \begin{bayeslist2}
            \item   $\mathcal{U}(x) = f(x) =
                        \begin{cases}
                        \frac{1}{b-a},  & \text{for}\; a=0 \le x \le b=1 \\
                        0,              & \text{otherwise}
                        \end{cases}$
            \item   $E[X] = \frac{b-a}{2}$.
            \item   $\Var[X] = \frac{(b-a)^2}{12}$.
        \end{bayeslist2}

    \item {\bf Weibull }\index{subjectindex}{distribution!Weibull}
        \begin{bayeslist2}
            \item   PDF: $w(x|\gamma,\beta) = \frac{\gamma}{\beta} x^{\gamma-1} \exp\left(-\left(\frac{x}{\beta}\right)^{\gamma}\right)$
                         if $x \ge 0$ and $0$ otherwise,
                        \text{where:} \;
                        $\gamma, \beta > 0$. 
            \item  $E[\X_{ij}] = \beta \Gamma\left[ 1 + \frac{1}{\gamma} \right]$.
            \item  $\Var[X_{ij}] = \beta^2\left( \Gamma\left[1+\frac{2}{\gamma}\right] - \gamma\left[1 + \frac{1}{\gamma}\right]^2\right)$
        \end{bayeslist2}

    \item {\bf Wishart}\index{subjectindex}{distribution!Wishart}
        \begin{bayeslist2}
            \item  PDF: $\mathcal{W}(\X|\alpha,\B) = \frac{ |\X|^{(\alpha-(k+1))/2} }
							  { \Gamma_k(\alpha)|\B|^{\alpha/2} }
                                     \exp[-\text{tr}(\B^{-1}\X)/2]\\
                        \text{where:} \;
					\Gamma_k(\alpha) = 2^{\alpha k/2}
                                         \pi^{k(k-1)/4}
                                         \prod_{i=1}^{k}\Gamma\left(\frac{\alpha+1-i}{2}\right),\;
                                         2\alpha>k-1,\\ 
			\B \, \text{symmetric nonsingular, and} \, \X \, \text{symmetric positive definite}$.
            \item  $E[\X_{ij}] = \alpha\B_{ij}$
            \item  $\Var[X_{ij}] = \alpha(\B_{ij}^2 + \B_{ii}\B_{jj})$
            \item  $\Cov[X_{ij},X_{kl}] = \alpha(\B_{ik}\B_{jl} + \B_{il}\B_{jk})$
        \end{bayeslist2}

\end{bayeslist}
\end{small}


